{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"periomod","text":""},{"location":"#documentation","title":"Documentation","text":""},{"location":"#overview","title":"Overview","text":"<p>Welcome to the periodontal-modeling documentation, a fully automated benchmarking and evaluation package for short-term periodontal modeling. The package provides flexible and efficient preprocessing, model tuning, evaluation, inference, and descriptive analysis with an interactive Gradio frontend, allowing users to perform comprehensive periodontal data modeling.</p> <p>This documentation includes detailed information on the functionality, setup, and usage of each module and provides code examples for streamlined integration into your projects.</p> <p><code>peridontal-modeling</code>, or in short <code>periomod</code>, is specifically tailored to hierarchical periodontal patient data and was developed for Python 3.11, but is also compatible with Python 3.10.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#getting-started","title":"Getting Started","text":"<p>If you're not familiar with Python or programming, the following resources can help you get started with installing Python, managing environments, and setting up tools like Jupyter Notebooks:</p>"},{"location":"#installing-python","title":"Installing Python","text":"<ol> <li>Official Python Website: Download and install Python from the official Python website.</li> <li>Make sure to download Python 3.10 or 3.11.</li> <li> <p>During installation, check the box to add Python to your system's PATH.</p> </li> <li> <p>Python Installation Guide: Follow this step-by-step guide for detailed instructions on setting up Python on your system.</p> </li> </ol>"},{"location":"#installing-conda","title":"Installing Conda","text":"<p>Conda is an environment and package manager that makes it easy to install dependencies and manage Python environments.</p> <ol> <li>Miniconda: Download and install Miniconda, a lightweight version of Conda.</li> <li>Full Anaconda Distribution: Alternatively, install the Anaconda Distribution, which includes Conda and many pre-installed libraries.</li> </ol> <p>You can create a virtual environment with conda:</p> <pre><code>conda create -n periomod python=3.11\nconda activate periomod\n</code></pre>"},{"location":"#setting-up-jupyter-notebooks","title":"Setting Up Jupyter Notebooks","text":"<p>Jupyter Notebooks are an excellent tool for analyzing data and running experiments interactively.</p> <ul> <li>Learn Jupyter Basics: Refer to the Jupyter Notebook Beginner\u2019s Guide to learn how to use it.</li> </ul>"},{"location":"#installing-periomod","title":"Installing periomod","text":"<p>Ensure you have Python 3 installed. You may preferebly setup a new environment with Python 3.10 or 3.11. Install the package with all its dependencies via pip:</p> <pre><code>pip install periomod\n</code></pre>"},{"location":"#core-modules","title":"Core Modules","text":"<p>The following sections summarize each core module within <code>periodontal-modeling</code>, with links to detailed documentation and examples for each function.</p>"},{"location":"#app-module","title":"App Module","text":"<p>The App module hosts the Gradio-based interface, providing a unified platform for tasks such as plotting descriptives, conducting benchmarks, evaluating models, and performing inference.</p> <p>Read more on the App Module</p>"},{"location":"#data-module","title":"Data Module","text":"<p>The Data module provides tools for loading, transforming, and saving processed periodontal data. Classes like <code>StaticProcessEngine</code> handle data preparation, while <code>ProcessedDataLoader</code> is tailored for fully preprocessed datasets, supporting encoding and scaling options.</p> <p>Read more on the Data Module</p>"},{"location":"#descriptive-analysis","title":"Descriptive Analysis","text":"<p>The Descriptives module enables users to perform statistical analysis and visualize periodontal data pre- and post-therapy using tools like <code>DescriptivesPlotter</code>. It includes methods for generating matrices, histograms, and pocket depth comparisons.</p> <p>Read more on the Descriptives Module</p>"},{"location":"#training-module","title":"Training Module","text":"<p>The Training module provides core training and model-building functionalities, including threshold tuning. The <code>Trainer</code> classes allow you to streamline model fitting.</p> <p>Read more on the Training Module</p>"},{"location":"#tuning-module","title":"Tuning Module","text":"<p>The Tuning module offers classes like <code>HEBOTuner</code> and <code>RandomSearchTuner</code> for hyperparameter optimization, supporting both Bayesian optimization and random search.</p> <p>Read more on the Tuning Module</p>"},{"location":"#evaluation-module","title":"Evaluation Module","text":"<p>The Evaluation module supplies methods for post-model training analysis, including prediction assessment, feature importance calculations, and feature clustering. The <code>ModelEvaluator</code> class enables confusion matrix plotting and other feature-based evaluations.</p> <p>Read more on the Evaluation Module</p>"},{"location":"#inference-module","title":"Inference Module","text":"<p>The Inference module handles patient-specific predictions, confidence intervals through jackknife resampling, and both single and batch inference. The <code>ModelInference</code> class supports robust inference operations.</p> <p>Read more on the Inference Module</p>"},{"location":"#benchmarking-module","title":"Benchmarking Module","text":"<p>The Benchmarking module provides tools for running and comparing multiple experiments across different tuning setups. Use <code>Experiment</code> for single benchmark execution or <code>Benchmarker</code> to perform full comparative analyses across models and configurations.</p> <p>Read more on the Benchmarking Module</p>"},{"location":"#wrapper-module","title":"Wrapper Module","text":"<p>The Wrapper module simplifies the benchmarking and evaluation setup, consolidating various submodules for a more concise codebase. <code>BenchmarkWrapper</code> enables straightforward benchmarking, while <code>EvaluatorWrapper</code> supports model evaluation, feature importance, and patient-level inference.</p> <p>Read more on the Wrapper Module</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Preprocessing Pipeline: Flexible preprocessing of periodontal data, including encoding, scaling, data imputation and transformation.</li> <li>Descriptive Analysis: Generate descriptive statistics and plots such as confusion matrices and bar plots.</li> <li>Automatic Model Tuning: Supports multiple learners and tuning strategies for optimized model training.</li> <li>Resampling: Allows the use of grouped holdout and cross-validation resampling.</li> <li>Imbalanced Data Handling: Enables the application of SMOTE and upsampling/downsampling to balance dataset classes.</li> <li>Model Evaluation: Provides a wide range of evaluation tools, including confusion matrices, clustering and feature importance.</li> <li>Inference: Patient-level inference, jackknife resampling and confidence intervals.</li> <li>Interactive Gradio App: A simple Gradio interface for streamlined model benchmarking, evaluation and inference.</li> </ul>"},{"location":"#usage","title":"Usage","text":""},{"location":"#app-module_1","title":"App Module","text":"<p>The periomod app provides a streamlined gradio interface for plotting descriptives, performing benchmarks, model evaluation and inference.</p> <pre><code>from periomod.app import perioapp\n\nperioapp.launch()\n</code></pre> <p>If you download the repository and install the package in editable mode, the following <code>make</code> command starts the app:</p> <pre><code>pip install -e .\nmake app\n</code></pre> <p>The app can also be launched using docker. Run the following commands in the root of the repository:</p> <pre><code>docker build -f docker/app.dockerfile -t periomod-image .\ndocker run -p 7890:7890 periomod-image\n</code></pre> <p>By default, the app will be launched on port 7890 and can be accessed at <code>http://localhost:7890</code>.</p> <p>Alternatively, the following <code>make</code> commands are available to build and run the docker image:</p> <pre><code>make docker-build\nmake docker-run\n</code></pre>"},{"location":"#app-features","title":"App Features","text":""},{"location":"#data-tab","title":"Data Tab","text":"<p>The data tab enables data loading, processing and saving. It further provides multiple plotting methods for data visualization.The Data Tab enables data loading, processing, and saving. It provides several options for exploring and visualizing the dataset, making it easy to gain insights before proceeding to modeling or benchmarks.</p> <p></p>"},{"location":"#benchmarking-tab","title":"Benchmarking Tab","text":"<p>The Benchmarking Tab allows you to perform comparisons of different machine learning models, incorporating hyperparameter tuning, sampling, resampling and different criteria. The results are displayed in a clear and interactive format and allow the comparison with a model baseline.</p> <p></p>"},{"location":"#evaluation-tab","title":"Evaluation Tab","text":"<p>The Evaluation Tab provides detailed performance metrics and visualizations for the trained models. These include confusion matrices, calibration plots, feature importance analysis, clustering, and Brier skill scores. It offers a comprehensive overview of the model performance.</p> <p></p>"},{"location":"#inference-tab","title":"Inference Tab","text":"<p>In the Inference Tab, patient data can be selected and submitted for predictions using a trained model. Additionally, Jackknife confidence intervals can be calculated to evaluate prediction stability.</p> <p></p>"},{"location":"#data-module_1","title":"Data Module","text":"<p>Use the <code>StaticProcessEngine</code> class to preprocess your data. This class handles data transformations and imputation.</p> <pre><code>from periomod.data import StaticProcessEngine\n\nengine = StaticProcessEngine()\ndata = engine.load_data(path=\"data/raw/raw_data.xlsx\")\ndata = engine.process_data(data)\nengine.save_data(df=datas, path=\"data/processed/processed_data.csv\")\n</code></pre> <p>The <code>ProcessedDataLoader</code> requires a fully imputed dataset. It contains methods for scaling and encoding. As encoding types, 'one_hot' and 'target' can be selected. The scale argument scales numerical columns. One out of four periodontal task can be selected, either \"pocketclosure\", \"pocketclosureinf\", \"pdgrouprevaluation\" or \"improvement\".</p> <pre><code>from periomod.data import ProcessedDataLoader\n\n# instantiate with one-hot encoding and scale numerical variables\ndataloader = ProcessedDataLoader(\n    task=\"pocketclosure\", encoding=\"one_hot\", encode=True, scale=True\n)\ndata = dataloader.load_data(path=\"data/processed/processed_data.csv\")\ndata = dataloader.transform_data(data=data)\ndataloader.save_data(df=data, path=\"data/training/training_data.csv\")\n</code></pre>"},{"location":"#descriptives-module","title":"Descriptives Module","text":"<p><code>DesctiptivesPlotter</code> can be used to plot descriptive plots for target columns before and after treatment. It should be applied on a processed dataframe.</p> <pre><code>from periomod.data import ProcessedDataLoader\nfrom periomod.descriptives import DescriptivesPlotter\n\ndata = dataloader.load_data(path=\"data/processed/processed_data.csv\")\n\n# instantiate plotter with dataframe\nplotter = DescriptivesPlotter(data)\nplotter.plt_matrix(vertical=\"pdgrouprevaluation\", horizontal=\"pdgroupbase\")\nplotter.pocket_comparison(col1=\"pdbaseline\", col2=\"pdrevaluation\")\nplotter.histogram_2d(col_before=\"pdbaseline\", col_after=\"pdrevaluation\")\n</code></pre>"},{"location":"#resampling-module","title":"Resampling Module","text":"<p>The <code>Resampler</code> class allows for straightforward grouped splitting operations. It also includes different sampling strategies to treat the minority classes. It can be applied on the training data.</p> <pre><code>from periomod.data import ProcessedDataLoader\nfrom periomod.resampling import Resampler\n\ndata = dataloader.load_data(path=\"data/processed/training_data.csv\")\n\nresampler = Resampler(classification=\"binary\", encoding=\"one_hot\")\ntrain_df, test_df = resampler.split_train_test_df(df=data, seed=42, test_size=0.3)\n\n# upsample minority class by a factor of 2.\nX_train, y_train, X_test, y_test = resampler.split_x_y(\n    train_df, test_df, sampling=\"upsampling\", factor=2\n)\n# performs grouped cross-validation with \"smote\" sampling on the training folds\nouter_splits, cv_folds_indices = resampler.cv_folds(\n    df=train_df, sampling=\"smote\", factor=2.0, seed=42, n_folds=5\n)\n</code></pre>"},{"location":"#training-module_1","title":"Training Module","text":"<p><code>Trainer</code> contains different training methods that are used during hyperparameter tuning and benchmarking. It further includes methods for threshold tuning in the case of binary classification and when the criterion \"f1\" is selected. The <code>Resampler</code>can be used to split the data for training.</p> <pre><code>from periomod.training import Trainer\nfrom sklearn.ensemble import RandomForestClassifier\n\ntrainer = Trainer(classification=\"binary\", criterion=\"f1\", tuning=\"cv\", hpo=\"hebo\")\n\nscore, trained_model, threshold = trainer.train(\n    model=RandomForestClassifier,\n    X_train=X_train,\n    y_train=y_train,\n    X_val=X_val,\n    y_val=y_val,\n)\nprint(f\"Score: {score}, Optimal Threshold: {threshold}\")\n</code></pre> <p>The <code>trainer.train_mlp</code> function uses the <code>partial_fit</code> method of the 'MLPClassifier' to leverage early stopping during the training process.</p> <pre><code>from sklearn.neural_network import MLPClassifier\n\nscore, trained_mlp, threshold = trainer.train_mlp(\n    mlp_model=MLPClassifier,\n    X_train=X_train,\n    y_train=y_train,\n    X_val=X_val,\n    y_val=y_val,\n    final=True,\n)\nprint(f\"MLP Validation Score: {score}, Optimal Threshold: {threshold}\")\n</code></pre>"},{"location":"#tuning-module_1","title":"Tuning Module","text":"<p>The tuning module contains the <code>HEBOTuner</code>and <code>RandomSearchTuner</code> classes that can be used for hyperparameter tuning. <code>HEBOTuner</code> leverages Bayesian optimization to obtain the optimal set of hyperparameters.</p> <pre><code>from periomod.training import Trainer\nfrom periomod.tuning import HEBOTuner\n\ntrainer = Trainer(\n    classification=\"binary\",\n    criterion=\"f1\",\n    tuning=\"holdout\",\n    hpo=\"hebo\",\n    mlp_training=True,\n    threshold_tuning=True,\n)\n\ntuner = HEBOTuner(\n    classification=\"binary\",\n    criterion=\"f1\",\n    tuning=\"holdout\",\n    hpo=\"hebo\",\n    n_configs=20,\n    n_jobs=-1,\n    trainer=trainer,\n)\n\n# Running holdout-based tuning\nbest_params, best_threshold = tuner.holdout(\n    learner=\"rf\", X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val\n)\n\n# Running cross-validation tuning\nbest_params, best_threshold = tuner.cv(learner=\"rf\", outer_splits=cross_val_splits)\n</code></pre> <p><code>RandomSearchTuner</code> implements random search tuning by sampling parameters at random from specified ranges. Also, allows for racing when cross-validation is used as tuning technique.</p> <pre><code>from periomod.training import Trainer\nfrom periomod.tuning import RandomSearchTuner\n\ntrainer = Trainer(\n    classification=\"binary\",\n    criterion=\"f1\",\n    tuning=\"cv\",\n    hpo=\"rs\",\n    mlp_training=True,\n    threshold_tuning=True,\n)\n\ntuner = RandomSearchTuner(\n    classification=\"binary\",\n    criterion=\"f1\",\n    tuning=\"cv\",\n    hpo=\"rs\",\n    n_configs=15,\n    n_jobs=4,\n    trainer=trainer,\n)\n\n# Running holdout-based tuning\nbest_params, best_threshold = tuner.holdout(\n    learner=\"rf\", X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val\n)\n\n# Running cross-validation tuning\nbest_params, best_threshold = tuner.cv(learner=\"rf\", outer_splits=cross_val_splits)\n</code></pre>"},{"location":"#evaluation-module_1","title":"Evaluation Module","text":"<p><code>ModelEvaluator</code> contains method for model evaluation after training, including prediction analysis and feature importance.</p> <pre><code>from periomod.evaluation import ModelEvaluator\n\nevaluator = ModelEvaluator(\n    X=X_test, y=y_test, model=trained_rf_model, encoding=\"one_hot\"\n)\n\n# Plots confusion matrix of target column\nevaluator.plot_confusion_matrix()\n\n# plot feature importances\nevaluator.evaluate_feature_importance(fi_types=[\"shap\", \"permutation\"])\n\n# perform feature clustering\nbrier_plot, heatmap_plot, clustered_data = evaluator.analyze_brier_within_clusters()\n</code></pre>"},{"location":"#inference-module_1","title":"Inference Module","text":"<p>The inference module includes methods for single but also patient-level predictions. Jackknife resampling and confidence intervals are also available.</p> <pre><code>from periomod.base import Patient, patient_to_dataframe\nfrom periomod.inference import ModelInference\n\nmodel_inference = ModelInference(\n    classification=\"binary\", model=trained_model, verbose=True\n)\n\n# Define a patient instance\npatient = Patient()\npatient_df = patient_to_df(patient=patient)\n\n# Prepare data for inference\nprepared_data, patient_data = model_inference.prepare_inference(\n    task=\"pocketclosure\",\n    patient_data=patient_df,\n    encoding=\"one_hot\",\n    X_train=X_train,\n    y_train=y_train,\n)\n\n# Run inference on patient data\ninference_results = model_inference.patient_inference(\n    predict_data=prepared_data, patient_data=patient_data\n)\n\n# Perform jackknife inference with confidence interval plotting\njackknife_results, ci_plot = model_inference.jackknife_inference(\n    model=trained_model,\n    train_df=train_df,\n    patient_data=patient_df,\n    encoding=\"target\",\n    inference_results=inference_results,\n    alpha=0.05,\n    sample_fraction=0.8,\n    n_jobs=4,\n)\n</code></pre>"},{"location":"#benchmarking-module_1","title":"Benchmarking Module","text":"<p>The benchmarking module contains methods to run single or multiple experiments with a specified tuning setup. For a single experiment the <code>Experiment</code>class can be used.</p> <pre><code>from periomod.benchmarking import Experiment\nfrom periomod.data import ProcessedDataLoader\n\n# Load a dataframe with the correct target and encoding selected\ndataloader = ProcessedDataLoader(task=\"pocketclosure\", encoding=\"one_hot\")\ndata = dataloader.load_data(path=\"data/processed/processed_data.csv\")\ndata = dataloader.transform_data(data=data)\n\nexperiment = Experiment(\n    data=data,\n    task=\"pocketclosure\",\n    learner=\"rf\",\n    criterion=\"f1\",\n    encoding=\"one_hot\",\n    tuning=\"cv\",\n    hpo=\"rs\",\n    sampling=\"upsample\",\n    factor=1.5,\n    n_configs=20,\n    racing_folds=5,\n)\n\n# Perform the evaluation based on cross-validation\nfinal_metrics = experiment.perform_evaluation()\nprint(final_metrics)\n</code></pre> <p>For multiple experiments, the <code>Benchmarker</code> class provides a streamlined benchmarking process. It will output a dictionary based on the best 4 models for a respective tuning criterion and the full experiment runs in a dataframe.</p> <pre><code>from periomod.benchmarking import Benchmarker\n\nbenchmarker = Benchmarker(\n    task=\"pocketclosure\",\n    learners=[\"xgb\", \"rf\", \"lr\"],\n    tuning_methods=[\"holdout\", \"cv\"],\n    hpo_methods=[\"hebo\", \"rs\"],\n    criteria=[\"f1\", \"brier_score\"],\n    encodings=[\"one_hot\", \"target\"],\n    sampling=[None, \"upsampling\", \"downsampling\"],\n    factor=2,\n    path=\"/data/processed/processed_data.csv\",\n)\n\n# Running all benchmarks\nresults_df, top_models = benchmarker.run_all_benchmarks()\nprint(results_df)\nprint(top_models)\n</code></pre>"},{"location":"#wrapper-module_1","title":"Wrapper Module","text":"<p>The wrapper module wraps benchmark and evaluation methods to provide a straightforward setup that requires a minimal amount of code while making use of all the submodules contained in the <code>periomod</code> package.</p> <p>The <code>BenchmarkWrapper</code> includes the functionality of the <code>Benchmarker</code> while also wrapping methods for baseline benchmarking and saving.</p> <pre><code>from periomod.wrapper import BenchmarkWrapper\n\n# Initialize the BenchmarkWrapper\nbenchmarker = BenchmarkWrapper(\n    task=\"pocketclosure\",\n    encodings=[\"one_hot\", \"target\"],\n    learners=[\"rf\", \"xgb\", \"lr\", \"mlp\"],\n    tuning_methods=[\"holdout\", \"cv\"],\n    hpo_methods=[\"rs\", \"hebo\"],\n    criteria=[\"f1\", \"brier_score\"],\n    sampling=[\"upsampling\"],\n    factor=2,\n    path=\"/data/processed/processed_data.csv\",\n)\n\n# Run baseline benchmarking\nbaseline_df = benchmarker.baseline()\n\n# Run full benchmark and retrieve results\nbenchmark, learners = benchmarker.wrapped_benchmark()\n\n# Save the benchmark results\nbenchmarker.save_benchmark(\n    benchmark_df=benchmark,\n    path=\"reports/experiment/benchmark.csv\",\n)\n\n# Save the trained learners\nbenchmarker.save_learners(learners_dict=learners, path=\"models/experiment\")\n</code></pre> <p>The <code>EvaluatorWrapper</code> contains methods of the <code>periomod.evaluation</code>and <code>periomod.inference</code> modules.</p> <pre><code>from periomod.base import Patient, patient_to_dataframe\nfrom periomod.wrapper import EvaluatorWrapper, load_benchmark, load_learners\n\nbenchmark = load_benchmark(path=\"reports/experiment/benchmark.csv\")\nlearners = load_learners(path=\"models/experiments\")\n\n# Initialize the evaluator with learners from BenchmarkWrapper and specified criterion\nevaluator = EvaluatorWrapper(\n    learners_dict=learners, criterion=\"f1\", path=\"data/processed/processed_data.csv\"\n)\n\n# Evaluate the model and generate plots\nevaluator.wrapped_evaluation()\n\n# Perform cluster analysis on predictions with brier score smaller than threshold\nevaluator.evaluate_cluster(brier_threshold=0.15)\n\n# Calculate feature importance\nevaluator.evaluate_feature_importance(fi_types=[\"shap\", \"permutation\"])\n\n# Train and average over multiple random splits\navg_metrics_df = evaluator.average_over_splits(num_splits=5, n_jobs=-1)\n\n# Define a patient instance\npatient = Patient()\npatient_df = patient_to_df(patient=patient)\n\n# Run inference on a specific patient's data\npredict_data, output, results = evaluator.wrapped_patient_inference(patient=patient)\n\n# Execute jackknife resampling for robust inference\njackknife_results, ci_plots = evaluator.wrapped_jackknife(\n    patient=my_patient, results=results_df, sample_fraction=0.8, n_jobs=-1\n)\n</code></pre>"},{"location":"#license","title":"License","text":"<p>\u00a9 2024 Tobias Brock, Elias Walter</p> <p>This project is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please open an issue or submit a pull request.</p>"},{"location":"reference/app/","title":"periomod.app Overview","text":"<p>The <code>periomod.app</code> module includes application-specific classes and methods for running the main functionality of <code>periomod</code>.</p>"},{"location":"reference/app/#available-components","title":"Available Components","text":"Component Description App The main application class. Handles configuration, user input, and execution of model workflows."},{"location":"reference/app/app/","title":"App","text":""},{"location":"reference/app/app/#app-module","title":"App Module","text":"<p>The periomod app provides a streamlined gradio interface for plotting descriptives, performing benchmarks, model evaluation and inference.</p> <pre><code>from periomod.app import perioapp\n\nperioapp.launch()\n</code></pre> <p>If you download the repository and install the package in editable mode, the following <code>make</code> command starts the app:</p> <pre><code>pip install -e .\nmake app\n</code></pre> <p>The app can also be launched using docker. Run the following commands in the root of the repository:</p> <pre><code>docker build -f docker/app.dockerfile -t periomod-image .\ndocker run -p 7890:7890 periomod-image\n</code></pre> <p>By default, the app will be launched on port 7890 and can be accessed at <code>http://localhost:7890</code>.</p> <p>Alternatively, the following <code>make</code> commands are available to build and run the docker image:</p> <pre><code>make docker-build\nmake docker-run\n</code></pre>"},{"location":"reference/base/","title":"periomod.base Overview","text":"<p>The <code>periomod.base</code> module contains the foundational classes and helper functions, providing data structure and configuration components.</p>"},{"location":"reference/base/#available-components","title":"Available Components","text":"Component Description BaseConfig Configuration settings class. BaseValidator Validation class for inputs and model outputs. patient_to_df Function for converting patient data to DataFrame. Patient Patient data structure. Side Represents a side of a patient's dental chart. Tooth Represents an individual tooth structure."},{"location":"reference/base/baseconfig/","title":"BaseConfig","text":"<p>Base class to initialize Hydra configuration.</p> <p>This class loads and sets various configuration parameters used across the package, providing easy access to these parameters by initializing them from a Hydra configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the Hydra config directory.</p> <code>'../config'</code> <code>config_name</code> <code>str</code> <p>Name of the configuration file (without extension).</p> <code>'config'</code> <p>Attributes:</p> Name Type Description <code>group_col</code> <code>str</code> <p>Column name used for group-based splitting.</p> <code>y</code> <code>str</code> <p>Target column name in the dataset.</p> <code>target_state</code> <code>int</code> <p>Random state of target encoding.</p> <code>learner_state</code> <code>int</code> <p>Random state of learners.</p> <code>xgb_obj_binary</code> <code>str</code> <p>Objective function for binary classification in XGBoost.</p> <code>xgb_loss_binary</code> <code>str</code> <p>Loss function for binary classification in XGBoost.</p> <code>xgb_obj_multi</code> <code>str</code> <p>Objective function for multiclass classification in XGBoost.</p> <code>xgb_loss_multi</code> <code>str</code> <p>Loss function for multiclass classification in XGBoost.</p> <code>lr_solver_binary</code> <code>str</code> <p>Solver type for binary classification in logistic regression.</p> <code>lr_solver_multi</code> <code>str</code> <p>Solver type for multiclass classification in logistic regression.</p> <code>lr_multi_loss</code> <code>str</code> <p>Loss function for multiclass logistic regression.</p> <code>patient_columns</code> <code>List[str]</code> <p>List of column names related to patient data.</p> <code>tooth_columns</code> <code>List[str]</code> <p>List of column names related to tooth data.</p> <code>side_columns</code> <code>List[str]</code> <p>List of column names related to side data.</p> <code>feature_mapping</code> <code>dict[str, str]</code> <p>Mapping of feature names for plotting.</p> <code>cat_vars</code> <code>List[str]</code> <p>List of categorical variables in the dataset.</p> <code>bin_vars</code> <code>List[str]</code> <p>List of binary variables in the dataset.</p> <code>scale_vars</code> <code>List[str]</code> <p>List of numeric variables to scale in preprocessing.</p> <code>behavior_columns</code> <code>Dict[str, List[str]]</code> <p>Dictionary categorizing behavior-related columns by type.</p> <code>task_cols</code> <code>List[str]</code> <p>List of task-specific columns in the dataset.</p> <code>no_train_cols</code> <code>List[str]</code> <p>Columns excluded from training.</p> <code>infect_vars</code> <code>List[str]</code> <p>Columns indicating infection status.</p> <code>cat_map</code> <code>Dict[str, int]</code> <p>Mapping of categorical features and their maximum values for encoding.</p> <code>target_cols</code> <code>List[str]</code> <p>Columns related to the prediction target.</p> <code>all_cat_vars</code> <code>List[str]</code> <p>Combined list of categorical variables for encoding.</p> <code>required_columns</code> <code>List[str]</code> <p>Combined list of columns required in the dataset for analysis.</p> <code>rs_state</code> <code>int</code> <p>State for random search parameter selection.</p> Example <pre><code>config = BaseConfig()\nprint(config.tooth_columns)\n</code></pre> Note <p>This class assumes Hydra configuration files are correctly set up and stored at <code>config_path</code>. Make sure the file structure and values are properly defined within the configuration.</p> Source code in <code>periomod/base.py</code> <pre><code>class BaseConfig:\n    \"\"\"Base class to initialize Hydra configuration.\n\n    This class loads and sets various configuration parameters used across the package,\n    providing easy access to these parameters by initializing them from a Hydra\n    configuration file.\n\n    Args:\n        config_path (str): Path to the Hydra config directory.\n        config_name (str): Name of the configuration file (without extension).\n\n    Attributes:\n        group_col (str): Column name used for group-based splitting.\n        y (str): Target column name in the dataset.\n        target_state (int): Random state of target encoding.\n        learner_state (int): Random state of learners.\n        xgb_obj_binary (str): Objective function for binary classification in XGBoost.\n        xgb_loss_binary (str): Loss function for binary classification in XGBoost.\n        xgb_obj_multi (str): Objective function for multiclass classification in\n            XGBoost.\n        xgb_loss_multi (str): Loss function for multiclass classification in XGBoost.\n        lr_solver_binary (str): Solver type for binary classification in logistic\n            regression.\n        lr_solver_multi (str): Solver type for multiclass classification in logistic\n            regression.\n        lr_multi_loss (str): Loss function for multiclass logistic regression.\n        patient_columns (List[str]): List of column names related to patient data.\n        tooth_columns (List[str]): List of column names related to tooth data.\n        side_columns (List[str]): List of column names related to side data.\n        feature_mapping (dict[str, str]): Mapping of feature names for plotting.\n        cat_vars (List[str]): List of categorical variables in the dataset.\n        bin_vars (List[str]): List of binary variables in the dataset.\n        scale_vars (List[str]): List of numeric variables to scale in preprocessing.\n        behavior_columns (Dict[str, List[str]]): Dictionary categorizing\n            behavior-related columns by type.\n        task_cols (List[str]): List of task-specific columns in the dataset.\n        no_train_cols (List[str]): Columns excluded from training.\n        infect_vars (List[str]): Columns indicating infection status.\n        cat_map (Dict[str, int]): Mapping of categorical features and their maximum\n            values for encoding.\n        target_cols (List[str]): Columns related to the prediction target.\n        all_cat_vars (List[str]): Combined list of categorical variables for encoding.\n        required_columns (List[str]): Combined list of columns required in the dataset\n            for analysis.\n        rs_state (int): State for random search parameter selection.\n\n    Example:\n        ```\n        config = BaseConfig()\n        print(config.tooth_columns)\n        ```\n\n    Note:\n        This class assumes Hydra configuration files are correctly set up and stored at\n        `config_path`. Make sure the file structure and values are properly defined\n        within the configuration.\n    \"\"\"\n\n    def __init__(\n        self, config_path: str = \"../config\", config_name: str = \"config\"\n    ) -&gt; None:\n        \"\"\"Initializes the Hydra configuration for use in other classes.\"\"\"\n        with hydra.initialize(config_path=config_path, version_base=\"1.2\"):\n            cfg = hydra.compose(config_name=config_name)\n\n        self.group_col = cfg.resample.group_col\n        self.y = cfg.resample.y\n        self.target_state = cfg.resample.target_state\n        self.learner_state = cfg.learner.learner_state\n        self.xgb_obj_binary = cfg.learner.xgb_obj_binary\n        self.xgb_loss_binary = cfg.learner.xgb_loss_binary\n        self.xgb_obj_multi = cfg.learner.xgb_obj_multi\n        self.xgb_loss_multi = cfg.learner.xgb_loss_multi\n        self.lr_solver_binary = cfg.learner.lr_solver_binary\n        self.lr_solver_multi = cfg.learner.lr_solver_multi\n        self.lr_multi_loss = cfg.learner.lr_multi_loss\n        self.patient_columns = cfg.data.patient_columns\n        self.tooth_columns = cfg.data.tooth_columns\n        self.side_columns = cfg.data.side_columns\n        self.feature_mapping = cfg.data.feature_mapping\n        self.cat_vars = cfg.data.cat_vars\n        self.bin_vars = cfg.data.bin_vars\n        self.scale_vars = cfg.data.scale_vars\n        self.behavior_columns = cfg.data.behavior_columns\n        self.task_cols = cfg.data.task_cols\n        self.no_train_cols = cfg.data.no_train_cols\n        self.infect_vars = cfg.data.infect_cols\n        self.cat_map = cfg.data.cat_map\n        self.target_cols = cfg.data.target_cols\n        self.all_cat_vars = self.cat_vars + cfg.data.behavior_columns[\"categorical\"]\n        self.required_columns = (\n            self.patient_columns + self.tooth_columns + self.side_columns\n        )\n        self.rs_state = cfg.tuning.rs_state\n</code></pre>"},{"location":"reference/base/baseconfig/#periomod.base.BaseConfig.__init__","title":"<code>__init__(config_path='../config', config_name='config')</code>","text":"<p>Initializes the Hydra configuration for use in other classes.</p> Source code in <code>periomod/base.py</code> <pre><code>def __init__(\n    self, config_path: str = \"../config\", config_name: str = \"config\"\n) -&gt; None:\n    \"\"\"Initializes the Hydra configuration for use in other classes.\"\"\"\n    with hydra.initialize(config_path=config_path, version_base=\"1.2\"):\n        cfg = hydra.compose(config_name=config_name)\n\n    self.group_col = cfg.resample.group_col\n    self.y = cfg.resample.y\n    self.target_state = cfg.resample.target_state\n    self.learner_state = cfg.learner.learner_state\n    self.xgb_obj_binary = cfg.learner.xgb_obj_binary\n    self.xgb_loss_binary = cfg.learner.xgb_loss_binary\n    self.xgb_obj_multi = cfg.learner.xgb_obj_multi\n    self.xgb_loss_multi = cfg.learner.xgb_loss_multi\n    self.lr_solver_binary = cfg.learner.lr_solver_binary\n    self.lr_solver_multi = cfg.learner.lr_solver_multi\n    self.lr_multi_loss = cfg.learner.lr_multi_loss\n    self.patient_columns = cfg.data.patient_columns\n    self.tooth_columns = cfg.data.tooth_columns\n    self.side_columns = cfg.data.side_columns\n    self.feature_mapping = cfg.data.feature_mapping\n    self.cat_vars = cfg.data.cat_vars\n    self.bin_vars = cfg.data.bin_vars\n    self.scale_vars = cfg.data.scale_vars\n    self.behavior_columns = cfg.data.behavior_columns\n    self.task_cols = cfg.data.task_cols\n    self.no_train_cols = cfg.data.no_train_cols\n    self.infect_vars = cfg.data.infect_cols\n    self.cat_map = cfg.data.cat_map\n    self.target_cols = cfg.data.target_cols\n    self.all_cat_vars = self.cat_vars + cfg.data.behavior_columns[\"categorical\"]\n    self.required_columns = (\n        self.patient_columns + self.tooth_columns + self.side_columns\n    )\n    self.rs_state = cfg.tuning.rs_state\n</code></pre>"},{"location":"reference/base/basevalidator/","title":"BaseValidator","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Base class for initializing classification, criterion, tuning, and HPO.</p> <p>This class extends <code>BaseConfig</code> and validates classification types, evaluation criteria, and tuning methods.</p> Inherits <ul> <li><code>BaseLoader</code>: Provides loading and saving capabilities for processed data.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>classification</code> <code>str</code> <p>Type of classification, either 'binary' or 'multiclass'.</p> required <code>criterion</code> <code>str</code> <p>Evaluation criterion (e.g., 'f1', 'macro_f1').</p> required <code>tuning</code> <code>Optional[str]</code> <p>Tuning method, either 'holdout' or 'cv'. Defaults to None.</p> <code>None</code> <code>hpo</code> <code>Optional[str]</code> <p>Hyperparameter optimization type, either 'rs' or 'hebo'. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>classification</code> <code>str</code> <p>Type of classification, either 'binary' or 'multiclass'.</p> <code>criterion</code> <code>str</code> <p>Evaluation criterion for model performance.</p> <code>tuning</code> <code>Optional[str]</code> <p>Tuning method ('holdout' or 'cv').</p> <code>hpo</code> <code>Optional[str]</code> <p>Type of hyperparameter optimization ('rs' or 'hebo').</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the classification, criterion, or tuning method is invalid.</p> Example <pre><code>validator = BaseValidator(classification=\"binary\", criterion=\"f1\")\nprint(validator.criterion)\n</code></pre> Source code in <code>periomod/base.py</code> <pre><code>class BaseValidator(BaseConfig):\n    \"\"\"Base class for initializing classification, criterion, tuning, and HPO.\n\n    This class extends `BaseConfig` and validates classification types, evaluation\n    criteria, and tuning methods.\n\n    Inherits:\n        - `BaseLoader`: Provides loading and saving capabilities for processed data.\n\n    Args:\n        classification (str): Type of classification, either 'binary' or 'multiclass'.\n        criterion (str): Evaluation criterion (e.g., 'f1', 'macro_f1').\n        tuning (Optional[str], optional): Tuning method, either 'holdout' or 'cv'.\n            Defaults to None.\n        hpo (Optional[str], optional): Hyperparameter optimization type, either 'rs' or\n            'hebo'. Defaults to None.\n\n    Attributes:\n        classification (str): Type of classification, either 'binary' or 'multiclass'.\n        criterion (str): Evaluation criterion for model performance.\n        tuning (Optional[str]): Tuning method ('holdout' or 'cv').\n        hpo (Optional[str]): Type of hyperparameter optimization ('rs' or 'hebo').\n\n    Raises:\n        ValueError: If the classification, criterion, or tuning method is invalid.\n\n    Example:\n        ```\n        validator = BaseValidator(classification=\"binary\", criterion=\"f1\")\n        print(validator.criterion)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        classification: str,\n        criterion: str,\n        tuning: Optional[str] = None,\n        hpo: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Initializes BaseValidator method for with validation functions for inputs.\"\"\"\n        super().__init__()\n        self.classification = classification\n        self.criterion = criterion\n        self.hpo = hpo\n        self.tuning = tuning\n        self._validate_classification()\n        self._validate_hpo()\n        self._validate_criterion()\n        self._validate_tuning()\n\n    def _validate_classification(self) -&gt; None:\n        \"\"\"Validates the classification type for the model.\n\n        Raises:\n            ValueError: If `self.classification` is not 'binary' or 'multiclass'.\n        \"\"\"\n        if self.classification.lower().strip() not in [\"binary\", \"multiclass\"]:\n            raise ValueError(\n                f\"{self.classification} is an invalid classification type. \"\n                \"Choose 'binary' or 'multiclass'.\"\n            )\n\n    def _validate_hpo(self) -&gt; None:\n        \"\"\"Validates the hyperparameter optimization (HPO) type.\n\n        Raises:\n            ValueError: If `self.hpo` is not None, 'rs', or 'hebo'.\n        \"\"\"\n        if self.hpo not in [None, \"rs\", \"hebo\"]:\n            raise ValueError(\n                f\"{self.hpo} is an unsupported HPO type. Choose 'rs' or 'hebo'.\"\n            )\n\n    def _validate_criterion(self) -&gt; None:\n        \"\"\"Validates the evaluation criterion for model performance.\n\n        Raises:\n            ValueError: If `self.criterion` is not a supported evaluation metric.\n        \"\"\"\n        if self.criterion not in [\"f1\", \"macro_f1\", \"brier_score\"]:\n            raise ValueError(\n                \"Unsupported criterion. Choose 'f1', 'macro_f1', or 'brier_score'.\"\n            )\n\n    def _validate_tuning(self) -&gt; None:\n        \"\"\"Validates the tuning method for hyperparameter optimization.\n\n        Raises:\n            ValueError: If `self.tuning` is not None, 'holdout', or 'cv'.\n        \"\"\"\n        if self.tuning not in [None, \"holdout\", \"cv\"]:\n            raise ValueError(\n                \"Unsupported tuning method. Choose either 'holdout' or 'cv'.\"\n            )\n</code></pre>"},{"location":"reference/base/basevalidator/#periomod.base.BaseValidator.__init__","title":"<code>__init__(classification, criterion, tuning=None, hpo=None)</code>","text":"<p>Initializes BaseValidator method for with validation functions for inputs.</p> Source code in <code>periomod/base.py</code> <pre><code>def __init__(\n    self,\n    classification: str,\n    criterion: str,\n    tuning: Optional[str] = None,\n    hpo: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Initializes BaseValidator method for with validation functions for inputs.\"\"\"\n    super().__init__()\n    self.classification = classification\n    self.criterion = criterion\n    self.hpo = hpo\n    self.tuning = tuning\n    self._validate_classification()\n    self._validate_hpo()\n    self._validate_criterion()\n    self._validate_tuning()\n</code></pre>"},{"location":"reference/base/patient/","title":"Patient","text":"<p>Contains patient-level data along with dental health information for each tooth.</p> <p>This dataclass encapsulates patient demographic and health information, along with detailed data about each tooth and its associated sides. It serves as a structured container for organizing patient records in dental health applications.</p> <p>Attributes:</p> Name Type Description <code>age</code> <code>int</code> <p>The age of the patient in years.</p> <code>gender</code> <code>int</code> <p>Gender code for the patient (e.g., 0 for female, 1 for male).</p> <code>bodymassindex</code> <code>float</code> <p>Body Mass Index (BMI) of the patient.</p> <code>periofamilyhistory</code> <code>int</code> <p>Indicator of family history with periodontal disease.</p> <code>diabetes</code> <code>int</code> <p>Diabetes status, where 0 indicates no diabetes and 1 indicates diabetes.</p> <code>smokingtype</code> <code>int</code> <p>Type of smoking habit (e.g., 0 for non-smoker, 1 for occasional, 2 for frequent).</p> <code>cigarettenumber</code> <code>int</code> <p>Number of cigarettes smoked per day.</p> <code>antibiotictreatment</code> <code>int</code> <p>Indicator of antibiotic treatment history, where 0 means no treatment and 1 indicates treatment.</p> <code>stresslvl</code> <code>int</code> <p>Stress level rating on a scale (e.g., 0 to 3).</p> <code>teeth</code> <code>List[Tooth]</code> <p>A list of <code>Tooth</code> instances containing specific tooth data, where each tooth may have up to 6 sides with separate health metrics.</p> Example <pre><code>patient = Patient(\n    age=45,\n    gender=1,\n    bodymassindex=23.5,\n    periofamilyhistory=1,\n    diabetes=0,\n    smokingtype=2,\n    cigarettenumber=10,\n    antibiotictreatment=0,\n    stresslvl=2,\n    teeth=[\n        Tooth(\n            tooth=11,\n            toothtype=1,\n            rootnumber=1,\n            mobility=1,\n            restoration=0,\n            percussion=0,\n            sensitivity=1,\n            sides=[\n                Side(\n                    furcationbaseline=1,\n                    side=1,\n                    pdbaseline=2,\n                    recbaseline=2,\n                    plaque=1,\n                    bop=1\n                    ),\n                Side(\n                    furcationbaseline=2,\n                    side=2,\n                    pdbaseline=3,\n                    recbaseline=3,\n                    plaque=1,\n                    bop=1\n                    ),\n                # Additional sides can be added similarly\n            ]\n        ),\n        Tooth(\n            tooth=18,\n            toothtype=3,\n            rootnumber=2,\n            mobility=0,\n            restoration=1,\n            percussion=1,\n            sensitivity=0,\n            sides=[\n                Side(\n                    furcationbaseline=3,\n                    side=1,\n                    pdbaseline=4,\n                    recbaseline=5,\n                    plaque=2,\n                    bop=0\n                    ),\n                # Additional sides can be added similarly\n            ]\n        )\n    ]\n)\n</code></pre> Source code in <code>periomod/base.py</code> <pre><code>@dataclass\nclass Patient:\n    \"\"\"Contains patient-level data along with dental health information for each tooth.\n\n    This dataclass encapsulates patient demographic and health information, along with\n    detailed data about each tooth and its associated sides. It serves as a structured\n    container for organizing patient records in dental health applications.\n\n    Attributes:\n        age (int): The age of the patient in years.\n        gender (int): Gender code for the patient (e.g., 0 for female, 1 for male).\n        bodymassindex (float): Body Mass Index (BMI) of the patient.\n        periofamilyhistory (int): Indicator of family history with periodontal disease.\n        diabetes (int): Diabetes status, where 0 indicates no diabetes and 1 indicates\n            diabetes.\n        smokingtype (int): Type of smoking habit (e.g., 0 for non-smoker, 1 for\n            occasional, 2 for frequent).\n        cigarettenumber (int): Number of cigarettes smoked per day.\n        antibiotictreatment (int): Indicator of antibiotic treatment history, where 0\n            means no treatment and 1 indicates treatment.\n        stresslvl (int): Stress level rating on a scale (e.g., 0 to 3).\n        teeth (List[Tooth]): A list of `Tooth` instances containing specific tooth\n            data, where each tooth may have up to 6 sides with separate health metrics.\n\n    Example:\n        ```\n        patient = Patient(\n            age=45,\n            gender=1,\n            bodymassindex=23.5,\n            periofamilyhistory=1,\n            diabetes=0,\n            smokingtype=2,\n            cigarettenumber=10,\n            antibiotictreatment=0,\n            stresslvl=2,\n            teeth=[\n                Tooth(\n                    tooth=11,\n                    toothtype=1,\n                    rootnumber=1,\n                    mobility=1,\n                    restoration=0,\n                    percussion=0,\n                    sensitivity=1,\n                    sides=[\n                        Side(\n                            furcationbaseline=1,\n                            side=1,\n                            pdbaseline=2,\n                            recbaseline=2,\n                            plaque=1,\n                            bop=1\n                            ),\n                        Side(\n                            furcationbaseline=2,\n                            side=2,\n                            pdbaseline=3,\n                            recbaseline=3,\n                            plaque=1,\n                            bop=1\n                            ),\n                        # Additional sides can be added similarly\n                    ]\n                ),\n                Tooth(\n                    tooth=18,\n                    toothtype=3,\n                    rootnumber=2,\n                    mobility=0,\n                    restoration=1,\n                    percussion=1,\n                    sensitivity=0,\n                    sides=[\n                        Side(\n                            furcationbaseline=3,\n                            side=1,\n                            pdbaseline=4,\n                            recbaseline=5,\n                            plaque=2,\n                            bop=0\n                            ),\n                        # Additional sides can be added similarly\n                    ]\n                )\n            ]\n        )\n        ```\n    \"\"\"\n\n    age: int\n    gender: int\n    bodymassindex: float\n    periofamilyhistory: int\n    diabetes: int\n    smokingtype: int\n    cigarettenumber: int\n    antibiotictreatment: int\n    stresslvl: int\n    teeth: List[Tooth] = field(default_factory=list)\n</code></pre>"},{"location":"reference/base/patient_to_df/","title":"patient_to_df","text":"<p>Converts a Patient instance into a DataFrame suitable for prediction.</p> <p>This function takes a <code>Patient</code> dataclass instance and flattens its attributes along with nested <code>Tooth</code> and <code>Side</code> instances to generate a DataFrame. Each row in the DataFrame corresponds to a side of a tooth, with all relevant patient, tooth, and side attributes in a single row.</p> <p>Parameters:</p> Name Type Description Default <code>patient</code> <code>Patient</code> <p>The Patient dataclass instance.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame where each row represents a tooth side.</p> Example <pre><code>patient = Patient(..)\npatient_data = patient_to_df(patient=patient)\n</code></pre> Source code in <code>periomod/base.py</code> <pre><code>def patient_to_df(patient: Patient) -&gt; pd.DataFrame:\n    \"\"\"Converts a Patient instance into a DataFrame suitable for prediction.\n\n    This function takes a `Patient` dataclass instance and flattens its attributes\n    along with nested `Tooth` and `Side` instances to generate a DataFrame. Each row\n    in the DataFrame corresponds to a side of a tooth, with all relevant patient,\n    tooth, and side attributes in a single row.\n\n    Args:\n        patient (Patient): The Patient dataclass instance.\n\n    Returns:\n        pd.DataFrame: DataFrame where each row represents a tooth side.\n\n    Example:\n        ```\n        patient = Patient(..)\n        patient_data = patient_to_df(patient=patient)\n        ```\n    \"\"\"\n    rows = []\n    patient_dict = asdict(patient)\n\n    for tooth in patient_dict[\"teeth\"]:\n        for side in tooth[\"sides\"]:\n            data = {\n                **{k: v for k, v in patient_dict.items() if k != \"teeth\"},\n                **{k: v for k, v in tooth.items() if k != \"sides\"},\n                **side,\n            }\n            rows.append(data)\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/base/side/","title":"Side","text":"<p>Represents a single side of a tooth with relevant features.</p> <p>Attributes:</p> Name Type Description <code>furcationbaseline</code> <code>Optional[int]</code> <p>Baseline furcation measurement, if available.</p> <code>side</code> <code>int</code> <p>Identifier for the side of the tooth.</p> <code>pdbaseline</code> <code>Optional[int]</code> <p>Baseline probing depth measurement.</p> <code>recbaseline</code> <code>Optional[int]</code> <p>Baseline recession measurement.</p> <code>plaque</code> <code>Optional[int]</code> <p>Plaque presence status.</p> <code>bop</code> <code>Optional[int]</code> <p>Bleeding on probing status.</p> Example <pre><code>side_1 = Side(\n    furcationbaseline=1,\n    side=1,\n    pdbaseline=2,\n    recbaseline=2,\n    plaque=1,\n    bop=1\n)\n</code></pre> Source code in <code>periomod/base.py</code> <pre><code>@dataclass\nclass Side:\n    \"\"\"Represents a single side of a tooth with relevant features.\n\n    Attributes:\n        furcationbaseline (Optional[int]): Baseline furcation measurement, if available.\n        side (int): Identifier for the side of the tooth.\n        pdbaseline (Optional[int]): Baseline probing depth measurement.\n        recbaseline (Optional[int]): Baseline recession measurement.\n        plaque (Optional[int]): Plaque presence status.\n        bop (Optional[int]): Bleeding on probing status.\n\n    Example:\n        ```\n        side_1 = Side(\n            furcationbaseline=1,\n            side=1,\n            pdbaseline=2,\n            recbaseline=2,\n            plaque=1,\n            bop=1\n        )\n        ```\n    \"\"\"\n\n    furcationbaseline: Optional[int]\n    side: int\n    pdbaseline: Optional[int]\n    recbaseline: Optional[int]\n    plaque: Optional[int]\n    bop: Optional[int]\n</code></pre>"},{"location":"reference/base/tooth/","title":"Tooth","text":"<p>Represents a tooth with specific features and associated sides.</p> <p>Attributes:</p> Name Type Description <code>tooth</code> <code>int</code> <p>Identifier number for the tooth.</p> <code>toothtype</code> <code>int</code> <p>Type classification of the tooth (e.g., molar, incisor).</p> <code>rootnumber</code> <code>int</code> <p>Count of roots associated with the tooth.</p> <code>mobility</code> <code>Optional[int]</code> <p>Mobility status of the tooth.</p> <code>restoration</code> <code>Optional[int]</code> <p>Restoration status of the tooth.</p> <code>percussion</code> <code>Optional[int]</code> <p>Percussion sensitivity, if applicable.</p> <code>sensitivity</code> <code>Optional[int]</code> <p>Sensitivity status of the tooth.</p> <code>sides</code> <code>List[Side]</code> <p>Collection of <code>Side</code> instances for each side of the tooth.</p> Example <pre><code>side_1 = Side(\n    furcationbaseline=1, side=1, pdbaseline=2, recbaseline=2, plaque=1, bop=1\n    )\nside_2 = Side(\n    furcationbaseline=2, side=2, pdbaseline=3, recbaseline=3, plaque=1, bop=0\n    )\n\ntooth = Tooth(\n    tooth=11,\n    toothtype=2,\n    rootnumber=1,\n    mobility=1,\n    restoration=0,\n    percussion=0,\n    sensitivity=1,\n    sides=[side_1, side_2]\n)\n</code></pre> Source code in <code>periomod/base.py</code> <pre><code>@dataclass\nclass Tooth:\n    \"\"\"Represents a tooth with specific features and associated sides.\n\n    Attributes:\n        tooth (int): Identifier number for the tooth.\n        toothtype (int): Type classification of the tooth (e.g., molar, incisor).\n        rootnumber (int): Count of roots associated with the tooth.\n        mobility (Optional[int]): Mobility status of the tooth.\n        restoration (Optional[int]): Restoration status of the tooth.\n        percussion (Optional[int]): Percussion sensitivity, if applicable.\n        sensitivity (Optional[int]): Sensitivity status of the tooth.\n        sides (List[Side]): Collection of `Side` instances for each side of the tooth.\n\n    Example:\n        ```\n        side_1 = Side(\n            furcationbaseline=1, side=1, pdbaseline=2, recbaseline=2, plaque=1, bop=1\n            )\n        side_2 = Side(\n            furcationbaseline=2, side=2, pdbaseline=3, recbaseline=3, plaque=1, bop=0\n            )\n\n        tooth = Tooth(\n            tooth=11,\n            toothtype=2,\n            rootnumber=1,\n            mobility=1,\n            restoration=0,\n            percussion=0,\n            sensitivity=1,\n            sides=[side_1, side_2]\n        )\n        ```\n    \"\"\"\n\n    tooth: int\n    toothtype: int\n    rootnumber: int\n    mobility: Optional[int]\n    restoration: Optional[int]\n    percussion: Optional[int]\n    sensitivity: Optional[int]\n    sides: List[Side] = field(default_factory=list)\n</code></pre>"},{"location":"reference/benchmarking/","title":"periomod.benchmarking Overview","text":"<p>The <code>periomod.benchmarking</code> module provides classes and tools for managing and evaluating benchmarks across different model and data configurations.</p>"},{"location":"reference/benchmarking/#available-components","title":"Available Components","text":"Component Description Baseline Class for model baselines without tuning. BaseExperiment Base class defining the structure for experiments. Experiment Class for handling benchmarking experiments. BaseBenchmark Core class for managing benchmark runs and storing results. Benchmarker Handles model benchmarking based on predefined criteria."},{"location":"reference/benchmarking/basebenchmark/","title":"BaseBenchmark","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Base class for benchmarking models on specified tasks with various settings.</p> <p>This class initializes common parameters for benchmarking, including task specifications, encoding and sampling methods, tuning strategies, and model evaluation criteria.</p> Inherits <ul> <li><code>BaseConfig</code>: Base configuration class providing configuration loading.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task for evaluation (pocketclosure', 'pocketclosureinf', 'improvement', or 'pdgrouprevaluation'.).</p> required <code>learners</code> <code>List[str]</code> <p>List of models or algorithms to benchmark, including 'xgb', 'rf', 'lr' or 'mlp'.</p> required <code>tuning_methods</code> <code>List[str]</code> <p>List of tuning methods for model training, such as 'holdout' or 'cv'.</p> required <code>hpo_methods</code> <code>List[str]</code> <p>Hyperparameter optimization strategies to apply, includes 'rs' and 'hebo'.</p> required <code>criteria</code> <code>List[str]</code> <p>List of evaluation criteria ('f1', 'macro_f1', 'brier_score').</p> required <code>encodings</code> <code>List[str]</code> <p>Encoding types to transform categorical features, can either be 'one_hot' or 'target' encoding.</p> required <code>sampling</code> <code>Optional[List[Union[str, None]]]</code> <p>Sampling strategies to handle class imbalance, options include None, 'upsampling', 'downsampling', or 'smote'.</p> required <code>factor</code> <code>Optional[float]</code> <p>Factor specifying the amount of sampling to apply during resampling, if applicable.</p> required <code>n_configs</code> <code>int</code> <p>Number of configurations to evaluate in hyperparameter tuning.</p> required <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs to use for processing; set to -1 to utilize all available cores.</p> required <code>cv_folds</code> <code>int</code> <p>Number of cross-validation folds for model training. Defaults to None.</p> required <code>racing_folds</code> <code>Optional[int]</code> <p>Number of racing folds to use in Random Search (rs) for optimized tuning.</p> required <code>test_seed</code> <code>int</code> <p>Random seed for reproducible train-test splits.</p> required <code>test_size</code> <code>float</code> <p>Fraction of the dataset to allocate to test set.</p> required <code>val_size</code> <code>float</code> <p>Fraction of the dataset to allocate to validation in a holdout setup.</p> required <code>cv_seed</code> <code>int</code> <p>Seed for cross-validation splitting.</p> required <code>mlp_flag</code> <code>bool</code> <p>If True, enables Multi-Layer Perceptron (MLP) training with early stopping.</p> required <code>threshold_tuning</code> <code>bool</code> <p>Enables decision threshold tuning for binary classification when optimizing for 'f1'.</p> required <code>verbose</code> <code>bool</code> <p>Enables detailed logging of processes if set to True.</p> required <code>path</code> <code>Path</code> <p>Directory path where processed data will be stored.</p> required <p>Attributes:</p> Name Type Description <code>task</code> <code>str</code> <p>Task used for model classification or regression evaluation.</p> <code>learners</code> <code>List[str]</code> <p>Selected models or algorithms for benchmarking.</p> <code>tuning_methods</code> <code>List[str]</code> <p>List of model tuning approaches.</p> <code>hpo_methods</code> <code>List[str]</code> <p>Hyperparameter optimization techniques to apply.</p> <code>criteria</code> <code>List[str]</code> <p>Criteria used to evaluate model performance.</p> <code>encodings</code> <code>List[str]</code> <p>Encoding methods applied to categorical features.</p> <code>sampling</code> <code>Optional[List[Union[str, None]]]</code> <p>Sampling strategies employed to address class imbalance.</p> <code>factor</code> <code>Optional[float]</code> <p>Specifies the degree of sampling applied within the chosen strategy.</p> <code>n_configs</code> <code>int</code> <p>Number of configurations assessed during hyperparameter optimization.</p> <code>n_jobs</code> <code>int</code> <p>Number of parallel processes for model training and evaluation.</p> <code>cv_folds</code> <code>int</code> <p>Number of cross-validation folds for model training.</p> <code>racing_folds</code> <code>Optional[int]</code> <p>Racing folds used in tuning with cross-validation and random search..</p> <code>test_seed</code> <code>int</code> <p>Seed for consistent test-train splitting.</p> <code>test_size</code> <code>float</code> <p>Proportion of the data set aside for testing.</p> <code>val_size</code> <code>float</code> <p>Proportion of data allocated to validation in holdout tuning.</p> <code>cv_seed</code> <code>int</code> <p>Seed for cross-validation splitting.</p> <code>mlp_flag</code> <code>bool</code> <p>Flag for MLP training with early stopping.</p> <code>threshold_tuning</code> <code>bool</code> <p>Enables threshold adjustment for optimizing F1 in binary classification tasks.</p> <code>verbose</code> <code>bool</code> <p>Flag to enable detailed logging during training and evaluation.</p> <code>path</code> <code>Path</code> <p>Path where processed data is saved.</p> Source code in <code>periomod/benchmarking/_basebenchmark.py</code> <pre><code>class BaseBenchmark(BaseConfig):\n    \"\"\"Base class for benchmarking models on specified tasks with various settings.\n\n    This class initializes common parameters for benchmarking, including task\n    specifications, encoding and sampling methods, tuning strategies, and model\n    evaluation criteria.\n\n    Inherits:\n        - `BaseConfig`: Base configuration class providing configuration loading.\n\n    Args:\n        task (str): Task for evaluation (pocketclosure', 'pocketclosureinf',\n            'improvement', or 'pdgrouprevaluation'.).\n        learners (List[str]): List of models or algorithms to benchmark,\n            including 'xgb', 'rf', 'lr' or 'mlp'.\n        tuning_methods (List[str]): List of tuning methods for model training,\n            such as 'holdout' or 'cv'.\n        hpo_methods (List[str]): Hyperparameter optimization strategies to apply,\n            includes 'rs' and 'hebo'.\n        criteria (List[str]): List of evaluation criteria ('f1', 'macro_f1',\n            'brier_score').\n        encodings (List[str]): Encoding types to transform categorical features,\n            can either be 'one_hot' or 'target' encoding.\n        sampling (Optional[List[Union[str, None]]]): Sampling strategies to handle\n            class imbalance, options include None, 'upsampling', 'downsampling', or\n            'smote'.\n        factor (Optional[float]): Factor specifying the amount of sampling to apply\n            during resampling, if applicable.\n        n_configs (int): Number of configurations to evaluate in hyperparameter tuning.\n        n_jobs (int): Number of parallel jobs to use for processing; set\n            to -1 to utilize all available cores.\n        cv_folds (int): Number of cross-validation folds for model\n            training. Defaults to None.\n        racing_folds (Optional[int]): Number of racing folds to use in Random Search\n            (rs) for optimized tuning.\n        test_seed (int): Random seed for reproducible train-test splits.\n        test_size (float): Fraction of the dataset to allocate to test set.\n        val_size (float): Fraction of the dataset to allocate to validation\n            in a holdout setup.\n        cv_seed (int): Seed for cross-validation splitting.\n        mlp_flag (bool): If True, enables Multi-Layer Perceptron (MLP)\n            training with early stopping.\n        threshold_tuning (bool): Enables decision threshold tuning for binary\n            classification when optimizing for 'f1'.\n        verbose (bool): Enables detailed logging of processes if set to True.\n        path (Path): Directory path where processed data will be stored.\n\n    Attributes:\n        task (str): Task used for model classification or regression evaluation.\n        learners (List[str]): Selected models or algorithms for benchmarking.\n        tuning_methods (List[str]): List of model tuning approaches.\n        hpo_methods (List[str]): Hyperparameter optimization techniques to apply.\n        criteria (List[str]): Criteria used to evaluate model performance.\n        encodings (List[str]): Encoding methods applied to categorical features.\n        sampling (Optional[List[Union[str, None]]]): Sampling strategies employed\n            to address class imbalance.\n        factor (Optional[float]): Specifies the degree of sampling applied\n            within the chosen strategy.\n        n_configs (int): Number of configurations assessed during hyperparameter\n            optimization.\n        n_jobs (int): Number of parallel processes for model training\n            and evaluation.\n        cv_folds (int): Number of cross-validation folds for model training.\n        racing_folds (Optional[int]): Racing folds used in tuning with cross-validation\n            and random search..\n        test_seed (int): Seed for consistent test-train splitting.\n        test_size (float): Proportion of the data set aside for testing.\n        val_size (float): Proportion of data allocated to validation\n            in holdout tuning.\n        cv_seed (int): Seed for cross-validation splitting.\n        mlp_flag (bool): Flag for MLP training with early stopping.\n        threshold_tuning (bool): Enables threshold adjustment for optimizing F1\n            in binary classification tasks.\n        verbose (bool): Flag to enable detailed logging during training and evaluation.\n        path (Path): Path where processed data is saved.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        learners: List[str],\n        tuning_methods: List[str],\n        hpo_methods: List[str],\n        criteria: List[str],\n        encodings: List[str],\n        sampling: Optional[List[Union[str, None]]],\n        factor: Optional[float],\n        n_configs: int,\n        n_jobs: int,\n        cv_folds: Optional[int],\n        racing_folds: Optional[int],\n        test_seed: int,\n        test_size: float,\n        val_size: Optional[float],\n        cv_seed: Optional[int],\n        mlp_flag: Optional[bool],\n        threshold_tuning: Optional[bool],\n        verbose: bool,\n        path: Path,\n    ) -&gt; None:\n        \"\"\"Initialize the base benchmark class with common parameters.\"\"\"\n        super().__init__()\n        self.task = task\n        self.learners = learners\n        self.tuning_methods = tuning_methods\n        self.hpo_methods = hpo_methods\n        self.criteria = criteria\n        self.encodings = encodings\n        self.sampling = sampling\n        self.factor = factor\n        self.n_configs = n_configs\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n        self.cv_folds = cv_folds\n        self.racing_folds = racing_folds\n        self.test_seed = test_seed\n        self.test_size = test_size\n        self.val_size = val_size\n        self.cv_seed = cv_seed\n        self.mlp_flag = mlp_flag\n        self.threshold_tuning = threshold_tuning\n        self.path = path\n        self._validate_task()\n\n    def _validate_task(self) -&gt; None:\n        \"\"\"Validates the task type for the model.\n\n        Raises:\n            ValueError: If `self.task` is not one of the recognized task types.\n        \"\"\"\n        if self.task not in {\n            \"pocketclosure\",\n            \"pocketclosureinf\",\n            \"improvement\",\n            \"pdgrouprevaluation\",\n        }:\n            raise ValueError(\n                f\"Unknown task: {self.task}. Unable to determine classification.\"\n            )\n</code></pre>"},{"location":"reference/benchmarking/basebenchmark/#periomod.benchmarking.BaseBenchmark.__init__","title":"<code>__init__(task, learners, tuning_methods, hpo_methods, criteria, encodings, sampling, factor, n_configs, n_jobs, cv_folds, racing_folds, test_seed, test_size, val_size, cv_seed, mlp_flag, threshold_tuning, verbose, path)</code>","text":"<p>Initialize the base benchmark class with common parameters.</p> Source code in <code>periomod/benchmarking/_basebenchmark.py</code> <pre><code>def __init__(\n    self,\n    task: str,\n    learners: List[str],\n    tuning_methods: List[str],\n    hpo_methods: List[str],\n    criteria: List[str],\n    encodings: List[str],\n    sampling: Optional[List[Union[str, None]]],\n    factor: Optional[float],\n    n_configs: int,\n    n_jobs: int,\n    cv_folds: Optional[int],\n    racing_folds: Optional[int],\n    test_seed: int,\n    test_size: float,\n    val_size: Optional[float],\n    cv_seed: Optional[int],\n    mlp_flag: Optional[bool],\n    threshold_tuning: Optional[bool],\n    verbose: bool,\n    path: Path,\n) -&gt; None:\n    \"\"\"Initialize the base benchmark class with common parameters.\"\"\"\n    super().__init__()\n    self.task = task\n    self.learners = learners\n    self.tuning_methods = tuning_methods\n    self.hpo_methods = hpo_methods\n    self.criteria = criteria\n    self.encodings = encodings\n    self.sampling = sampling\n    self.factor = factor\n    self.n_configs = n_configs\n    self.n_jobs = n_jobs\n    self.verbose = verbose\n    self.cv_folds = cv_folds\n    self.racing_folds = racing_folds\n    self.test_seed = test_seed\n    self.test_size = test_size\n    self.val_size = val_size\n    self.cv_seed = cv_seed\n    self.mlp_flag = mlp_flag\n    self.threshold_tuning = threshold_tuning\n    self.path = path\n    self._validate_task()\n</code></pre>"},{"location":"reference/benchmarking/baseexperiment/","title":"BaseExperiment","text":"<p>               Bases: <code>BaseValidator</code>, <code>ABC</code></p> <p>Base class for experiment workflows with model benchmarking.</p> <p>This class provides a shared framework for setting up and running experiments with model training, resampling, tuning, and evaluation. It supports configurations for task-specific classification, tuning methods, hyperparameter optimization, and sampling strategies, providing core methods to set up tuning, training, and evaluation for different machine learning tasks.</p> Inherits <ul> <li><code>BaseValidator</code>: Validates instance-level variables and parameters.</li> <li><code>ABC</code>: Specifies abstract methods for subclasses to implement.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The preloaded dataset used for training and evaluation.</p> required <code>task</code> <code>str</code> <p>Task name, used to determine the classification type based on the Can be 'pocketclosure', 'pocketclosureinf', 'improvement', or 'pdgrouprevaluation'.</p> required <code>learner</code> <code>str</code> <p>Specifies the machine learning model or algorithm to use for evaluation, including 'xgb', 'rf', 'lr' or 'mlp'.</p> required <code>criterion</code> <code>str</code> <p>Evaluation criterion for model performance. Options are 'f1' and 'macro_f1' for F1 score and 'brier_score' for Brier Score.</p> required <code>encoding</code> <code>str</code> <p>Encoding type for categorical features. Choose between 'one_hot' or 'target' encoding based on model requirements.</p> required <code>tuning</code> <code>Optional[str]</code> <p>The tuning method to apply during model training, either 'holdout' or 'cv' for cross-validation.</p> required <code>hpo</code> <code>Optional[str]</code> <p>Hyperparameter optimization strategy. Options include 'rs' (Random Search) and 'hebo'.</p> required <code>sampling</code> <code>Optional[str]</code> <p>Sampling strategy to address class imbalance in the dataset. Includes None, 'upsampling', 'downsampling', and 'smote'.</p> required <code>factor</code> <code>Optional[float]</code> <p>Factor used during resampling, specifying the amount of class balancing to apply.</p> required <code>n_configs</code> <code>int</code> <p>Number of configurations to evaluate during hyperparameter tuning, used to limit the search space.</p> required <code>racing_folds</code> <code>Optional[int]</code> <p>Number of racing folds used during random search for efficient hyperparameter optimization.</p> required <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs to use for processing. Set to -1 to use all available cores.</p> required <code>cv_folds</code> <code>int</code> <p>Number of folds for cross-validation.</p> required <code>test_seed</code> <code>int</code> <p>Seed for random train-test split for reproducibility.</p> required <code>test_size</code> <code>float</code> <p>Proportion of data to use for testing.</p> required <code>val_size</code> <code>float</code> <p>Proportion of data to use for validation in a holdout strategy.</p> required <code>cv_seed</code> <code>int</code> <p>Seed for cross-validation splits for reproducibility.</p> required <code>mlp_flag</code> <code>Optional[bool]</code> <p>If True, enables training with a Multi-Layer Perceptron (MLP) with early stopping. Defaults to <code>self.mlp_training</code>.</p> required <code>threshold_tuning</code> <code>bool</code> <p>If True, tunes the decision threshold in binary classification to optimize for <code>f1</code> score.</p> required <code>verbose</code> <code>bool</code> <p>If True, enables detailed logging of the model training, tuning, and evaluation processes for better traceability.</p> required <p>Attributes:</p> Name Type Description <code>task</code> <code>str</code> <p>The task name, used to set the evaluation objective.</p> <code>classification</code> <code>str</code> <p>Classification type derived from the task ('binary' or 'multiclass') for configuring the evaluation.</p> <code>data</code> <code>DataFrame</code> <p>DataFrame containing the dataset for training, validation, and testing purposes.</p> <code>learner</code> <code>str</code> <p>The chosen machine learning model or algorithm for evaluation.</p> <code>encoding</code> <code>str</code> <p>Encoding type applied to categorical features, either 'one_hot' or 'target'.</p> <code>sampling</code> <code>str</code> <p>Resampling strategy used to address class imbalance in the dataset.</p> <code>factor</code> <code>float</code> <p>Resampling factor applied to balance classes as per the chosen sampling strategy.</p> <code>n_configs</code> <code>int</code> <p>Number of configurations evaluated during hyperparameter tuning.</p> <code>racing_folds</code> <code>int</code> <p>Number of racing folds applied during random search for efficient tuning.</p> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs used for model training and evaluation.</p> <code>cv_folds</code> <code>int</code> <p>Number of folds used for cross-validation.</p> <code>test_seed</code> <code>int</code> <p>Seed for splitting data into training and test sets, ensuring reproducibility.</p> <code>test_size</code> <code>float</code> <p>Proportion of the dataset assigned to the test split.</p> <code>val_size</code> <code>float</code> <p>Proportion of the dataset assigned to validation split in holdout validation.</p> <code>cv_seed</code> <code>int</code> <p>Seed for cross-validation splits to ensure consistency across runs.</p> <code>mlp_flag</code> <code>bool</code> <p>Enables training with a Multi-Layer Perceptron (MLP) and early stopping.</p> <code>threshold_tuning</code> <code>bool</code> <p>Enables tuning of the classification threshold in binary classification for optimizing the F1 score.</p> <code>verbose</code> <code>bool</code> <p>Controls the verbosity level of the output for detailed logs during training and evaluation.</p> <code>resampler</code> <code>Resampler</code> <p>Instance of the <code>Resampler</code> class for handling dataset resampling based on the specified strategy.</p> <code>trainer</code> <code>Trainer</code> <p>Instance of the <code>Trainer</code> class for managing the model training process.</p> <code>tuner</code> <code>Tuner</code> <p>Instance of the <code>Tuner</code> class used for performing hyperparameter optimization.</p> Abstract Method <ul> <li><code>perform_evaluation</code>: Abstract method to handle the model evaluation process.</li> </ul> Source code in <code>periomod/benchmarking/_basebenchmark.py</code> <pre><code>class BaseExperiment(BaseValidator, ABC):\n    \"\"\"Base class for experiment workflows with model benchmarking.\n\n    This class provides a shared framework for setting up and running\n    experiments with model training, resampling, tuning, and evaluation. It\n    supports configurations for task-specific classification, tuning methods,\n    hyperparameter optimization, and sampling strategies, providing core methods\n    to set up tuning, training, and evaluation for different machine learning\n    tasks.\n\n    Inherits:\n        - `BaseValidator`: Validates instance-level variables and parameters.\n        - `ABC`: Specifies abstract methods for subclasses to implement.\n\n    Args:\n        data (pd.DataFrame): The preloaded dataset used for training and evaluation.\n        task (str): Task name, used to determine the classification type based on the\n            Can be 'pocketclosure', 'pocketclosureinf', 'improvement', or\n            'pdgrouprevaluation'.\n        learner (str): Specifies the machine learning model or algorithm to use for\n            evaluation, including 'xgb', 'rf', 'lr' or 'mlp'.\n        criterion (str): Evaluation criterion for model performance. Options are\n            'f1' and 'macro_f1' for F1 score and 'brier_score' for Brier Score.\n        encoding (str): Encoding type for categorical features. Choose between\n            'one_hot' or 'target' encoding based on model requirements.\n        tuning (Optional[str]): The tuning method to apply during model training,\n            either 'holdout' or 'cv' for cross-validation.\n        hpo (Optional[str]): Hyperparameter optimization strategy. Options include\n            'rs' (Random Search) and 'hebo'.\n        sampling (Optional[str]): Sampling strategy to address class imbalance in\n            the dataset. Includes None, 'upsampling', 'downsampling', and 'smote'.\n        factor (Optional[float]): Factor used during resampling, specifying the\n            amount of class balancing to apply.\n        n_configs (int): Number of configurations to evaluate during hyperparameter\n            tuning, used to limit the search space.\n        racing_folds (Optional[int]): Number of racing folds used during random\n            search for efficient hyperparameter optimization.\n        n_jobs (int): Number of parallel jobs to use for processing.\n            Set to -1 to use all available cores.\n        cv_folds (int): Number of folds for cross-validation.\n        test_seed (int): Seed for random train-test split for reproducibility.\n        test_size (float): Proportion of data to use for testing.\n        val_size (float): Proportion of data to use for validation in a\n            holdout strategy.\n        cv_seed (int): Seed for cross-validation splits for reproducibility.\n        mlp_flag (Optional[bool]): If True, enables training with a Multi-Layer\n            Perceptron (MLP) with early stopping. Defaults to `self.mlp_training`.\n        threshold_tuning (bool): If True, tunes the decision threshold in binary\n            classification to optimize for `f1` score.\n        verbose (bool): If True, enables detailed logging of the model training,\n            tuning, and evaluation processes for better traceability.\n\n    Attributes:\n        task (str): The task name, used to set the evaluation objective.\n        classification (str): Classification type derived from the task ('binary'\n            or 'multiclass') for configuring the evaluation.\n        data (pd.DataFrame): DataFrame containing the dataset for training, validation,\n            and testing purposes.\n        learner (str): The chosen machine learning model or algorithm for evaluation.\n        encoding (str): Encoding type applied to categorical features, either\n            'one_hot' or 'target'.\n        sampling (str): Resampling strategy used to address class imbalance in\n            the dataset.\n        factor (float): Resampling factor applied to balance classes as per\n            the chosen sampling strategy.\n        n_configs (int): Number of configurations evaluated during hyperparameter\n            tuning.\n        racing_folds (int): Number of racing folds applied during random search for\n            efficient tuning.\n        n_jobs (int): Number of parallel jobs used for model training and evaluation.\n        cv_folds (int): Number of folds used for cross-validation.\n        test_seed (int): Seed for splitting data into training and test sets,\n            ensuring reproducibility.\n        test_size (float): Proportion of the dataset assigned to the test split.\n        val_size (float): Proportion of the dataset assigned to validation split in\n            holdout validation.\n        cv_seed (int): Seed for cross-validation splits to ensure consistency across\n            runs.\n        mlp_flag (bool): Enables training with a Multi-Layer Perceptron (MLP) and\n            early stopping.\n        threshold_tuning (bool): Enables tuning of the classification threshold\n            in binary classification for optimizing the F1 score.\n        verbose (bool): Controls the verbosity level of the output for detailed\n            logs during training and evaluation.\n        resampler (Resampler): Instance of the `Resampler` class for handling\n            dataset resampling based on the specified strategy.\n        trainer (Trainer): Instance of the `Trainer` class for managing the model\n            training process.\n        tuner (Tuner): Instance of the `Tuner` class used for performing\n            hyperparameter optimization.\n\n\n    Abstract Method:\n        - `perform_evaluation`: Abstract method to handle the model evaluation process.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: pd.DataFrame,\n        task: str,\n        learner: str,\n        criterion: str,\n        encoding: str,\n        tuning: Optional[str],\n        hpo: Optional[str],\n        sampling: Optional[str],\n        factor: Optional[float],\n        n_configs: int,\n        racing_folds: Optional[int],\n        n_jobs: int,\n        cv_folds: Optional[int],\n        test_seed: int,\n        test_size: float,\n        val_size: Optional[float],\n        cv_seed: Optional[int],\n        mlp_flag: Optional[bool],\n        threshold_tuning: Optional[bool],\n        verbose: bool,\n    ) -&gt; None:\n        \"\"\"Initialize the Experiment class with tuning parameters.\"\"\"\n        self.task = task\n        classification = self._determine_classification()\n        super().__init__(\n            classification=classification, criterion=criterion, tuning=tuning, hpo=hpo\n        )\n        self.data = data\n        self.learner = learner\n        self.encoding = encoding\n        self.sampling = sampling\n        self.factor = factor\n        self.n_configs = n_configs\n        self.racing_folds = racing_folds\n        self.n_jobs = n_jobs\n        self.cv_folds = cv_folds\n        self.test_seed = test_seed\n        self.test_size = test_size\n        self.val_size = val_size\n        self.cv_seed = cv_seed\n        self.mlp_flag = mlp_flag\n        self.threshold_tuning = threshold_tuning\n        self.verbose = verbose\n        self.resampler = Resampler(self.classification, self.encoding)\n        self.trainer = Trainer(\n            self.classification,\n            self.criterion,\n            tuning=self.tuning,\n            hpo=self.hpo,\n            mlp_training=self.mlp_flag,\n            threshold_tuning=self.threshold_tuning,\n        )\n        self.tuner = self._initialize_tuner()\n\n    def _determine_classification(self) -&gt; str:\n        \"\"\"Determine classification type based on the task name.\n\n        Returns:\n            str: The classification type ('binary' or 'multiclass').\n\n        Raises:\n            ValueError: If `self.task` is unknown and classification cannot be\n            determined.\n        \"\"\"\n        if self.task in [\"pocketclosure\", \"pocketclosureinf\", \"improvement\"]:\n            return \"binary\"\n        elif self.task == \"pdgrouprevaluation\":\n            return \"multiclass\"\n        else:\n            raise ValueError(\n                f\"Unknown task: {self.task}. Unable to determine classification.\"\n            )\n\n    def _initialize_tuner(self):\n        \"\"\"Initialize the appropriate tuner based on the HPO method.\n\n        Returns:\n            RandomSearchTuner | HEBOTuner: The initialized tuner instance.\n\n        Raises:\n            ValueError: If `self.hpo` is not a supported method ('rs' or 'hebo').\n        \"\"\"\n        if self.hpo == \"rs\":\n            return RandomSearchTuner(\n                classification=self.classification,\n                criterion=self.criterion,\n                tuning=self.tuning,\n                hpo=self.hpo,\n                n_configs=self.n_configs,\n                n_jobs=self.n_jobs,\n                verbose=self.verbose,\n                trainer=self.trainer,\n                mlp_training=self.mlp_flag,\n                threshold_tuning=self.threshold_tuning,\n            )\n        elif self.hpo == \"hebo\":\n            return HEBOTuner(\n                classification=self.classification,\n                criterion=self.criterion,\n                tuning=self.tuning,\n                hpo=self.hpo,\n                n_configs=self.n_configs,\n                n_jobs=self.n_jobs,\n                verbose=self.verbose,\n                trainer=self.trainer,\n                mlp_training=self.mlp_flag,\n                threshold_tuning=self.threshold_tuning,\n            )\n        else:\n            raise ValueError(f\"Unsupported HPO method: {self.hpo}\")\n\n    def _train_final_model(\n        self, final_model_tuple: Tuple[str, Dict, Optional[float]]\n    ) -&gt; dict:\n        \"\"\"Helper method to train the final model with best parameters.\n\n        Args:\n            final_model_tuple (Tuple[str, Dict, Optional[float]]): A tuple containing\n                the learner name, best hyperparameters, and an optional best threshold.\n\n        Returns:\n            dict: A dictionary containing the trained model and its evaluation metrics.\n        \"\"\"\n        return self.trainer.train_final_model(\n            df=self.data,\n            resampler=self.resampler,\n            model=final_model_tuple,\n            sampling=self.sampling,\n            factor=self.factor,\n            n_jobs=self.n_jobs,\n            seed=self.test_seed,\n            test_size=self.test_size,\n            verbose=self.verbose,\n        )\n\n    @abstractmethod\n    def perform_evaluation(self) -&gt; dict:\n        \"\"\"Perform model evaluation and return final metrics.\"\"\"\n\n    @abstractmethod\n    def _evaluate_holdout(self, train_df: pd.DataFrame) -&gt; dict:\n        \"\"\"Perform holdout validation and return the final model metrics.\n\n        Args:\n            train_df (pd.DataFrame): train df for holdout tuning.\n        \"\"\"\n\n    @abstractmethod\n    def _evaluate_cv(self) -&gt; dict:\n        \"\"\"Perform cross-validation and return the final model metrics.\"\"\"\n</code></pre>"},{"location":"reference/benchmarking/baseexperiment/#periomod.benchmarking.BaseExperiment.__init__","title":"<code>__init__(data, task, learner, criterion, encoding, tuning, hpo, sampling, factor, n_configs, racing_folds, n_jobs, cv_folds, test_seed, test_size, val_size, cv_seed, mlp_flag, threshold_tuning, verbose)</code>","text":"<p>Initialize the Experiment class with tuning parameters.</p> Source code in <code>periomod/benchmarking/_basebenchmark.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    task: str,\n    learner: str,\n    criterion: str,\n    encoding: str,\n    tuning: Optional[str],\n    hpo: Optional[str],\n    sampling: Optional[str],\n    factor: Optional[float],\n    n_configs: int,\n    racing_folds: Optional[int],\n    n_jobs: int,\n    cv_folds: Optional[int],\n    test_seed: int,\n    test_size: float,\n    val_size: Optional[float],\n    cv_seed: Optional[int],\n    mlp_flag: Optional[bool],\n    threshold_tuning: Optional[bool],\n    verbose: bool,\n) -&gt; None:\n    \"\"\"Initialize the Experiment class with tuning parameters.\"\"\"\n    self.task = task\n    classification = self._determine_classification()\n    super().__init__(\n        classification=classification, criterion=criterion, tuning=tuning, hpo=hpo\n    )\n    self.data = data\n    self.learner = learner\n    self.encoding = encoding\n    self.sampling = sampling\n    self.factor = factor\n    self.n_configs = n_configs\n    self.racing_folds = racing_folds\n    self.n_jobs = n_jobs\n    self.cv_folds = cv_folds\n    self.test_seed = test_seed\n    self.test_size = test_size\n    self.val_size = val_size\n    self.cv_seed = cv_seed\n    self.mlp_flag = mlp_flag\n    self.threshold_tuning = threshold_tuning\n    self.verbose = verbose\n    self.resampler = Resampler(self.classification, self.encoding)\n    self.trainer = Trainer(\n        self.classification,\n        self.criterion,\n        tuning=self.tuning,\n        hpo=self.hpo,\n        mlp_training=self.mlp_flag,\n        threshold_tuning=self.threshold_tuning,\n    )\n    self.tuner = self._initialize_tuner()\n</code></pre>"},{"location":"reference/benchmarking/baseexperiment/#periomod.benchmarking.BaseExperiment.perform_evaluation","title":"<code>perform_evaluation()</code>  <code>abstractmethod</code>","text":"<p>Perform model evaluation and return final metrics.</p> Source code in <code>periomod/benchmarking/_basebenchmark.py</code> <pre><code>@abstractmethod\ndef perform_evaluation(self) -&gt; dict:\n    \"\"\"Perform model evaluation and return final metrics.\"\"\"\n</code></pre>"},{"location":"reference/benchmarking/baseline/","title":"Baseline","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Evaluates baseline models on a given dataset.</p> <p>This class loads, preprocesses, and evaluates a set of baseline models on a specified dataset. The baseline models include a Random Forest, Logistic Regression, and a Dummy Classifier, which are trained and evaluated on split data, returning a summary of performance metrics for each model.</p> Inherits <ul> <li><code>BaseConfig</code>: Provides configuration settings for data processing.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task name used to determine the classification type.</p> required <code>encoding</code> <code>str</code> <p>Encoding type for categorical columns.</p> required <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 0.</p> <code>0</code> <code>lr_solver</code> <code>str</code> <p>Solver used by Logistic Regression. Defaults to 'saga'.</p> <code>'saga'</code> <code>dummy_strategy</code> <code>str</code> <p>Strategy for DummyClassifier, defaults to 'prior'.</p> <code>'prior'</code> <code>models</code> <code>List[Tuple[str, object]]</code> <p>List of models to benchmark. If not provided, default models are initialized.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs. Defaults to -1.</p> <code>-1</code> <code>path</code> <code>Path</code> <p>Path to the directory containing processed data files. Defaults to Path(\"data/processed/processed_data.csv\").</p> <code>Path('data/processed/processed_data.csv')</code> <p>Attributes:</p> Name Type Description <code>classification</code> <code>str</code> <p>Specifies classification type ('binary' or 'multiclass') based on the task.</p> <code>resampler</code> <code>Resampler</code> <p>Strategy for resampling data during training/testing split.</p> <code>dataloader</code> <code>ProcessedDataLoader</code> <p>Loader for processing and transforming the dataset.</p> <code>dummy_strategy</code> <code>str</code> <p>Strategy used by the DummyClassifier, default is 'prior'.</p> <code>lr_solver</code> <code>str</code> <p>Solver for Logistic Regression, default is 'saga'.</p> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility, default is 0.</p> <code>models</code> <code>List[Tuple[str, object]]</code> <p>List of models to benchmark, each represented as a tuple containing the model's name and the initialized model object.</p> <code>path</code> <code>Path</code> <p>Path to the directory containing processed data files.</p> <p>Methods:</p> Name Description <code>train_baselines</code> <p>Trains and returns baseline models with test data.</p> <code>baseline</code> <p>Trains and evaluates each model in the models list, returning a DataFrame with evaluation metrics.</p> Example <pre><code># Initialize baseline evaluation for pocket closure task\nbaseline = Baseline(\n    task=\"pocketclosure\",\n    encoding=\"one_hot\",\n    random_state=42,\n    lr_solver=\"saga\",\n    dummy_strategy=\"most_frequent\"\n)\n\n# Evaluate baseline models and display results\nresults_df = baseline.baseline()\nprint(results_df)\n</code></pre> Source code in <code>periomod/benchmarking/_baseline.py</code> <pre><code>class Baseline(BaseConfig):\n    \"\"\"Evaluates baseline models on a given dataset.\n\n    This class loads, preprocesses, and evaluates a set of baseline models on a\n    specified dataset. The baseline models include a Random Forest, Logistic\n    Regression, and a Dummy Classifier, which are trained and evaluated on\n    split data, returning a summary of performance metrics for each model.\n\n    Inherits:\n        - `BaseConfig`: Provides configuration settings for data processing.\n\n    Args:\n        task (str): Task name used to determine the classification type.\n        encoding (str): Encoding type for categorical columns.\n        random_state (int, optional): Random seed for reproducibility. Defaults to 0.\n        lr_solver (str, optional): Solver used by Logistic Regression. Defaults to\n            'saga'.\n        dummy_strategy (str, optional): Strategy for DummyClassifier, defaults to\n            'prior'.\n        models (List[Tuple[str, object]], optional): List of models to benchmark.\n            If not provided, default models are initialized.\n        n_jobs (int): Number of parallel jobs. Defaults to -1.\n        path (Path): Path to the directory containing processed data files.\n            Defaults to Path(\"data/processed/processed_data.csv\").\n\n    Attributes:\n        classification (str): Specifies classification type ('binary' or\n            'multiclass') based on the task.\n        resampler (Resampler): Strategy for resampling data during training/testing\n            split.\n        dataloader (ProcessedDataLoader): Loader for processing and transforming the\n            dataset.\n        dummy_strategy (str): Strategy used by the DummyClassifier, default is 'prior'.\n        lr_solver (str): Solver for Logistic Regression, default is 'saga'.\n        random_state (int): Random seed for reproducibility, default is 0.\n        models (List[Tuple[str, object]]): List of models to benchmark, each\n            represented as a tuple containing the model's name and the initialized\n            model object.\n        path (Path): Path to the directory containing processed data files.\n\n    Methods:\n        train_baselines: Trains and returns baseline models with test data.\n        baseline: Trains and evaluates each model in the models list, returning\n            a DataFrame with evaluation metrics.\n\n    Example:\n        ```\n        # Initialize baseline evaluation for pocket closure task\n        baseline = Baseline(\n            task=\"pocketclosure\",\n            encoding=\"one_hot\",\n            random_state=42,\n            lr_solver=\"saga\",\n            dummy_strategy=\"most_frequent\"\n        )\n\n        # Evaluate baseline models and display results\n        results_df = baseline.baseline()\n        print(results_df)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        encoding: str,\n        random_state: int = 0,\n        lr_solver: str = \"saga\",\n        dummy_strategy: str = \"prior\",\n        models: Union[List[Tuple[str, object]], None] = None,\n        n_jobs: int = -1,\n        path: Path = Path(\"data/processed/processed_data.csv\"),\n    ) -&gt; None:\n        \"\"\"Initializes the Baseline class with default or user-specified models.\n\n        Raises:\n            ValueError: If the task cannot be determined.\n        \"\"\"\n        if task in [\"pocketclosure\", \"pocketclosureinf\", \"improvement\"]:\n            self.classification = \"binary\"\n        elif task == \"pdgrouprevaluation\":\n            self.classification = \"multiclass\"\n        else:\n            raise ValueError(\n                f\"Unknown task: {task}. Unable to determine classification.\"\n            )\n\n        self.resampler = Resampler(\n            classification=self.classification, encoding=encoding\n        )\n        self.dataloader = ProcessedDataLoader(task=task, encoding=encoding)\n        self.dummy_strategy = dummy_strategy\n        self.lr_solver = lr_solver\n        self.random_state = random_state\n        self.path = path\n        self.default_models: Union[list[str], None]\n\n        if models is None:\n            self.models = [\n                (\n                    \"Random Forest\",\n                    RandomForestClassifier(\n                        n_jobs=n_jobs, random_state=self.random_state\n                    ),\n                ),\n                (\n                    \"Logistic Regression\",\n                    LogisticRegression(\n                        solver=self.lr_solver,\n                        random_state=self.random_state,\n                        n_jobs=n_jobs,\n                    ),\n                ),\n                (\n                    \"Dummy Classifier\",\n                    DummyClassifier(strategy=self.dummy_strategy),\n                ),\n            ]\n            self.default_models = [name for name, _ in self.models]\n        else:\n            self.models = models\n            self.default_models = None\n\n    @staticmethod\n    def _bss_helper(\n        results_df: pd.DataFrame, classification: str\n    ) -&gt; Tuple[pd.DataFrame, List[str]]:\n        \"\"\"Calculates Brier Skill Score (BSS) and determines column order.\n\n        Args:\n            results_df (pd.DataFrame): DataFrame containing evaluation metrics.\n            classification (str): Classification type ('binary' or 'multiclass').\n\n        Returns:\n            Tuple[pd.DataFrame, List[str]]: Updated DataFrame with BSS and column order.\n\n        Raises:\n            ValueError: If the classification type is not supported.\n        \"\"\"\n        if classification == \"binary\":\n            metric_column = \"Brier Score\"\n            column_order = column_order_binary\n        elif classification == \"multiclass\":\n            metric_column = \"Multiclass Brier Score\"\n            column_order = column_order_multiclass\n            if \"Class F1 Scores\" in results_df.columns:\n                results_df[\"Class F1 Scores\"] = results_df[\"Class F1 Scores\"].apply(\n                    lambda scores: [round(score, 4) for score in scores]\n                )\n        else:\n            raise ValueError(f\"Unsupported classification type: {classification}\")\n\n        if metric_column in results_df.columns:\n            dummy_brier = results_df.loc[\n                results_df[\"Model\"] == \"Dummy Classifier\", metric_column\n            ].iloc[0]\n            logreg_brier = results_df.loc[\n                results_df[\"Model\"] == \"Logistic Regression\", metric_column\n            ].iloc[0]\n            results_df[\"Brier Skill Score\"] = results_df.apply(\n                lambda row: _brier_skill_score(\n                    row, dummy_brier, logreg_brier, metric_column\n                ),\n                axis=1,\n            ).round(4)\n\n        return results_df, column_order\n\n    def train_baselines(\n        self,\n    ) -&gt; Tuple[Dict[Tuple[str, str], Any], pd.DataFrame, pd.Series]:\n        \"\"\"Trains each model in the models list and returns related data splits.\n\n        Returns:\n            Tuple:\n                - Dictionary containing trained models.\n                - Testing feature set (X_test).\n                - Testing labels (y_test).\n        \"\"\"\n        data = self.dataloader.load_data(path=self.path)\n        data = self.dataloader.transform_data(data=data)\n        train_df, test_df = self.resampler.split_train_test_df(\n            df=data, seed=self.random_state\n        )\n        X_train, y_train, X_test, y_test = self.resampler.split_x_y(\n            train_df=train_df, test_df=test_df\n        )\n\n        trained_models = {}\n        for model_name, model in self.models:\n            model.fit(X_train, y_train)\n            trained_models[(model_name, \"Baseline\")] = model\n\n        return trained_models, X_test, y_test\n\n    def baseline(self) -&gt; pd.DataFrame:\n        \"\"\"Trains and evaluates each model in the models list on the given dataset.\n\n        This method loads and transforms the dataset, splits it into training and\n        testing sets, and evaluates each model in the `self.models` list. Metrics\n        such as predictions and probabilities are computed and displayed.\n\n        Returns:\n            DataFrame: A DataFrame containing the evaluation metrics for each\n                baseline model, with model names as row indices.\n        \"\"\"\n        trained_models, X_test, y_test = self.train_baselines()\n        results = []\n\n        for model_name, model in self.models:\n            preds = trained_models[(model_name, \"Baseline\")].predict(X_test)\n            probs = (\n                get_probs(\n                    model=trained_models[(model_name, \"Baseline\")],\n                    classification=self.classification,\n                    X=X_test,\n                )\n                if hasattr(model, \"predict_proba\")\n                else None\n            )\n            metrics = final_metrics(\n                classification=self.classification,\n                y=y_test,\n                preds=preds,\n                probs=probs,\n            )\n            metrics[\"Model\"] = model_name\n            results.append(metrics)\n\n        results_df = pd.DataFrame(results).drop(\n            columns=[\"Best Threshold\"], errors=\"ignore\"\n        )\n\n        results_df, column_order = self._bss_helper(\n            results_df, classification=self.classification\n        )\n\n        existing_columns = [col for col in column_order if col in results_df.columns]\n        results_df = results_df[\n            existing_columns\n            + [col for col in results_df.columns if col not in existing_columns]\n        ].round(4)\n\n        if self.default_models is not None:\n            baseline_order = [\n                \"Dummy Classifier\",\n                \"Logistic Regression\",\n                \"Random Forest\",\n            ]\n            results_df[\"Model\"] = pd.Categorical(\n                results_df[\"Model\"], categories=baseline_order, ordered=True\n            )\n            results_df = results_df.sort_values(\"Model\").reset_index(drop=True)\n\n        else:\n            results_df = results_df.reset_index(drop=True)\n        pd.set_option(\"display.max_columns\", None, \"display.width\", 1000)\n\n        return results_df\n</code></pre>"},{"location":"reference/benchmarking/baseline/#periomod.benchmarking.Baseline.__init__","title":"<code>__init__(task, encoding, random_state=0, lr_solver='saga', dummy_strategy='prior', models=None, n_jobs=-1, path=Path('data/processed/processed_data.csv'))</code>","text":"<p>Initializes the Baseline class with default or user-specified models.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the task cannot be determined.</p> Source code in <code>periomod/benchmarking/_baseline.py</code> <pre><code>def __init__(\n    self,\n    task: str,\n    encoding: str,\n    random_state: int = 0,\n    lr_solver: str = \"saga\",\n    dummy_strategy: str = \"prior\",\n    models: Union[List[Tuple[str, object]], None] = None,\n    n_jobs: int = -1,\n    path: Path = Path(\"data/processed/processed_data.csv\"),\n) -&gt; None:\n    \"\"\"Initializes the Baseline class with default or user-specified models.\n\n    Raises:\n        ValueError: If the task cannot be determined.\n    \"\"\"\n    if task in [\"pocketclosure\", \"pocketclosureinf\", \"improvement\"]:\n        self.classification = \"binary\"\n    elif task == \"pdgrouprevaluation\":\n        self.classification = \"multiclass\"\n    else:\n        raise ValueError(\n            f\"Unknown task: {task}. Unable to determine classification.\"\n        )\n\n    self.resampler = Resampler(\n        classification=self.classification, encoding=encoding\n    )\n    self.dataloader = ProcessedDataLoader(task=task, encoding=encoding)\n    self.dummy_strategy = dummy_strategy\n    self.lr_solver = lr_solver\n    self.random_state = random_state\n    self.path = path\n    self.default_models: Union[list[str], None]\n\n    if models is None:\n        self.models = [\n            (\n                \"Random Forest\",\n                RandomForestClassifier(\n                    n_jobs=n_jobs, random_state=self.random_state\n                ),\n            ),\n            (\n                \"Logistic Regression\",\n                LogisticRegression(\n                    solver=self.lr_solver,\n                    random_state=self.random_state,\n                    n_jobs=n_jobs,\n                ),\n            ),\n            (\n                \"Dummy Classifier\",\n                DummyClassifier(strategy=self.dummy_strategy),\n            ),\n        ]\n        self.default_models = [name for name, _ in self.models]\n    else:\n        self.models = models\n        self.default_models = None\n</code></pre>"},{"location":"reference/benchmarking/baseline/#periomod.benchmarking.Baseline.baseline","title":"<code>baseline()</code>","text":"<p>Trains and evaluates each model in the models list on the given dataset.</p> <p>This method loads and transforms the dataset, splits it into training and testing sets, and evaluates each model in the <code>self.models</code> list. Metrics such as predictions and probabilities are computed and displayed.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the evaluation metrics for each baseline model, with model names as row indices.</p> Source code in <code>periomod/benchmarking/_baseline.py</code> <pre><code>def baseline(self) -&gt; pd.DataFrame:\n    \"\"\"Trains and evaluates each model in the models list on the given dataset.\n\n    This method loads and transforms the dataset, splits it into training and\n    testing sets, and evaluates each model in the `self.models` list. Metrics\n    such as predictions and probabilities are computed and displayed.\n\n    Returns:\n        DataFrame: A DataFrame containing the evaluation metrics for each\n            baseline model, with model names as row indices.\n    \"\"\"\n    trained_models, X_test, y_test = self.train_baselines()\n    results = []\n\n    for model_name, model in self.models:\n        preds = trained_models[(model_name, \"Baseline\")].predict(X_test)\n        probs = (\n            get_probs(\n                model=trained_models[(model_name, \"Baseline\")],\n                classification=self.classification,\n                X=X_test,\n            )\n            if hasattr(model, \"predict_proba\")\n            else None\n        )\n        metrics = final_metrics(\n            classification=self.classification,\n            y=y_test,\n            preds=preds,\n            probs=probs,\n        )\n        metrics[\"Model\"] = model_name\n        results.append(metrics)\n\n    results_df = pd.DataFrame(results).drop(\n        columns=[\"Best Threshold\"], errors=\"ignore\"\n    )\n\n    results_df, column_order = self._bss_helper(\n        results_df, classification=self.classification\n    )\n\n    existing_columns = [col for col in column_order if col in results_df.columns]\n    results_df = results_df[\n        existing_columns\n        + [col for col in results_df.columns if col not in existing_columns]\n    ].round(4)\n\n    if self.default_models is not None:\n        baseline_order = [\n            \"Dummy Classifier\",\n            \"Logistic Regression\",\n            \"Random Forest\",\n        ]\n        results_df[\"Model\"] = pd.Categorical(\n            results_df[\"Model\"], categories=baseline_order, ordered=True\n        )\n        results_df = results_df.sort_values(\"Model\").reset_index(drop=True)\n\n    else:\n        results_df = results_df.reset_index(drop=True)\n    pd.set_option(\"display.max_columns\", None, \"display.width\", 1000)\n\n    return results_df\n</code></pre>"},{"location":"reference/benchmarking/baseline/#periomod.benchmarking.Baseline.train_baselines","title":"<code>train_baselines()</code>","text":"<p>Trains each model in the models list and returns related data splits.</p> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[Dict[Tuple[str, str], Any], DataFrame, Series]</code> <ul> <li>Dictionary containing trained models.</li> <li>Testing feature set (X_test).</li> <li>Testing labels (y_test).</li> </ul> Source code in <code>periomod/benchmarking/_baseline.py</code> <pre><code>def train_baselines(\n    self,\n) -&gt; Tuple[Dict[Tuple[str, str], Any], pd.DataFrame, pd.Series]:\n    \"\"\"Trains each model in the models list and returns related data splits.\n\n    Returns:\n        Tuple:\n            - Dictionary containing trained models.\n            - Testing feature set (X_test).\n            - Testing labels (y_test).\n    \"\"\"\n    data = self.dataloader.load_data(path=self.path)\n    data = self.dataloader.transform_data(data=data)\n    train_df, test_df = self.resampler.split_train_test_df(\n        df=data, seed=self.random_state\n    )\n    X_train, y_train, X_test, y_test = self.resampler.split_x_y(\n        train_df=train_df, test_df=test_df\n    )\n\n    trained_models = {}\n    for model_name, model in self.models:\n        model.fit(X_train, y_train)\n        trained_models[(model_name, \"Baseline\")] = model\n\n    return trained_models, X_test, y_test\n</code></pre>"},{"location":"reference/benchmarking/benchmarker/","title":"Benchmarker","text":"<p>               Bases: <code>BaseBenchmark</code></p> <p>Benchmarker for evaluating machine learning models with tuning strategies.</p> <p>This class provides functionality to benchmark various machine learning models, tuning methods, HPO techniques, and criteria over multiple encodings, sampling strategies, and evaluation criteria. It supports training and evaluation workflows for different tasks and handles configurations for holdout or cross-validation tuning with threshold optimization.</p> Inherits <ul> <li><code>BaseBenchmark</code>: Provides common benchmarking attributes.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task for evaluation ('pocketclosure', 'pocketclosureinf', 'improvement', or 'pdgrouprevaluation'.).</p> required <code>learners</code> <code>List[str]</code> <p>List of learners to benchmark ('xgb', 'rf', 'lr' or 'mlp').</p> required <code>tuning_methods</code> <code>List[str]</code> <p>Tuning methods for each learner ('holdout', 'cv').</p> required <code>hpo_methods</code> <code>List[str]</code> <p>HPO methods ('hebo' or 'rs').</p> required <code>criteria</code> <code>List[str]</code> <p>List of evaluation criteria ('f1', 'macro_f1', 'brier_score').</p> required <code>encodings</code> <code>List[str]</code> <p>List of encodings ('one_hot' or 'target').</p> required <code>sampling</code> <code>Optional[List[str]]</code> <p>Sampling strategies for class imbalance. Includes None, 'upsampling', 'downsampling', and 'smote'.</p> <code>None</code> <code>factor</code> <code>Optional[float]</code> <p>Factor to apply during resampling.</p> <code>None</code> <code>n_configs</code> <code>int</code> <p>Number of configurations for hyperparameter tuning. Defaults to 10.</p> <code>10</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for processing. Defaults to 1.</p> <code>1</code> <code>cv_folds</code> <code>Optional[int]</code> <p>Number of folds for cross-validation. Defaults to 10.</p> <code>10</code> <code>racing_folds</code> <code>Optional[int]</code> <p>Number of racing folds for Random Search (RS). Defaults to None.</p> <code>None</code> <code>test_seed</code> <code>int</code> <p>Random seed for test splitting. Defaults to 0.</p> <code>0</code> <code>test_size</code> <code>float</code> <p>Proportion of data used for testing. Defaults to 0.2.</p> <code>0.2</code> <code>val_size</code> <code>Optional[float]</code> <p>Size of validation set in holdout tuning. Defaults to 0.2.</p> <code>0.2</code> <code>cv_seed</code> <code>Optional[int]</code> <p>Random seed for cross-validation. Defaults to 0</p> <code>0</code> <code>mlp_flag</code> <code>Optional[bool]</code> <p>Enables MLP training with early stopping. Defaults to None.</p> <code>None</code> <code>threshold_tuning</code> <code>Optional[bool]</code> <p>Enables threshold tuning for binary classification. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, enables detailed logging during benchmarking. Defaults to True.</p> <code>True</code> <code>path</code> <code>Path</code> <p>Path to the directory containing processed data files. Defaults to Path(\"data/processed/processed_data.csv\").</p> <code>Path('data/processed/processed_data.csv')</code> <p>Attributes:</p> Name Type Description <code>task</code> <code>str</code> <p>The specified task for evaluation.</p> <code>learners</code> <code>List[str]</code> <p>List of learners to evaluate.</p> <code>tuning_methods</code> <code>List[str]</code> <p>Tuning methods for model evaluation.</p> <code>hpo_methods</code> <code>List[str]</code> <p>HPO methods for hyperparameter tuning.</p> <code>criteria</code> <code>List[str]</code> <p>List of evaluation metrics.</p> <code>encodings</code> <code>List[str]</code> <p>Encoding types for categorical features.</p> <code>sampling</code> <code>List[str]</code> <p>Resampling strategies for class balancing.</p> <code>factor</code> <code>float</code> <p>Resampling factor for balancing.</p> <code>n_configs</code> <code>int</code> <p>Number of configurations for hyperparameter tuning.</p> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for model training.</p> <code>cv_folds</code> <code>int</code> <p>Number of cross-validation folds.</p> <code>racing_folds</code> <code>int</code> <p>Number of racing folds for random search.</p> <code>test_seed</code> <code>int</code> <p>Seed for reproducible train-test splits.</p> <code>test_size</code> <code>float</code> <p>Size of the test split.</p> <code>val_size</code> <code>float</code> <p>Size of the validation split in holdout tuning.</p> <code>cv_seed</code> <code>int</code> <p>Seed for cross-validation splits.</p> <code>mlp_flag</code> <code>bool</code> <p>Indicates if MLP training with early stopping is used.</p> <code>threshold_tuning</code> <code>bool</code> <p>Enables threshold tuning for binary classification.</p> <code>verbose</code> <code>bool</code> <p>Enables detailed logging during benchmarking.</p> <code>path</code> <code>Path</code> <p>Directory path for processed data.</p> <code>name</code> <code>str</code> <p>File name for processed data.</p> <code>data_cache</code> <code>dict</code> <p>Cached data for different task and encoding combinations.</p> <p>Methods:</p> Name Description <code>run_all_benchmarks</code> <p>Executes benchmarks for all combinations of learners, tuning methods, HPO, criteria, encodings, and sampling strategies, and returns a DataFrame summary and a dictionary of top models.</p> Example <pre><code>benchmarker = Benchmarker(\n    task=\"pocketclosure\",\n    learners=[\"xgb\", \"rf\"],\n    tuning_methods=[\"holdout\", \"cv\"],\n    hpo_methods=[\"hebo\", \"rs\"],\n    criteria=[\"f1\", \"brier_score\"],\n    encodings=[\"one_hot\", \"target\"],\n    sampling=[\"upsampling\", \"downsampling\"],\n    factor=1.5,\n    n_configs=20,\n    n_jobs=4,\n    cv_folds=5,\n    test_seed=42,\n    test_size=0.2,\n    verbose=True,\n    path=\"/data/processed/processed_data.csv\",\n)\n\n# Running all benchmarks\nresults_df, top_models = benchmarker.run_all_benchmarks()\nprint(results_df)\nprint(top_models)\n</code></pre> Source code in <code>periomod/benchmarking/_benchmark.py</code> <pre><code>class Benchmarker(BaseBenchmark):\n    \"\"\"Benchmarker for evaluating machine learning models with tuning strategies.\n\n    This class provides functionality to benchmark various machine learning\n    models, tuning methods, HPO techniques, and criteria over multiple\n    encodings, sampling strategies, and evaluation criteria. It supports\n    training and evaluation workflows for different tasks and handles\n    configurations for holdout or cross-validation tuning with threshold\n    optimization.\n\n    Inherits:\n        - `BaseBenchmark`: Provides common benchmarking attributes.\n\n    Args:\n        task (str): Task for evaluation ('pocketclosure', 'pocketclosureinf',\n            'improvement', or 'pdgrouprevaluation'.).\n        learners (List[str]): List of learners to benchmark ('xgb', 'rf', 'lr' or\n            'mlp').\n        tuning_methods (List[str]): Tuning methods for each learner ('holdout',\n            'cv').\n        hpo_methods (List[str]): HPO methods ('hebo' or 'rs').\n        criteria (List[str]): List of evaluation criteria ('f1', 'macro_f1',\n            'brier_score').\n        encodings (List[str]): List of encodings ('one_hot' or 'target').\n        sampling (Optional[List[str]]): Sampling strategies for class imbalance.\n            Includes None, 'upsampling', 'downsampling', and 'smote'.\n        factor (Optional[float]): Factor to apply during resampling.\n        n_configs (int): Number of configurations for hyperparameter tuning.\n            Defaults to 10.\n        n_jobs (int): Number of parallel jobs for processing. Defaults to 1.\n        cv_folds (Optional[int]): Number of folds for cross-validation.\n            Defaults to 10.\n        racing_folds (Optional[int]): Number of racing folds for Random Search (RS).\n            Defaults to None.\n        test_seed (int): Random seed for test splitting. Defaults to 0.\n        test_size (float): Proportion of data used for testing. Defaults to\n            0.2.\n        val_size (Optional[float]): Size of validation set in holdout tuning.\n            Defaults to 0.2.\n        cv_seed (Optional[int]): Random seed for cross-validation. Defaults to 0\n        mlp_flag (Optional[bool]): Enables MLP training with early stopping.\n            Defaults to None.\n        threshold_tuning (Optional[bool]): Enables threshold tuning for binary\n            classification. Defaults to None.\n        verbose (bool): If True, enables detailed logging during benchmarking.\n            Defaults to True.\n        path (Path): Path to the directory containing processed data files.\n            Defaults to Path(\"data/processed/processed_data.csv\").\n\n    Attributes:\n        task (str): The specified task for evaluation.\n        learners (List[str]): List of learners to evaluate.\n        tuning_methods (List[str]): Tuning methods for model evaluation.\n        hpo_methods (List[str]): HPO methods for hyperparameter tuning.\n        criteria (List[str]): List of evaluation metrics.\n        encodings (List[str]): Encoding types for categorical features.\n        sampling (List[str]): Resampling strategies for class balancing.\n        factor (float): Resampling factor for balancing.\n        n_configs (int): Number of configurations for hyperparameter tuning.\n        n_jobs (int): Number of parallel jobs for model training.\n        cv_folds (int): Number of cross-validation folds.\n        racing_folds (int): Number of racing folds for random search.\n        test_seed (int): Seed for reproducible train-test splits.\n        test_size (float): Size of the test split.\n        val_size (float): Size of the validation split in holdout tuning.\n        cv_seed (int): Seed for cross-validation splits.\n        mlp_flag (bool): Indicates if MLP training with early stopping is used.\n        threshold_tuning (bool): Enables threshold tuning for binary classification.\n        verbose (bool): Enables detailed logging during benchmarking.\n        path (Path): Directory path for processed data.\n        name (str): File name for processed data.\n        data_cache (dict): Cached data for different task and encoding combinations.\n\n    Methods:\n        run_all_benchmarks: Executes benchmarks for all combinations of learners,\n            tuning methods, HPO, criteria, encodings, and sampling strategies,\n            and returns a DataFrame summary and a dictionary of top models.\n\n    Example:\n        ```\n        benchmarker = Benchmarker(\n            task=\"pocketclosure\",\n            learners=[\"xgb\", \"rf\"],\n            tuning_methods=[\"holdout\", \"cv\"],\n            hpo_methods=[\"hebo\", \"rs\"],\n            criteria=[\"f1\", \"brier_score\"],\n            encodings=[\"one_hot\", \"target\"],\n            sampling=[\"upsampling\", \"downsampling\"],\n            factor=1.5,\n            n_configs=20,\n            n_jobs=4,\n            cv_folds=5,\n            test_seed=42,\n            test_size=0.2,\n            verbose=True,\n            path=\"/data/processed/processed_data.csv\",\n        )\n\n        # Running all benchmarks\n        results_df, top_models = benchmarker.run_all_benchmarks()\n        print(results_df)\n        print(top_models)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        learners: List[str],\n        tuning_methods: List[str],\n        hpo_methods: List[str],\n        criteria: List[str],\n        encodings: List[str],\n        sampling: Optional[List[Union[str, None]]] = None,\n        factor: Optional[float] = None,\n        n_configs: int = 10,\n        n_jobs: int = 1,\n        cv_folds: Optional[int] = 10,\n        racing_folds: Optional[int] = None,\n        test_seed: int = 0,\n        test_size: float = 0.2,\n        val_size: Optional[float] = 0.2,\n        cv_seed: Optional[int] = 0,\n        mlp_flag: Optional[bool] = None,\n        threshold_tuning: Optional[bool] = None,\n        verbose: bool = True,\n        path: Path = Path(\"data/processed/processed_data.csv\"),\n    ) -&gt; None:\n        \"\"\"Initialize the Experiment with different tasks, learners, etc.\n\n        Args:\n            task (str): Task for evaluation ('pocketclosure', 'pocketclosureinf',\n                'improvement', or 'pdgrouprevaluation'.).\n            learners (List[str]): List of learners to benchmark ('xgb', 'rf', 'lr' or\n                'mlp').\n            tuning_methods (List[str]): Tuning methods for each learner ('holdout',\n                'cv').\n            hpo_methods (List[str]): HPO methods ('hebo' or 'rs').\n            criteria (List[str]): List of evaluation criteria ('f1', 'macro_f1',\n                'brier_score').\n            encodings (List[str]): List of encodings ('one_hot' or 'target').\n            sampling (Optional[List[str]]): Sampling strategies for class imbalance.\n                Includes None, 'upsampling', 'downsampling', and 'smote'.\n            factor (Optional[float]): Factor to apply during resampling.\n            n_configs (int): Number of configurations for hyperparameter tuning.\n                Defaults to 10.\n            n_jobs (int): Number of parallel jobs for processing. Defaults to 1.\n            cv_folds (Optional[int]): Number of folds for cross-validation.\n                Defaults to 10.\n            racing_folds (Optional[int]): Number of racing folds for Random Search (RS).\n                Defaults to None.\n            test_seed (int): Random seed for test splitting. Defaults to 0.\n            test_size (float): Proportion of data used for testing. Defaults to\n                0.2.\n            val_size (Optional[float]): Size of validation set in holdout tuning.\n                Defaults to 0.2.\n            cv_seed (Optional[int]): Random seed for cross-validation. Defaults to 0\n            mlp_flag (Optional[bool]): Enables MLP training with early stopping.\n                Defaults to None.\n            threshold_tuning (Optional[bool]): Enables threshold tuning for binary\n                classification. Defaults to None.\n            verbose (bool): If True, enables detailed logging during benchmarking.\n                Defaults to True.\n            path (Path): Path to the directory containing processed data files.\n                Defaults to Path(\"data/processed/processed_data.csv\").\n        \"\"\"\n        super().__init__(\n            task=task,\n            learners=learners,\n            tuning_methods=tuning_methods,\n            hpo_methods=hpo_methods,\n            criteria=criteria,\n            encodings=encodings,\n            sampling=sampling,\n            factor=factor,\n            n_configs=n_configs,\n            n_jobs=n_jobs,\n            cv_folds=cv_folds,\n            racing_folds=racing_folds,\n            test_seed=test_seed,\n            test_size=test_size,\n            val_size=val_size,\n            cv_seed=cv_seed,\n            mlp_flag=mlp_flag,\n            threshold_tuning=threshold_tuning,\n            verbose=verbose,\n            path=path,\n        )\n        self.data_cache = self._load_data_for_tasks()\n\n    def _load_data_for_tasks(self) -&gt; dict:\n        \"\"\"Load and transform data for each task and encoding combination once.\n\n        Returns:\n            dict: A dictionary containing transformed data for each task-encoding pair.\n        \"\"\"\n        data_cache = {}\n        for encoding in self.encodings:\n            cache_key = encoding\n\n            if cache_key not in data_cache:\n                dataloader = ProcessedDataLoader(task=self.task, encoding=encoding)\n                data = dataloader.load_data(path=self.path)\n                transformed_df = dataloader.transform_data(data=data)\n                data_cache[cache_key] = transformed_df\n\n        return data_cache\n\n    def run_benchmarks(self) -&gt; Tuple[pd.DataFrame, dict]:\n        \"\"\"Benchmark all combinations of inputs.\n\n        Returns:\n            tuple: DataFrame summarizing the benchmark results with metrics for each\n                configuration and dictionary mapping model keys to models for top\n                configurations per criterion.\n\n        Raises:\n            KeyError: If an unknown criterion is encountered in `metric_map`.\n        \"\"\"\n        results = []\n        learners_dict = {}\n        top_models_per_criterion: Dict[\n            str, List[Tuple[float, object, str, str, str, str]]\n        ] = {criterion: [] for criterion in self.criteria}\n\n        metric_map = {\n            \"f1\": \"F1 Score\",\n            \"brier_score\": (\n                \"Multiclass Brier Score\"\n                if self.task == \"pdgrouprevaluation\"\n                else \"Brier Score\"\n            ),\n            \"macro_f1\": \"Macro F1\",\n        }\n\n        for learner, tuning, hpo, criterion, encoding, sampling in itertools.product(\n            self.learners,\n            self.tuning_methods,\n            self.hpo_methods,\n            self.criteria,\n            self.encodings,\n            self.sampling or [\"no_sampling\"],\n        ):\n            if sampling is None:\n                self.factor = None\n\n            if (criterion == \"macro_f1\" and self.task != \"pdgrouprevaluation\") or (\n                criterion == \"f1\" and self.task == \"pdgrouprevaluation\"\n            ):\n                print(f\"Criterion '{criterion}' and task '{self.task}' not valid.\")\n                continue\n            if self.verbose:\n                print(\n                    f\"\\nRunning benchmark for Task: {self.task}, Learner: {learner}, \"\n                    f\"Tuning: {tuning}, HPO: {hpo}, Criterion: {criterion}, \"\n                    f\"Sampling: {sampling}, Factor: {self.factor}.\"\n                )\n            data = self.data_cache[(encoding)]\n\n            exp = Experiment(\n                data=data,\n                task=self.task,\n                learner=learner,\n                criterion=criterion,\n                encoding=encoding,\n                tuning=tuning,\n                hpo=hpo,\n                sampling=sampling,\n                factor=self.factor,\n                n_configs=self.n_configs,\n                racing_folds=self.racing_folds,\n                n_jobs=self.n_jobs,\n                cv_folds=self.cv_folds,\n                test_seed=self.test_seed,\n                test_size=self.test_size,\n                val_size=self.val_size,\n                cv_seed=self.cv_seed,\n                mlp_flag=self.mlp_flag,\n                threshold_tuning=self.threshold_tuning,\n                verbose=self.verbose,\n            )\n\n            try:\n                result = exp.perform_evaluation()\n                metrics = result[\"metrics\"]\n                trained_model = result[\"model\"]\n\n                unpacked_metrics = {\n                    k: round(v, 4) if isinstance(v, float) else v\n                    for k, v in metrics.items()\n                }\n                results.append({\n                    \"Task\": self.task,\n                    \"Learner\": learner,\n                    \"Tuning\": tuning,\n                    \"HPO\": hpo,\n                    \"Criterion\": criterion,\n                    \"Sampling\": sampling,\n                    \"Factor\": self.factor,\n                    **unpacked_metrics,\n                })\n\n                metric_key = metric_map.get(criterion)\n                if metric_key is None:\n                    raise KeyError(f\"Unknown criterion '{criterion}'\")\n\n                criterion_value = metrics[metric_key]\n\n                current_model_data = (\n                    criterion_value,\n                    trained_model,\n                    learner,\n                    tuning,\n                    hpo,\n                    encoding,\n                )\n\n                if len(top_models_per_criterion[criterion]) &lt; 4:\n                    top_models_per_criterion[criterion].append(current_model_data)\n                else:\n                    worst_model_idx = min(\n                        range(len(top_models_per_criterion[criterion])),\n                        key=lambda idx: (\n                            top_models_per_criterion[criterion][idx][0]\n                            if criterion != \"brier_score\"\n                            else -top_models_per_criterion[criterion][idx][0]\n                        ),\n                    )\n                    worst_model_score = top_models_per_criterion[criterion][\n                        worst_model_idx\n                    ][0]\n                    if (\n                        criterion != \"brier_score\"\n                        and criterion_value &gt; worst_model_score\n                    ) or (\n                        criterion == \"brier_score\"\n                        and criterion_value &lt; worst_model_score\n                    ):\n                        top_models_per_criterion[criterion][worst_model_idx] = (\n                            current_model_data\n                        )\n\n            except Exception as e:\n                error_message = str(e)\n                if (\n                    \"Matrix not positive definite after repeatedly adding jitter\"\n                    in error_message\n                    or \"elements of the\" in error_message\n                    and \"are NaN\" in error_message\n                    or \"cholesky_cpu\" in error_message\n                ):\n                    print(\n                        f\"Suppressed NotPSDError for {self.task}, {learner} due to\"\n                        f\"convergence issue \\n\"\n                    )\n                else:\n                    print(\n                        f\"Error running benchmark for {self.task}, {learner}: \"\n                        f\"{error_message}\\n\"\n                    )\n                    traceback.print_exc()\n\n        for criterion, models in top_models_per_criterion.items():\n            sorted_models = sorted(\n                models, key=lambda x: -x[0] if criterion != \"brier_score\" else x[0]\n            )\n            for idx, (score, model, learner, tuning, hpo, encoding) in enumerate(\n                sorted_models\n            ):\n                learners_dict_key = (\n                    f\"{self.task}_{learner}_{tuning}_{hpo}_{criterion}_{encoding}_\"\n                    f\"{sampling or 'no_sampling'}_factor{self.factor}_rank{idx + 1}_\"\n                    f\"score{round(score, 4)}\"\n                )\n                learners_dict[learners_dict_key] = model\n\n        df_results = pd.DataFrame(results)\n        pd.set_option(\"display.max_columns\", None, \"display.width\", 1000)\n\n        if self.verbose:\n            print(f\"\\nBenchmark Results Summary:\\n{df_results}\")\n\n        return df_results, learners_dict\n</code></pre>"},{"location":"reference/benchmarking/benchmarker/#periomod.benchmarking.Benchmarker.__init__","title":"<code>__init__(task, learners, tuning_methods, hpo_methods, criteria, encodings, sampling=None, factor=None, n_configs=10, n_jobs=1, cv_folds=10, racing_folds=None, test_seed=0, test_size=0.2, val_size=0.2, cv_seed=0, mlp_flag=None, threshold_tuning=None, verbose=True, path=Path('data/processed/processed_data.csv'))</code>","text":"<p>Initialize the Experiment with different tasks, learners, etc.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task for evaluation ('pocketclosure', 'pocketclosureinf', 'improvement', or 'pdgrouprevaluation'.).</p> required <code>learners</code> <code>List[str]</code> <p>List of learners to benchmark ('xgb', 'rf', 'lr' or 'mlp').</p> required <code>tuning_methods</code> <code>List[str]</code> <p>Tuning methods for each learner ('holdout', 'cv').</p> required <code>hpo_methods</code> <code>List[str]</code> <p>HPO methods ('hebo' or 'rs').</p> required <code>criteria</code> <code>List[str]</code> <p>List of evaluation criteria ('f1', 'macro_f1', 'brier_score').</p> required <code>encodings</code> <code>List[str]</code> <p>List of encodings ('one_hot' or 'target').</p> required <code>sampling</code> <code>Optional[List[str]]</code> <p>Sampling strategies for class imbalance. Includes None, 'upsampling', 'downsampling', and 'smote'.</p> <code>None</code> <code>factor</code> <code>Optional[float]</code> <p>Factor to apply during resampling.</p> <code>None</code> <code>n_configs</code> <code>int</code> <p>Number of configurations for hyperparameter tuning. Defaults to 10.</p> <code>10</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for processing. Defaults to 1.</p> <code>1</code> <code>cv_folds</code> <code>Optional[int]</code> <p>Number of folds for cross-validation. Defaults to 10.</p> <code>10</code> <code>racing_folds</code> <code>Optional[int]</code> <p>Number of racing folds for Random Search (RS). Defaults to None.</p> <code>None</code> <code>test_seed</code> <code>int</code> <p>Random seed for test splitting. Defaults to 0.</p> <code>0</code> <code>test_size</code> <code>float</code> <p>Proportion of data used for testing. Defaults to 0.2.</p> <code>0.2</code> <code>val_size</code> <code>Optional[float]</code> <p>Size of validation set in holdout tuning. Defaults to 0.2.</p> <code>0.2</code> <code>cv_seed</code> <code>Optional[int]</code> <p>Random seed for cross-validation. Defaults to 0</p> <code>0</code> <code>mlp_flag</code> <code>Optional[bool]</code> <p>Enables MLP training with early stopping. Defaults to None.</p> <code>None</code> <code>threshold_tuning</code> <code>Optional[bool]</code> <p>Enables threshold tuning for binary classification. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, enables detailed logging during benchmarking. Defaults to True.</p> <code>True</code> <code>path</code> <code>Path</code> <p>Path to the directory containing processed data files. Defaults to Path(\"data/processed/processed_data.csv\").</p> <code>Path('data/processed/processed_data.csv')</code> Source code in <code>periomod/benchmarking/_benchmark.py</code> <pre><code>def __init__(\n    self,\n    task: str,\n    learners: List[str],\n    tuning_methods: List[str],\n    hpo_methods: List[str],\n    criteria: List[str],\n    encodings: List[str],\n    sampling: Optional[List[Union[str, None]]] = None,\n    factor: Optional[float] = None,\n    n_configs: int = 10,\n    n_jobs: int = 1,\n    cv_folds: Optional[int] = 10,\n    racing_folds: Optional[int] = None,\n    test_seed: int = 0,\n    test_size: float = 0.2,\n    val_size: Optional[float] = 0.2,\n    cv_seed: Optional[int] = 0,\n    mlp_flag: Optional[bool] = None,\n    threshold_tuning: Optional[bool] = None,\n    verbose: bool = True,\n    path: Path = Path(\"data/processed/processed_data.csv\"),\n) -&gt; None:\n    \"\"\"Initialize the Experiment with different tasks, learners, etc.\n\n    Args:\n        task (str): Task for evaluation ('pocketclosure', 'pocketclosureinf',\n            'improvement', or 'pdgrouprevaluation'.).\n        learners (List[str]): List of learners to benchmark ('xgb', 'rf', 'lr' or\n            'mlp').\n        tuning_methods (List[str]): Tuning methods for each learner ('holdout',\n            'cv').\n        hpo_methods (List[str]): HPO methods ('hebo' or 'rs').\n        criteria (List[str]): List of evaluation criteria ('f1', 'macro_f1',\n            'brier_score').\n        encodings (List[str]): List of encodings ('one_hot' or 'target').\n        sampling (Optional[List[str]]): Sampling strategies for class imbalance.\n            Includes None, 'upsampling', 'downsampling', and 'smote'.\n        factor (Optional[float]): Factor to apply during resampling.\n        n_configs (int): Number of configurations for hyperparameter tuning.\n            Defaults to 10.\n        n_jobs (int): Number of parallel jobs for processing. Defaults to 1.\n        cv_folds (Optional[int]): Number of folds for cross-validation.\n            Defaults to 10.\n        racing_folds (Optional[int]): Number of racing folds for Random Search (RS).\n            Defaults to None.\n        test_seed (int): Random seed for test splitting. Defaults to 0.\n        test_size (float): Proportion of data used for testing. Defaults to\n            0.2.\n        val_size (Optional[float]): Size of validation set in holdout tuning.\n            Defaults to 0.2.\n        cv_seed (Optional[int]): Random seed for cross-validation. Defaults to 0\n        mlp_flag (Optional[bool]): Enables MLP training with early stopping.\n            Defaults to None.\n        threshold_tuning (Optional[bool]): Enables threshold tuning for binary\n            classification. Defaults to None.\n        verbose (bool): If True, enables detailed logging during benchmarking.\n            Defaults to True.\n        path (Path): Path to the directory containing processed data files.\n            Defaults to Path(\"data/processed/processed_data.csv\").\n    \"\"\"\n    super().__init__(\n        task=task,\n        learners=learners,\n        tuning_methods=tuning_methods,\n        hpo_methods=hpo_methods,\n        criteria=criteria,\n        encodings=encodings,\n        sampling=sampling,\n        factor=factor,\n        n_configs=n_configs,\n        n_jobs=n_jobs,\n        cv_folds=cv_folds,\n        racing_folds=racing_folds,\n        test_seed=test_seed,\n        test_size=test_size,\n        val_size=val_size,\n        cv_seed=cv_seed,\n        mlp_flag=mlp_flag,\n        threshold_tuning=threshold_tuning,\n        verbose=verbose,\n        path=path,\n    )\n    self.data_cache = self._load_data_for_tasks()\n</code></pre>"},{"location":"reference/benchmarking/benchmarker/#periomod.benchmarking.Benchmarker.run_benchmarks","title":"<code>run_benchmarks()</code>","text":"<p>Benchmark all combinations of inputs.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[DataFrame, dict]</code> <p>DataFrame summarizing the benchmark results with metrics for each configuration and dictionary mapping model keys to models for top configurations per criterion.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If an unknown criterion is encountered in <code>metric_map</code>.</p> Source code in <code>periomod/benchmarking/_benchmark.py</code> <pre><code>def run_benchmarks(self) -&gt; Tuple[pd.DataFrame, dict]:\n    \"\"\"Benchmark all combinations of inputs.\n\n    Returns:\n        tuple: DataFrame summarizing the benchmark results with metrics for each\n            configuration and dictionary mapping model keys to models for top\n            configurations per criterion.\n\n    Raises:\n        KeyError: If an unknown criterion is encountered in `metric_map`.\n    \"\"\"\n    results = []\n    learners_dict = {}\n    top_models_per_criterion: Dict[\n        str, List[Tuple[float, object, str, str, str, str]]\n    ] = {criterion: [] for criterion in self.criteria}\n\n    metric_map = {\n        \"f1\": \"F1 Score\",\n        \"brier_score\": (\n            \"Multiclass Brier Score\"\n            if self.task == \"pdgrouprevaluation\"\n            else \"Brier Score\"\n        ),\n        \"macro_f1\": \"Macro F1\",\n    }\n\n    for learner, tuning, hpo, criterion, encoding, sampling in itertools.product(\n        self.learners,\n        self.tuning_methods,\n        self.hpo_methods,\n        self.criteria,\n        self.encodings,\n        self.sampling or [\"no_sampling\"],\n    ):\n        if sampling is None:\n            self.factor = None\n\n        if (criterion == \"macro_f1\" and self.task != \"pdgrouprevaluation\") or (\n            criterion == \"f1\" and self.task == \"pdgrouprevaluation\"\n        ):\n            print(f\"Criterion '{criterion}' and task '{self.task}' not valid.\")\n            continue\n        if self.verbose:\n            print(\n                f\"\\nRunning benchmark for Task: {self.task}, Learner: {learner}, \"\n                f\"Tuning: {tuning}, HPO: {hpo}, Criterion: {criterion}, \"\n                f\"Sampling: {sampling}, Factor: {self.factor}.\"\n            )\n        data = self.data_cache[(encoding)]\n\n        exp = Experiment(\n            data=data,\n            task=self.task,\n            learner=learner,\n            criterion=criterion,\n            encoding=encoding,\n            tuning=tuning,\n            hpo=hpo,\n            sampling=sampling,\n            factor=self.factor,\n            n_configs=self.n_configs,\n            racing_folds=self.racing_folds,\n            n_jobs=self.n_jobs,\n            cv_folds=self.cv_folds,\n            test_seed=self.test_seed,\n            test_size=self.test_size,\n            val_size=self.val_size,\n            cv_seed=self.cv_seed,\n            mlp_flag=self.mlp_flag,\n            threshold_tuning=self.threshold_tuning,\n            verbose=self.verbose,\n        )\n\n        try:\n            result = exp.perform_evaluation()\n            metrics = result[\"metrics\"]\n            trained_model = result[\"model\"]\n\n            unpacked_metrics = {\n                k: round(v, 4) if isinstance(v, float) else v\n                for k, v in metrics.items()\n            }\n            results.append({\n                \"Task\": self.task,\n                \"Learner\": learner,\n                \"Tuning\": tuning,\n                \"HPO\": hpo,\n                \"Criterion\": criterion,\n                \"Sampling\": sampling,\n                \"Factor\": self.factor,\n                **unpacked_metrics,\n            })\n\n            metric_key = metric_map.get(criterion)\n            if metric_key is None:\n                raise KeyError(f\"Unknown criterion '{criterion}'\")\n\n            criterion_value = metrics[metric_key]\n\n            current_model_data = (\n                criterion_value,\n                trained_model,\n                learner,\n                tuning,\n                hpo,\n                encoding,\n            )\n\n            if len(top_models_per_criterion[criterion]) &lt; 4:\n                top_models_per_criterion[criterion].append(current_model_data)\n            else:\n                worst_model_idx = min(\n                    range(len(top_models_per_criterion[criterion])),\n                    key=lambda idx: (\n                        top_models_per_criterion[criterion][idx][0]\n                        if criterion != \"brier_score\"\n                        else -top_models_per_criterion[criterion][idx][0]\n                    ),\n                )\n                worst_model_score = top_models_per_criterion[criterion][\n                    worst_model_idx\n                ][0]\n                if (\n                    criterion != \"brier_score\"\n                    and criterion_value &gt; worst_model_score\n                ) or (\n                    criterion == \"brier_score\"\n                    and criterion_value &lt; worst_model_score\n                ):\n                    top_models_per_criterion[criterion][worst_model_idx] = (\n                        current_model_data\n                    )\n\n        except Exception as e:\n            error_message = str(e)\n            if (\n                \"Matrix not positive definite after repeatedly adding jitter\"\n                in error_message\n                or \"elements of the\" in error_message\n                and \"are NaN\" in error_message\n                or \"cholesky_cpu\" in error_message\n            ):\n                print(\n                    f\"Suppressed NotPSDError for {self.task}, {learner} due to\"\n                    f\"convergence issue \\n\"\n                )\n            else:\n                print(\n                    f\"Error running benchmark for {self.task}, {learner}: \"\n                    f\"{error_message}\\n\"\n                )\n                traceback.print_exc()\n\n    for criterion, models in top_models_per_criterion.items():\n        sorted_models = sorted(\n            models, key=lambda x: -x[0] if criterion != \"brier_score\" else x[0]\n        )\n        for idx, (score, model, learner, tuning, hpo, encoding) in enumerate(\n            sorted_models\n        ):\n            learners_dict_key = (\n                f\"{self.task}_{learner}_{tuning}_{hpo}_{criterion}_{encoding}_\"\n                f\"{sampling or 'no_sampling'}_factor{self.factor}_rank{idx + 1}_\"\n                f\"score{round(score, 4)}\"\n            )\n            learners_dict[learners_dict_key] = model\n\n    df_results = pd.DataFrame(results)\n    pd.set_option(\"display.max_columns\", None, \"display.width\", 1000)\n\n    if self.verbose:\n        print(f\"\\nBenchmark Results Summary:\\n{df_results}\")\n\n    return df_results, learners_dict\n</code></pre>"},{"location":"reference/benchmarking/experiment/","title":"Experiment","text":"<p>               Bases: <code>BaseExperiment</code></p> <p>Concrete implementation for performing ML experiments and evaluation.</p> <p>This class extends <code>BaseExperiment</code>, providing methods for evaluating machine learning models using holdout or cross-validation strategies. It performs hyperparameter tuning, final model training, and evaluation based on specified tuning and optimization methods.</p> Inherits <p><code>BaseExperiment</code>: Provides core functionality for validation, resampling,     training, and tuning configurations.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The preloaded data for the experiment.</p> required <code>task</code> <code>str</code> <p>The task name used to determine classification type. Can be 'pocketclosure', 'pocketclosureinf', 'improvement', or 'pdgrouprevaluation'.</p> required <code>learner</code> <code>str</code> <p>Specifies the model or algorithm to evaluate. Includes 'xgb', 'rf', 'lr' or 'mlp'.</p> required <code>criterion</code> <code>str</code> <p>Criterion for optimization ('f1', 'macro_f1' or 'brier_score').</p> required <code>encoding</code> <code>str</code> <p>Encoding type for categorical features ('one_hot' or 'binary').</p> required <code>tuning</code> <code>Optional[str]</code> <p>Tuning method to apply ('holdout' or 'cv'). Can be None.</p> required <code>hpo</code> <code>Optional[str]</code> <p>Hyperparameter optimization method ('rs' or 'hebo'). Can be None.</p> required <code>sampling</code> <code>Optional[str]</code> <p>Resampling strategy to apply. Defaults to None. Includes None, 'upsampling', 'downsampling', and 'smote'.</p> <code>None</code> <code>factor</code> <code>Optional[float]</code> <p>Resampling factor. Defaults to None.</p> <code>None</code> <code>n_configs</code> <code>int</code> <p>Number of configurations for hyperparameter tuning. Defaults to 10.</p> <code>10</code> <code>racing_folds</code> <code>Optional[int]</code> <p>Number of racing folds for Random Search (RS). Defaults to None.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs to run for evaluation. Defaults to 1.</p> <code>1</code> <code>cv_folds</code> <code>Optional[int]</code> <p>Number of folds for cross-validation; Defaults to 10.</p> <code>10</code> <code>test_seed</code> <code>int</code> <p>Random seed for test splitting. Defaults to 0.</p> <code>0</code> <code>test_size</code> <code>float</code> <p>Proportion of data used for testing. Defaults to 0.2.</p> <code>0.2</code> <code>val_size</code> <code>Optional[float]</code> <p>Size of validation set in holdout tuning. Defaults to 0.2.</p> <code>0.2</code> <code>cv_seed</code> <code>Optional[int]</code> <p>Random seed for cross-validation. Defaults to 0</p> <code>0</code> <code>mlp_flag</code> <code>Optional[bool]</code> <p>Flag to enable MLP training with early stopping. Defaults to None.</p> <code>None</code> <code>threshold_tuning</code> <code>Optional[bool]</code> <p>If True, performs threshold tuning for binary classification if the criterion is \"f1\". Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Enables verbose output if set to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>DataFrame</code> <p>Dataset used for training and evaluation.</p> <code>task</code> <code>str</code> <p>Name of the task used to determine the classification type.</p> <code>learner</code> <code>str</code> <p>Model or algorithm name for the experiment.</p> <code>criterion</code> <code>str</code> <p>Criterion for performance evaluation.</p> <code>encoding</code> <code>str</code> <p>Encoding type for categorical features.</p> <code>sampling</code> <code>str</code> <p>Resampling method used in training.</p> <code>factor</code> <code>float</code> <p>Factor applied during resampling.</p> <code>n_configs</code> <code>int</code> <p>Number of configurations evaluated in hyperparameter tuning.</p> <code>racing_folds</code> <code>int</code> <p>Number of racing folds for random search.</p> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs used during processing.</p> <code>cv_folds</code> <code>int</code> <p>Number of cross-validation folds.</p> <code>test_seed</code> <code>int</code> <p>Seed for reproducible test splitting.</p> <code>test_size</code> <code>float</code> <p>Proportion of data reserved for testing.</p> <code>val_size</code> <code>float</code> <p>Size of the validation set in holdout tuning.</p> <code>cv_seed</code> <code>int</code> <p>Seed for reproducible cross-validation splits.</p> <code>mlp_flag</code> <code>bool</code> <p>Indicates if MLP training with early stopping is enabled.</p> <code>threshold_tuning</code> <code>bool</code> <p>Enables threshold tuning for binary classification.</p> <code>verbose</code> <code>bool</code> <p>Controls detailed output during the experiment.</p> <code>resampler</code> <code>Resampler</code> <p>Resampler instance for data handling.</p> <code>trainer</code> <code>Trainer</code> <p>Trainer instance for model training and evaluation.</p> <code>tuner</code> <code>Tuner</code> <p>Initialized tuner for hyperparameter optimization.</p> <p>Methods:</p> Name Description <code>perform_evaluation</code> <p>Conducts evaluation based on the tuning method.</p> Example <pre><code>from periomod.benchmarking import Experiment\nfrom periomod.data import ProcessedDataLoader\n\n# Load a dataframe with the correct target and encoding selected\ndataloader = ProcessedDataLoader(task=\"pocketclosure\", encoding=\"one_hot\")\ndata = dataloader.load_data(path=\"data/processed/processed_data.csv\")\ndata = dataloader.transform_data(data=data)\n\nexperiment = Experiment(\n    data=data,\n    task=\"pocketclosure\",\n    learner=\"rf\",\n    criterion=\"f1\",\n    encoding=\"one_hot\",\n    tuning=\"cv\",\n    hpo=\"rs\",\n    sampling=\"upsample\",\n    factor=1.5,\n    n_configs=20,\n    racing_folds=5,\n)\n\n# Perform the evaluation based on cross-validation\nfinal_metrics = experiment.perform_evaluation()\nprint(final_metrics)\n</code></pre> Source code in <code>periomod/benchmarking/_benchmark.py</code> <pre><code>class Experiment(BaseExperiment):\n    \"\"\"Concrete implementation for performing ML experiments and evaluation.\n\n    This class extends `BaseExperiment`, providing methods for evaluating machine\n    learning models using holdout or cross-validation strategies. It performs\n    hyperparameter tuning, final model training, and evaluation based on\n    specified tuning and optimization methods.\n\n    Inherits:\n        `BaseExperiment`: Provides core functionality for validation, resampling,\n            training, and tuning configurations.\n\n    Args:\n        data (pd.DataFrame): The preloaded data for the experiment.\n        task (str): The task name used to determine classification type.\n            Can be 'pocketclosure', 'pocketclosureinf', 'improvement', or\n            'pdgrouprevaluation'.\n        learner (str): Specifies the model or algorithm to evaluate.\n            Includes 'xgb', 'rf', 'lr' or 'mlp'.\n        criterion (str): Criterion for optimization ('f1', 'macro_f1' or 'brier_score').\n        encoding (str): Encoding type for categorical features ('one_hot' or 'binary').\n        tuning (Optional[str]): Tuning method to apply ('holdout' or 'cv'). Can be None.\n        hpo (Optional[str]): Hyperparameter optimization method ('rs' or 'hebo').\n            Can be None.\n        sampling (Optional[str]): Resampling strategy to apply. Defaults to None.\n            Includes None, 'upsampling', 'downsampling', and 'smote'.\n        factor (Optional[float]): Resampling factor. Defaults to None.\n        n_configs (int): Number of configurations for hyperparameter tuning.\n            Defaults to 10.\n        racing_folds (Optional[int]): Number of racing folds for Random Search (RS).\n            Defaults to None.\n        n_jobs (int): Number of parallel jobs to run for evaluation.\n            Defaults to 1.\n        cv_folds (Optional[int]): Number of folds for cross-validation;\n            Defaults to 10.\n        test_seed (int): Random seed for test splitting. Defaults to 0.\n        test_size (float): Proportion of data used for testing. Defaults to\n            0.2.\n        val_size (Optional[float]): Size of validation set in holdout tuning.\n            Defaults to 0.2.\n        cv_seed (Optional[int]): Random seed for cross-validation. Defaults to 0\n        mlp_flag (Optional[bool]): Flag to enable MLP training with early stopping.\n            Defaults to None.\n        threshold_tuning (Optional[bool]): If True, performs threshold tuning for binary\n            classification if the criterion is \"f1\". Defaults to None.\n        verbose (bool): Enables verbose output if set to True.\n\n    Attributes:\n        data (pd.DataFrame): Dataset used for training and evaluation.\n        task (str): Name of the task used to determine the classification type.\n        learner (str): Model or algorithm name for the experiment.\n        criterion (str): Criterion for performance evaluation.\n        encoding (str): Encoding type for categorical features.\n        sampling (str): Resampling method used in training.\n        factor (float): Factor applied during resampling.\n        n_configs (int): Number of configurations evaluated in hyperparameter tuning.\n        racing_folds (int): Number of racing folds for random search.\n        n_jobs (int): Number of parallel jobs used during processing.\n        cv_folds (int): Number of cross-validation folds.\n        test_seed (int): Seed for reproducible test splitting.\n        test_size (float): Proportion of data reserved for testing.\n        val_size (float): Size of the validation set in holdout tuning.\n        cv_seed (int): Seed for reproducible cross-validation splits.\n        mlp_flag (bool): Indicates if MLP training with early stopping is enabled.\n        threshold_tuning (bool): Enables threshold tuning for binary classification.\n        verbose (bool): Controls detailed output during the experiment.\n        resampler (Resampler): Resampler instance for data handling.\n        trainer (Trainer): Trainer instance for model training and evaluation.\n        tuner (Tuner): Initialized tuner for hyperparameter optimization.\n\n    Methods:\n        perform_evaluation: Conducts evaluation based on the tuning method.\n\n    Example:\n        ```\n        from periomod.benchmarking import Experiment\n        from periomod.data import ProcessedDataLoader\n\n        # Load a dataframe with the correct target and encoding selected\n        dataloader = ProcessedDataLoader(task=\"pocketclosure\", encoding=\"one_hot\")\n        data = dataloader.load_data(path=\"data/processed/processed_data.csv\")\n        data = dataloader.transform_data(data=data)\n\n        experiment = Experiment(\n            data=data,\n            task=\"pocketclosure\",\n            learner=\"rf\",\n            criterion=\"f1\",\n            encoding=\"one_hot\",\n            tuning=\"cv\",\n            hpo=\"rs\",\n            sampling=\"upsample\",\n            factor=1.5,\n            n_configs=20,\n            racing_folds=5,\n        )\n\n        # Perform the evaluation based on cross-validation\n        final_metrics = experiment.perform_evaluation()\n        print(final_metrics)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        data: pd.DataFrame,\n        task: str,\n        learner: str,\n        criterion: str,\n        encoding: str,\n        tuning: Optional[str],\n        hpo: Optional[str],\n        sampling: Optional[str] = None,\n        factor: Optional[float] = None,\n        n_configs: int = 10,\n        racing_folds: Optional[int] = None,\n        n_jobs: int = 1,\n        cv_folds: Optional[int] = 10,\n        test_seed: int = 0,\n        test_size: float = 0.2,\n        val_size: Optional[float] = 0.2,\n        cv_seed: Optional[int] = 0,\n        mlp_flag: Optional[bool] = None,\n        threshold_tuning: Optional[bool] = None,\n        verbose: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the Experiment class with tuning parameters.\n\n        Args:\n            data (pd.DataFrame): The preloaded data for the experiment.\n            task (str): The task name used to determine classification type.\n                Can be 'pocketclosure', 'pocketclosureinf', 'improvement', or\n                'pdgrouprevaluation'.\n            learner (str): Specifies the model or algorithm to evaluate.\n                Includes 'xgb', 'rf', 'lr' or 'mlp'.\n            criterion (str): Criterion for optimization ('f1', 'macro_f1' or\n                'brier_score').\n            encoding (str): Encoding type for categorical features ('one_hot' or\n                'binary').\n            tuning (Optional[str]): Tuning method to apply ('holdout' or 'cv').\n                Can be None.\n            hpo (Optional[str]): Hyperparameter optimization method ('rs' or 'hebo').\n                Can be None.\n            sampling (Optional[str]): Resampling strategy to apply. Defaults to None.\n                Includes None, 'upsampling', 'downsampling', and 'smote'.\n            factor (Optional[float]): Resampling factor. Defaults to None.\n            n_configs (int): Number of configurations for hyperparameter tuning.\n                Defaults to 10.\n            racing_folds (Optional[int]): Number of racing folds for Random Search (RS).\n                Defaults to None.\n            n_jobs (int): Number of parallel jobs to run for evaluation.\n                Defaults to 1.\n            cv_folds (Optional[int]): Number of folds for cross-validation;\n                Defaults to 10.\n            test_seed (int): Random seed for test splitting. Defaults to 0.\n            test_size (float): Proportion of data used for testing. Defaults to\n                0.2.\n            val_size (Optional[float]): Size of validation set in holdout tuning.\n                Defaults to 0.2.\n            cv_seed (Optional[int]): Random seed for cross-validation. Defaults to 0\n            mlp_flag (Optional[bool]): Flag to enable MLP training with early stopping.\n                Defaults to None.\n            threshold_tuning (Optional[bool]): If True, performs threshold tuning for\n                binary classification if the criterion is \"f1\". Defaults to None.\n            verbose (bool): Enables verbose output if set to True.\n        \"\"\"\n        super().__init__(\n            data=data,\n            task=task,\n            learner=learner,\n            criterion=criterion,\n            encoding=encoding,\n            tuning=tuning,\n            hpo=hpo,\n            sampling=sampling,\n            factor=factor,\n            n_configs=n_configs,\n            racing_folds=racing_folds,\n            n_jobs=n_jobs,\n            cv_folds=cv_folds,\n            test_seed=test_seed,\n            test_size=test_size,\n            val_size=val_size,\n            cv_seed=cv_seed,\n            mlp_flag=mlp_flag,\n            threshold_tuning=threshold_tuning,\n            verbose=verbose,\n        )\n\n    def perform_evaluation(self) -&gt; dict:\n        \"\"\"Perform model evaluation and return final metrics.\n\n        Returns:\n            dict: A dictionary containing the trained model and its evaluation metrics.\n\n        Raises:\n            ValueError: If self.tuning is invalid.\n        \"\"\"\n        train_df, _ = self.resampler.split_train_test_df(\n            df=self.data, seed=self.test_seed, test_size=self.test_size\n        )\n\n        if self.tuning == \"holdout\":\n            return self._evaluate_holdout(train_df=train_df)\n        elif self.tuning == \"cv\":\n            return self._evaluate_cv()\n        else:\n            raise ValueError(f\"Unsupported tuning method: {self.tuning}\")\n\n    def _evaluate_holdout(self, train_df: pd.DataFrame) -&gt; dict:\n        \"\"\"Perform holdout validation and return the final model metrics.\n\n        Args:\n            train_df (pd.DataFrame): train df for holdout tuning.\n\n        Returns:\n            dict: A dictionary of evaluation metrics for the final model.\n        \"\"\"\n        train_df_h, test_df_h = self.resampler.split_train_test_df(\n            df=train_df, seed=self.test_seed, test_size=self.val_size\n        )\n        X_train_h, y_train_h, X_val, y_val = self.resampler.split_x_y(\n            train_df=train_df_h,\n            test_df=test_df_h,\n            sampling=self.sampling,\n            factor=self.factor,\n        )\n        best_params, best_threshold = self.tuner.holdout(\n            learner=self.learner,\n            X_train=X_train_h,\n            y_train=y_train_h,\n            X_val=X_val,\n            y_val=y_val,\n        )\n        final_model = (self.learner, best_params, best_threshold)\n\n        return self._train_final_model(final_model)\n\n    def _evaluate_cv(self) -&gt; dict:\n        \"\"\"Perform cross-validation and return the final model metrics.\n\n        Returns:\n            dict: A dictionary of evaluation metrics for the final model.\n        \"\"\"\n        outer_splits, _ = self.resampler.cv_folds(\n            df=self.data,\n            sampling=self.sampling,\n            factor=self.factor,\n            seed=self.cv_seed,\n            n_folds=self.cv_folds,\n        )\n        best_params, best_threshold = self.tuner.cv(\n            learner=self.learner,\n            outer_splits=outer_splits,\n            racing_folds=self.racing_folds,\n        )\n        final_model = (self.learner, best_params, best_threshold)\n\n        return self._train_final_model(final_model_tuple=final_model)\n</code></pre>"},{"location":"reference/benchmarking/experiment/#periomod.benchmarking.Experiment.__init__","title":"<code>__init__(data, task, learner, criterion, encoding, tuning, hpo, sampling=None, factor=None, n_configs=10, racing_folds=None, n_jobs=1, cv_folds=10, test_seed=0, test_size=0.2, val_size=0.2, cv_seed=0, mlp_flag=None, threshold_tuning=None, verbose=True)</code>","text":"<p>Initialize the Experiment class with tuning parameters.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The preloaded data for the experiment.</p> required <code>task</code> <code>str</code> <p>The task name used to determine classification type. Can be 'pocketclosure', 'pocketclosureinf', 'improvement', or 'pdgrouprevaluation'.</p> required <code>learner</code> <code>str</code> <p>Specifies the model or algorithm to evaluate. Includes 'xgb', 'rf', 'lr' or 'mlp'.</p> required <code>criterion</code> <code>str</code> <p>Criterion for optimization ('f1', 'macro_f1' or 'brier_score').</p> required <code>encoding</code> <code>str</code> <p>Encoding type for categorical features ('one_hot' or 'binary').</p> required <code>tuning</code> <code>Optional[str]</code> <p>Tuning method to apply ('holdout' or 'cv'). Can be None.</p> required <code>hpo</code> <code>Optional[str]</code> <p>Hyperparameter optimization method ('rs' or 'hebo'). Can be None.</p> required <code>sampling</code> <code>Optional[str]</code> <p>Resampling strategy to apply. Defaults to None. Includes None, 'upsampling', 'downsampling', and 'smote'.</p> <code>None</code> <code>factor</code> <code>Optional[float]</code> <p>Resampling factor. Defaults to None.</p> <code>None</code> <code>n_configs</code> <code>int</code> <p>Number of configurations for hyperparameter tuning. Defaults to 10.</p> <code>10</code> <code>racing_folds</code> <code>Optional[int]</code> <p>Number of racing folds for Random Search (RS). Defaults to None.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs to run for evaluation. Defaults to 1.</p> <code>1</code> <code>cv_folds</code> <code>Optional[int]</code> <p>Number of folds for cross-validation; Defaults to 10.</p> <code>10</code> <code>test_seed</code> <code>int</code> <p>Random seed for test splitting. Defaults to 0.</p> <code>0</code> <code>test_size</code> <code>float</code> <p>Proportion of data used for testing. Defaults to 0.2.</p> <code>0.2</code> <code>val_size</code> <code>Optional[float]</code> <p>Size of validation set in holdout tuning. Defaults to 0.2.</p> <code>0.2</code> <code>cv_seed</code> <code>Optional[int]</code> <p>Random seed for cross-validation. Defaults to 0</p> <code>0</code> <code>mlp_flag</code> <code>Optional[bool]</code> <p>Flag to enable MLP training with early stopping. Defaults to None.</p> <code>None</code> <code>threshold_tuning</code> <code>Optional[bool]</code> <p>If True, performs threshold tuning for binary classification if the criterion is \"f1\". Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Enables verbose output if set to True.</p> <code>True</code> Source code in <code>periomod/benchmarking/_benchmark.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    task: str,\n    learner: str,\n    criterion: str,\n    encoding: str,\n    tuning: Optional[str],\n    hpo: Optional[str],\n    sampling: Optional[str] = None,\n    factor: Optional[float] = None,\n    n_configs: int = 10,\n    racing_folds: Optional[int] = None,\n    n_jobs: int = 1,\n    cv_folds: Optional[int] = 10,\n    test_seed: int = 0,\n    test_size: float = 0.2,\n    val_size: Optional[float] = 0.2,\n    cv_seed: Optional[int] = 0,\n    mlp_flag: Optional[bool] = None,\n    threshold_tuning: Optional[bool] = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the Experiment class with tuning parameters.\n\n    Args:\n        data (pd.DataFrame): The preloaded data for the experiment.\n        task (str): The task name used to determine classification type.\n            Can be 'pocketclosure', 'pocketclosureinf', 'improvement', or\n            'pdgrouprevaluation'.\n        learner (str): Specifies the model or algorithm to evaluate.\n            Includes 'xgb', 'rf', 'lr' or 'mlp'.\n        criterion (str): Criterion for optimization ('f1', 'macro_f1' or\n            'brier_score').\n        encoding (str): Encoding type for categorical features ('one_hot' or\n            'binary').\n        tuning (Optional[str]): Tuning method to apply ('holdout' or 'cv').\n            Can be None.\n        hpo (Optional[str]): Hyperparameter optimization method ('rs' or 'hebo').\n            Can be None.\n        sampling (Optional[str]): Resampling strategy to apply. Defaults to None.\n            Includes None, 'upsampling', 'downsampling', and 'smote'.\n        factor (Optional[float]): Resampling factor. Defaults to None.\n        n_configs (int): Number of configurations for hyperparameter tuning.\n            Defaults to 10.\n        racing_folds (Optional[int]): Number of racing folds for Random Search (RS).\n            Defaults to None.\n        n_jobs (int): Number of parallel jobs to run for evaluation.\n            Defaults to 1.\n        cv_folds (Optional[int]): Number of folds for cross-validation;\n            Defaults to 10.\n        test_seed (int): Random seed for test splitting. Defaults to 0.\n        test_size (float): Proportion of data used for testing. Defaults to\n            0.2.\n        val_size (Optional[float]): Size of validation set in holdout tuning.\n            Defaults to 0.2.\n        cv_seed (Optional[int]): Random seed for cross-validation. Defaults to 0\n        mlp_flag (Optional[bool]): Flag to enable MLP training with early stopping.\n            Defaults to None.\n        threshold_tuning (Optional[bool]): If True, performs threshold tuning for\n            binary classification if the criterion is \"f1\". Defaults to None.\n        verbose (bool): Enables verbose output if set to True.\n    \"\"\"\n    super().__init__(\n        data=data,\n        task=task,\n        learner=learner,\n        criterion=criterion,\n        encoding=encoding,\n        tuning=tuning,\n        hpo=hpo,\n        sampling=sampling,\n        factor=factor,\n        n_configs=n_configs,\n        racing_folds=racing_folds,\n        n_jobs=n_jobs,\n        cv_folds=cv_folds,\n        test_seed=test_seed,\n        test_size=test_size,\n        val_size=val_size,\n        cv_seed=cv_seed,\n        mlp_flag=mlp_flag,\n        threshold_tuning=threshold_tuning,\n        verbose=verbose,\n    )\n</code></pre>"},{"location":"reference/benchmarking/experiment/#periomod.benchmarking.Experiment.perform_evaluation","title":"<code>perform_evaluation()</code>","text":"<p>Perform model evaluation and return final metrics.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the trained model and its evaluation metrics.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If self.tuning is invalid.</p> Source code in <code>periomod/benchmarking/_benchmark.py</code> <pre><code>def perform_evaluation(self) -&gt; dict:\n    \"\"\"Perform model evaluation and return final metrics.\n\n    Returns:\n        dict: A dictionary containing the trained model and its evaluation metrics.\n\n    Raises:\n        ValueError: If self.tuning is invalid.\n    \"\"\"\n    train_df, _ = self.resampler.split_train_test_df(\n        df=self.data, seed=self.test_seed, test_size=self.test_size\n    )\n\n    if self.tuning == \"holdout\":\n        return self._evaluate_holdout(train_df=train_df)\n    elif self.tuning == \"cv\":\n        return self._evaluate_cv()\n    else:\n        raise ValueError(f\"Unsupported tuning method: {self.tuning}\")\n</code></pre>"},{"location":"reference/data/","title":"periomod.data Overview","text":"<p>The <code>periomod.data</code> module contains classes for data loading, preprocessing, and transformation within the <code>periomod</code> pipeline.</p>"},{"location":"reference/data/#available-components","title":"Available Components","text":"Component Description BaseLoader Base class for loading data. BaseProcessor Base class for data preprocessing and transformation. StaticProcessEngine Engine for static data processing steps. ProcessDataHelper Helper functions for data transformation and preparation. BaseDataLoader Core class for data loading configuration. ProcessedDataLoader Data loader for fully processed data, ready for analysis."},{"location":"reference/data/basedataloader/","title":"BaseDataLoader","text":"<p>               Bases: <code>BaseLoader</code>, <code>ABC</code></p> <p>Abstract base class for loading, encoding, and scaling processed data.</p> <p>This class provides methods for loading and saving processed data, verifying encoded and scaled columns, and defines abstract methods for encoding and scaling that must be implemented by subclasses.</p> Inherits <ul> <li><code>BaseLoader</code>: Provides loading and saving capabilities for processed data.</li> <li><code>ABC</code>: Specifies abstract methods for subclasses to implement.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Specifies the task column name.</p> required <code>encoding</code> <code>Optional[str]</code> <p>Defines the encoding method for categorical columns. Options include 'one_hot', 'target', or None.</p> required <code>encode</code> <code>bool</code> <p>If True, applies encoding to categorical columns.</p> required <code>scale</code> <code>bool</code> <p>If True, applies scaling to numeric columns.</p> required <p>Attributes:</p> Name Type Description <code>task</code> <code>str</code> <p>Task column name used in transformations.</p> <code>encoding</code> <code>str</code> <p>Encoding method specified for categorical columns.</p> <code>encode</code> <code>bool</code> <p>Flag to apply encoding to categorical columns.</p> <code>scale</code> <code>bool</code> <p>Flag to apply scaling to numeric columns.</p> <p>Methods:</p> Name Description <code>load_data</code> <p>Load processed data from the specified path and file.</p> <code>save_data</code> <p>Save processed data to the specified path and file.</p> Abstract Methods <ul> <li><code>encode_categorical_columns</code>: Encodes categorical columns in the DataFrame.</li> <li><code>scale_numeric_columns</code>: Scales numeric columns in the DataFrame.</li> <li><code>transform_data</code>: Processes and transforms the data.</li> </ul> Source code in <code>periomod/data/_basedata.py</code> <pre><code>class BaseDataLoader(BaseLoader, ABC):\n    \"\"\"Abstract base class for loading, encoding, and scaling processed data.\n\n    This class provides methods for loading and saving processed data, verifying\n    encoded and scaled columns, and defines abstract methods for encoding and scaling\n    that must be implemented by subclasses.\n\n    Inherits:\n        - `BaseLoader`: Provides loading and saving capabilities for processed data.\n        - `ABC`: Specifies abstract methods for subclasses to implement.\n\n    Args:\n        task (str): Specifies the task column name.\n        encoding (Optional[str]): Defines the encoding method for categorical columns.\n            Options include 'one_hot', 'target', or None.\n        encode (bool): If True, applies encoding to categorical columns.\n        scale (bool): If True, applies scaling to numeric columns.\n\n    Attributes:\n        task (str): Task column name used in transformations.\n        encoding (str): Encoding method specified for categorical columns.\n        encode (bool): Flag to apply encoding to categorical columns.\n        scale (bool): Flag to apply scaling to numeric columns.\n\n    Methods:\n        load_data: Load processed data from the specified path and file.\n        save_data: Save processed data to the specified path and file.\n\n    Abstract Methods:\n        - `encode_categorical_columns`: Encodes categorical columns in the DataFrame.\n        - `scale_numeric_columns`: Scales numeric columns in the DataFrame.\n        - `transform_data`: Processes and transforms the data.\n    \"\"\"\n\n    def __init__(\n        self, task: str, encoding: Optional[str], encode: bool, scale: bool\n    ) -&gt; None:\n        \"\"\"Initializes the ProcessedDataLoader with the specified task column.\"\"\"\n        super().__init__()\n        self.task = task\n        self.encoding = encoding\n        self.encode = encode\n        self.scale = scale\n\n    @staticmethod\n    def load_data(\n        path: Union[str, Path] = Path(\"data/processed/processed_data.csv\"),\n    ) -&gt; pd.DataFrame:\n        \"\"\"Loads the processed data from the specified path, with lowercasing.\n\n        Args:\n            path (str): Directory path for the processed data.\n\n        Returns:\n            pd.DataFrame: Loaded DataFrame with lowercase column names.\n\n        Raises:\n            FileNotFoundError: If file is not found in path.\n        \"\"\"\n        path = Path(path)\n\n        if not path.is_absolute():\n            path = Path.cwd() / path\n\n        if not path.exists():\n            raise FileNotFoundError(f\"File not found: {path}\")\n\n        return pd.read_csv(path).rename(columns=str.lower)\n\n    def save_data(\n        self,\n        data: pd.DataFrame,\n        path: Union[str, Path] = Path(\"data/training/training_data.csv\"),\n    ) -&gt; None:\n        \"\"\"Saves the processed DataFrame to a CSV file.\n\n        Args:\n            data (pd.DataFrame): The processed DataFrame.\n            path (str, optional): Directory where dataset is saved.\n                Path(\"data/training/training_data.csv\".\n        \"\"\"\n        super().save_data(data=data, path=path)\n\n    def _check_encoded_columns(self, data: pd.DataFrame) -&gt; None:\n        \"\"\"Verifies that categorical columns were correctly one-hot or target encoded.\n\n        Args:\n            data (pd.DataFrame): The DataFrame to check.\n\n        Raises:\n            ValueError: If columns are not correctly encoded.\n        \"\"\"\n        if self.encoding == \"one_hot\":\n            cat_vars = [col for col in self.all_cat_vars if col in data.columns]\n\n            for col in cat_vars:\n                if col in data.columns:\n                    raise ValueError(\n                        f\"Column '{col}' was not correctly one-hot encoded.\"\n                    )\n                matching_columns = [c for c in data.columns if c.startswith(f\"{col}_\")]\n                if not matching_columns:\n                    raise ValueError(f\"No one-hot encoded columns for '{col}'.\")\n        elif self.encoding == \"target\":\n            if \"toothside\" not in data.columns:\n                raise ValueError(\"Target encoding for 'toothside' failed.\")\n        elif self.encoding is None:\n            print(\"No encoding was applied.\")\n        else:\n            raise ValueError(f\"Invalid encoding '{self.encoding}'.\")\n\n    def _check_scaled_columns(self, data: pd.DataFrame) -&gt; None:\n        \"\"\"Verifies that scaled columns are within expected ranges.\n\n        Args:\n            data (pd.DataFrame): The DataFrame to check.\n\n        Raises:\n            ValueError: If any columns are not correctly scaled.\n        \"\"\"\n        if self.scale:\n            for col in self.scale_vars:\n                scaled_min = data[col].min()\n                scaled_max = data[col].max()\n                if scaled_min &lt; -10 or scaled_max &gt; 20:\n                    raise ValueError(f\"Column {col} is not correctly scaled.\")\n\n    @abstractmethod\n    def encode_categorical_columns(self, data: pd.DataFrame, fit_encoder: bool):\n        \"\"\"Encodes categorical columns in the DataFrame.\n\n        Args:\n            data (pd.DataFrame): The DataFrame containing categorical columns.\n            fit_encoder (bool): Whether to fit the encoder on this dataset\n                (only for training data).\n        \"\"\"\n\n    @abstractmethod\n    def scale_numeric_columns(self, data: pd.DataFrame):\n        \"\"\"Scales numeric columns in the DataFrame.\n\n        Args:\n            data (pd.DataFrame): The DataFrame containing numeric columns.\n        \"\"\"\n\n    @abstractmethod\n    def transform_data(self, data: pd.DataFrame, fit_encoder: bool):\n        \"\"\"Processes and transforms the data.\n\n        Args:\n            data (pd.DataFrame): The DataFrame to transform.\n            fit_encoder (bool): Whether to fit the encoder on this dataset\n                (only for training data).\n        \"\"\"\n</code></pre>"},{"location":"reference/data/basedataloader/#periomod.data.BaseDataLoader.__init__","title":"<code>__init__(task, encoding, encode, scale)</code>","text":"<p>Initializes the ProcessedDataLoader with the specified task column.</p> Source code in <code>periomod/data/_basedata.py</code> <pre><code>def __init__(\n    self, task: str, encoding: Optional[str], encode: bool, scale: bool\n) -&gt; None:\n    \"\"\"Initializes the ProcessedDataLoader with the specified task column.\"\"\"\n    super().__init__()\n    self.task = task\n    self.encoding = encoding\n    self.encode = encode\n    self.scale = scale\n</code></pre>"},{"location":"reference/data/basedataloader/#periomod.data.BaseDataLoader.encode_categorical_columns","title":"<code>encode_categorical_columns(data, fit_encoder)</code>  <code>abstractmethod</code>","text":"<p>Encodes categorical columns in the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame containing categorical columns.</p> required <code>fit_encoder</code> <code>bool</code> <p>Whether to fit the encoder on this dataset (only for training data).</p> required Source code in <code>periomod/data/_basedata.py</code> <pre><code>@abstractmethod\ndef encode_categorical_columns(self, data: pd.DataFrame, fit_encoder: bool):\n    \"\"\"Encodes categorical columns in the DataFrame.\n\n    Args:\n        data (pd.DataFrame): The DataFrame containing categorical columns.\n        fit_encoder (bool): Whether to fit the encoder on this dataset\n            (only for training data).\n    \"\"\"\n</code></pre>"},{"location":"reference/data/basedataloader/#periomod.data.BaseDataLoader.load_data","title":"<code>load_data(path=Path('data/processed/processed_data.csv'))</code>  <code>staticmethod</code>","text":"<p>Loads the processed data from the specified path, with lowercasing.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path for the processed data.</p> <code>Path('data/processed/processed_data.csv')</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Loaded DataFrame with lowercase column names.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file is not found in path.</p> Source code in <code>periomod/data/_basedata.py</code> <pre><code>@staticmethod\ndef load_data(\n    path: Union[str, Path] = Path(\"data/processed/processed_data.csv\"),\n) -&gt; pd.DataFrame:\n    \"\"\"Loads the processed data from the specified path, with lowercasing.\n\n    Args:\n        path (str): Directory path for the processed data.\n\n    Returns:\n        pd.DataFrame: Loaded DataFrame with lowercase column names.\n\n    Raises:\n        FileNotFoundError: If file is not found in path.\n    \"\"\"\n    path = Path(path)\n\n    if not path.is_absolute():\n        path = Path.cwd() / path\n\n    if not path.exists():\n        raise FileNotFoundError(f\"File not found: {path}\")\n\n    return pd.read_csv(path).rename(columns=str.lower)\n</code></pre>"},{"location":"reference/data/basedataloader/#periomod.data.BaseDataLoader.save_data","title":"<code>save_data(data, path=Path('data/training/training_data.csv'))</code>","text":"<p>Saves the processed DataFrame to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The processed DataFrame.</p> required <code>path</code> <code>str</code> <p>Directory where dataset is saved. Path(\"data/training/training_data.csv\".</p> <code>Path('data/training/training_data.csv')</code> Source code in <code>periomod/data/_basedata.py</code> <pre><code>def save_data(\n    self,\n    data: pd.DataFrame,\n    path: Union[str, Path] = Path(\"data/training/training_data.csv\"),\n) -&gt; None:\n    \"\"\"Saves the processed DataFrame to a CSV file.\n\n    Args:\n        data (pd.DataFrame): The processed DataFrame.\n        path (str, optional): Directory where dataset is saved.\n            Path(\"data/training/training_data.csv\".\n    \"\"\"\n    super().save_data(data=data, path=path)\n</code></pre>"},{"location":"reference/data/basedataloader/#periomod.data.BaseDataLoader.scale_numeric_columns","title":"<code>scale_numeric_columns(data)</code>  <code>abstractmethod</code>","text":"<p>Scales numeric columns in the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame containing numeric columns.</p> required Source code in <code>periomod/data/_basedata.py</code> <pre><code>@abstractmethod\ndef scale_numeric_columns(self, data: pd.DataFrame):\n    \"\"\"Scales numeric columns in the DataFrame.\n\n    Args:\n        data (pd.DataFrame): The DataFrame containing numeric columns.\n    \"\"\"\n</code></pre>"},{"location":"reference/data/basedataloader/#periomod.data.BaseDataLoader.transform_data","title":"<code>transform_data(data, fit_encoder)</code>  <code>abstractmethod</code>","text":"<p>Processes and transforms the data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame to transform.</p> required <code>fit_encoder</code> <code>bool</code> <p>Whether to fit the encoder on this dataset (only for training data).</p> required Source code in <code>periomod/data/_basedata.py</code> <pre><code>@abstractmethod\ndef transform_data(self, data: pd.DataFrame, fit_encoder: bool):\n    \"\"\"Processes and transforms the data.\n\n    Args:\n        data (pd.DataFrame): The DataFrame to transform.\n        fit_encoder (bool): Whether to fit the encoder on this dataset\n            (only for training data).\n    \"\"\"\n</code></pre>"},{"location":"reference/data/baseloader/","title":"BaseLoader","text":"<p>               Bases: <code>BaseConfig</code>, <code>ABC</code></p> <p>Abstract base class for loading and saving processed data.</p> <p>This class provides a structure for loading and saving datasets and defines abstract methods that need to be implemented by subclasses. It includes methods for specifying data paths and filenames for loading and saving operations.</p> Inherits <ul> <li><code>BaseConfig</code>: Provides configuration settings for data processing.</li> <li><code>ABC</code>: Specifies abstract methods for subclasses to implement.</li> </ul> Abstract Methods <ul> <li><code>load_data</code>: Load processed data from the specified path and file.</li> <li><code>save_data</code>: Save processed data to the specified path and file.</li> </ul> Example <pre><code>loader = SomeConcreteLoader()\ndata = loader.load_data(path=Path(\"/data\"), name=\"processed_data.csv\")\nloader.save_data(data=data, path=Path(\"/data\"), name=\"saved_data.csv\")\n</code></pre> Source code in <code>periomod/data/_basedata.py</code> <pre><code>class BaseLoader(BaseConfig, ABC):\n    \"\"\"Abstract base class for loading and saving processed data.\n\n    This class provides a structure for loading and saving datasets and defines\n    abstract methods that need to be implemented by subclasses. It includes methods\n    for specifying data paths and filenames for loading and saving operations.\n\n    Inherits:\n        - `BaseConfig`: Provides configuration settings for data processing.\n        - `ABC`: Specifies abstract methods for subclasses to implement.\n\n    Abstract Methods:\n        - `load_data`: Load processed data from the specified path and file.\n        - `save_data`: Save processed data to the specified path and file.\n\n    Example:\n        ```\n        loader = SomeConcreteLoader()\n        data = loader.load_data(path=Path(\"/data\"), name=\"processed_data.csv\")\n        loader.save_data(data=data, path=Path(\"/data\"), name=\"saved_data.csv\")\n        ```\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initializes the BaseLoader, defining the `load_data` abstract method.\"\"\"\n        super().__init__()\n\n    @abstractmethod\n    def load_data(self, path: Path):\n        \"\"\"Loads the processed data from the specified path.\n\n        Args:\n            path (Path): Directory path for the processed data.\n        \"\"\"\n\n    @abstractmethod\n    def save_data(self, data: pd.DataFrame, path: Union[str, Path]) -&gt; None:\n        \"\"\"Saves the processed data to the specified path as a CSV file.\n\n        Args:\n            data (pd.DataFrame): The processed DataFrame.\n            path (Union[str, Path]): Path (including filename) where data will be saved.\n\n        Raises:\n            ValueError: If the DataFrame is empty.\n        \"\"\"\n        path = Path(path)\n        if not path.is_absolute():\n            path = Path.cwd() / path\n        if data.empty:\n            raise ValueError(\"Data must be processed before saving.\")\n        if path.suffix != \".csv\":\n            path = path.with_suffix(\".csv\")\n        os.makedirs(path.parent, exist_ok=True)\n        data.to_csv(path, index=False)\n        print(f\"Data saved to {path}\")\n</code></pre>"},{"location":"reference/data/baseloader/#periomod.data.BaseLoader.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the BaseLoader, defining the <code>load_data</code> abstract method.</p> Source code in <code>periomod/data/_basedata.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes the BaseLoader, defining the `load_data` abstract method.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"reference/data/baseloader/#periomod.data.BaseLoader.load_data","title":"<code>load_data(path)</code>  <code>abstractmethod</code>","text":"<p>Loads the processed data from the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Directory path for the processed data.</p> required Source code in <code>periomod/data/_basedata.py</code> <pre><code>@abstractmethod\ndef load_data(self, path: Path):\n    \"\"\"Loads the processed data from the specified path.\n\n    Args:\n        path (Path): Directory path for the processed data.\n    \"\"\"\n</code></pre>"},{"location":"reference/data/baseloader/#periomod.data.BaseLoader.save_data","title":"<code>save_data(data, path)</code>  <code>abstractmethod</code>","text":"<p>Saves the processed data to the specified path as a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The processed DataFrame.</p> required <code>path</code> <code>Union[str, Path]</code> <p>Path (including filename) where data will be saved.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the DataFrame is empty.</p> Source code in <code>periomod/data/_basedata.py</code> <pre><code>@abstractmethod\ndef save_data(self, data: pd.DataFrame, path: Union[str, Path]) -&gt; None:\n    \"\"\"Saves the processed data to the specified path as a CSV file.\n\n    Args:\n        data (pd.DataFrame): The processed DataFrame.\n        path (Union[str, Path]): Path (including filename) where data will be saved.\n\n    Raises:\n        ValueError: If the DataFrame is empty.\n    \"\"\"\n    path = Path(path)\n    if not path.is_absolute():\n        path = Path.cwd() / path\n    if data.empty:\n        raise ValueError(\"Data must be processed before saving.\")\n    if path.suffix != \".csv\":\n        path = path.with_suffix(\".csv\")\n    os.makedirs(path.parent, exist_ok=True)\n    data.to_csv(path, index=False)\n    print(f\"Data saved to {path}\")\n</code></pre>"},{"location":"reference/data/baseprocessor/","title":"BaseProcessor","text":"<p>               Bases: <code>BaseLoader</code>, <code>ABC</code></p> <p>Abstract base class defining essential data processing methods.</p> <p>This class provides core processing capabilities such as loading and saving data, along with abstract methods that must be implemented by any subclass. These methods include data imputation, feature creation, and outcome variable generation for specialized data processing.</p> Inherits <ul> <li><code>BaseLoader</code>: Provides loading and saving capabilities for processed data.</li> <li><code>ABC</code>: Specifies abstract methods for subclasses to implement.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>behavior</code> <code>bool</code> <p>If True, includes behavior columns in the data processing.</p> required <p>Attributes:</p> Name Type Description <code>behavior</code> <code>bool</code> <p>Flag indicating whether to include behavior columns during data processing.</p> <p>Methods:</p> Name Description <code>load_data</code> <p>Load processed data from the specified path and file.</p> <code>save_data</code> <p>Save processed data to the specified path and file.</p> Abstract Methods <ul> <li><code>impute_missing_values</code>: Impute missing values in the DataFrame.</li> <li><code>create_tooth_features</code>: Generate features related to tooth data.</li> <li><code>create_outcome_variables</code>: Create outcome variables for analysis.</li> <li><code>process_data</code>: Clean, impute, and scale the data.</li> </ul> Source code in <code>periomod/data/_basedata.py</code> <pre><code>class BaseProcessor(BaseLoader, ABC):\n    \"\"\"Abstract base class defining essential data processing methods.\n\n    This class provides core processing capabilities such as loading and saving\n    data, along with abstract methods that must be implemented by any subclass.\n    These methods include data imputation, feature creation, and outcome variable\n    generation for specialized data processing.\n\n    Inherits:\n        - `BaseLoader`: Provides loading and saving capabilities for processed data.\n        - `ABC`: Specifies abstract methods for subclasses to implement.\n\n    Args:\n        behavior (bool): If True, includes behavior columns in the data processing.\n\n    Attributes:\n        behavior (bool): Flag indicating whether to include behavior columns\n            during data processing.\n\n    Methods:\n        load_data: Load processed data from the specified path and file.\n        save_data: Save processed data to the specified path and file.\n\n    Abstract Methods:\n        - `impute_missing_values`: Impute missing values in the DataFrame.\n        - `create_tooth_features`: Generate features related to tooth data.\n        - `create_outcome_variables`: Create outcome variables for analysis.\n        - `process_data`: Clean, impute, and scale the data.\n    \"\"\"\n\n    def __init__(self, behavior: bool) -&gt; None:\n        \"\"\"Initializes the BaseProcessor with behavior flag.\"\"\"\n        super().__init__()\n        self.behavior = behavior\n\n    def load_data(\n        self,\n        path: Union[str, Path] = Path(\"data/raw/raw_data.xlsx\"),\n    ) -&gt; pd.DataFrame:\n        \"\"\"Loads the dataset and validates required columns.\n\n        Args:\n            path (Union[str, Path], optional): Path to the dataset file.\n                Defaults to Path(\"data/raw/raw_data.xlsx\").\n\n        Returns:\n            pd.DataFrame: The loaded DataFrame.\n\n        Raises:\n            FileNotFoundError: If the specified file does not exist.\n        \"\"\"\n        path = Path(path)\n        if not path.is_absolute():\n            path = Path.cwd() / path\n\n        if not path.exists():\n            raise FileNotFoundError(f\"File not found: {path}\")\n\n        data = pd.read_excel(path, header=[1])\n\n        actual_columns_lower = {col.lower(): col for col in data.columns}\n        required_columns_lower = [col.lower() for col in self.required_columns]\n\n        missing_columns = [\n            col for col in required_columns_lower if col not in actual_columns_lower\n        ]\n        if missing_columns:\n            missing_columns_names = [\n                self.required_columns[required_columns_lower.index(col)]\n                for col in missing_columns\n            ]\n            warnings.warn(\n                f\"Warning: Missing columns: {', '.join(missing_columns_names)}\",\n                stacklevel=2,\n            )\n\n        available_required_columns = [\n            col for col in required_columns_lower if col in actual_columns_lower\n        ]\n        actual_required_columns = [\n            actual_columns_lower[col] for col in available_required_columns\n        ]\n\n        if self.behavior:\n            behavior_columns_lower = [\n                col.lower() for col in self.behavior_columns[\"binary\"]\n            ] + [col.lower() for col in self.behavior_columns[\"categorical\"]]\n            missing_behavior_columns = [\n                col for col in behavior_columns_lower if col not in actual_columns_lower\n            ]\n            if missing_behavior_columns:\n                missing_behavior_names = [\n                    col.capitalize() for col in missing_behavior_columns\n                ]\n                warnings.warn(\n                    f\"Warning: Missing cols: {', '.join(missing_behavior_names)}\",\n                    stacklevel=2,\n                )\n\n            available_behavior_columns = [\n                col for col in behavior_columns_lower if col in actual_columns_lower\n            ]\n            actual_required_columns += [\n                actual_columns_lower[col] for col in available_behavior_columns\n            ]\n\n        return data[actual_required_columns]\n\n    def save_data(\n        self,\n        data: pd.DataFrame,\n        path: Union[str, Path] = Path(\"data/processed/processed_data.csv\"),\n    ) -&gt; None:\n        \"\"\"Saves the processed DataFrame to a CSV file.\n\n        Args:\n            data (pd.DataFrame): The processed DataFrame.\n            path (str, optional): Directory where dataset is saved.\n                Defaults to Path(\"data/processed/processed_data.csv\".\n        \"\"\"\n        super().save_data(data=data, path=path)\n\n    @abstractmethod\n    def impute_missing_values(self, data: pd.DataFrame):\n        \"\"\"Imputes missing values in the DataFrame.\n\n        Args:\n            data (pd.DataFrame): The DataFrame with potential missing values.\n        \"\"\"\n\n    @abstractmethod\n    def create_tooth_features(self, data: pd.DataFrame):\n        \"\"\"Creates additional features related to tooth data.\n\n        Args:\n            data (pd.DataFrame): The DataFrame containing tooth data.\n        \"\"\"\n\n    @abstractmethod\n    def create_outcome_variables(self, data: pd.DataFrame):\n        \"\"\"Generates outcome variables for analysis.\n\n        Args:\n            data (pd.DataFrame): The DataFrame with original outcome variables.\n        \"\"\"\n\n    @abstractmethod\n    def process_data(self, data: pd.DataFrame):\n        \"\"\"Processes dataset with data cleaning, imputations and scaling.\n\n        Args:\n            data (pd.DataFrame): The input DataFrame.\n        \"\"\"\n</code></pre>"},{"location":"reference/data/baseprocessor/#periomod.data.BaseProcessor.__init__","title":"<code>__init__(behavior)</code>","text":"<p>Initializes the BaseProcessor with behavior flag.</p> Source code in <code>periomod/data/_basedata.py</code> <pre><code>def __init__(self, behavior: bool) -&gt; None:\n    \"\"\"Initializes the BaseProcessor with behavior flag.\"\"\"\n    super().__init__()\n    self.behavior = behavior\n</code></pre>"},{"location":"reference/data/baseprocessor/#periomod.data.BaseProcessor.create_outcome_variables","title":"<code>create_outcome_variables(data)</code>  <code>abstractmethod</code>","text":"<p>Generates outcome variables for analysis.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame with original outcome variables.</p> required Source code in <code>periomod/data/_basedata.py</code> <pre><code>@abstractmethod\ndef create_outcome_variables(self, data: pd.DataFrame):\n    \"\"\"Generates outcome variables for analysis.\n\n    Args:\n        data (pd.DataFrame): The DataFrame with original outcome variables.\n    \"\"\"\n</code></pre>"},{"location":"reference/data/baseprocessor/#periomod.data.BaseProcessor.create_tooth_features","title":"<code>create_tooth_features(data)</code>  <code>abstractmethod</code>","text":"<p>Creates additional features related to tooth data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame containing tooth data.</p> required Source code in <code>periomod/data/_basedata.py</code> <pre><code>@abstractmethod\ndef create_tooth_features(self, data: pd.DataFrame):\n    \"\"\"Creates additional features related to tooth data.\n\n    Args:\n        data (pd.DataFrame): The DataFrame containing tooth data.\n    \"\"\"\n</code></pre>"},{"location":"reference/data/baseprocessor/#periomod.data.BaseProcessor.impute_missing_values","title":"<code>impute_missing_values(data)</code>  <code>abstractmethod</code>","text":"<p>Imputes missing values in the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame with potential missing values.</p> required Source code in <code>periomod/data/_basedata.py</code> <pre><code>@abstractmethod\ndef impute_missing_values(self, data: pd.DataFrame):\n    \"\"\"Imputes missing values in the DataFrame.\n\n    Args:\n        data (pd.DataFrame): The DataFrame with potential missing values.\n    \"\"\"\n</code></pre>"},{"location":"reference/data/baseprocessor/#periomod.data.BaseProcessor.load_data","title":"<code>load_data(path=Path('data/raw/raw_data.xlsx'))</code>","text":"<p>Loads the dataset and validates required columns.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the dataset file. Defaults to Path(\"data/raw/raw_data.xlsx\").</p> <code>Path('data/raw/raw_data.xlsx')</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The loaded DataFrame.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> Source code in <code>periomod/data/_basedata.py</code> <pre><code>def load_data(\n    self,\n    path: Union[str, Path] = Path(\"data/raw/raw_data.xlsx\"),\n) -&gt; pd.DataFrame:\n    \"\"\"Loads the dataset and validates required columns.\n\n    Args:\n        path (Union[str, Path], optional): Path to the dataset file.\n            Defaults to Path(\"data/raw/raw_data.xlsx\").\n\n    Returns:\n        pd.DataFrame: The loaded DataFrame.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n    \"\"\"\n    path = Path(path)\n    if not path.is_absolute():\n        path = Path.cwd() / path\n\n    if not path.exists():\n        raise FileNotFoundError(f\"File not found: {path}\")\n\n    data = pd.read_excel(path, header=[1])\n\n    actual_columns_lower = {col.lower(): col for col in data.columns}\n    required_columns_lower = [col.lower() for col in self.required_columns]\n\n    missing_columns = [\n        col for col in required_columns_lower if col not in actual_columns_lower\n    ]\n    if missing_columns:\n        missing_columns_names = [\n            self.required_columns[required_columns_lower.index(col)]\n            for col in missing_columns\n        ]\n        warnings.warn(\n            f\"Warning: Missing columns: {', '.join(missing_columns_names)}\",\n            stacklevel=2,\n        )\n\n    available_required_columns = [\n        col for col in required_columns_lower if col in actual_columns_lower\n    ]\n    actual_required_columns = [\n        actual_columns_lower[col] for col in available_required_columns\n    ]\n\n    if self.behavior:\n        behavior_columns_lower = [\n            col.lower() for col in self.behavior_columns[\"binary\"]\n        ] + [col.lower() for col in self.behavior_columns[\"categorical\"]]\n        missing_behavior_columns = [\n            col for col in behavior_columns_lower if col not in actual_columns_lower\n        ]\n        if missing_behavior_columns:\n            missing_behavior_names = [\n                col.capitalize() for col in missing_behavior_columns\n            ]\n            warnings.warn(\n                f\"Warning: Missing cols: {', '.join(missing_behavior_names)}\",\n                stacklevel=2,\n            )\n\n        available_behavior_columns = [\n            col for col in behavior_columns_lower if col in actual_columns_lower\n        ]\n        actual_required_columns += [\n            actual_columns_lower[col] for col in available_behavior_columns\n        ]\n\n    return data[actual_required_columns]\n</code></pre>"},{"location":"reference/data/baseprocessor/#periomod.data.BaseProcessor.process_data","title":"<code>process_data(data)</code>  <code>abstractmethod</code>","text":"<p>Processes dataset with data cleaning, imputations and scaling.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame.</p> required Source code in <code>periomod/data/_basedata.py</code> <pre><code>@abstractmethod\ndef process_data(self, data: pd.DataFrame):\n    \"\"\"Processes dataset with data cleaning, imputations and scaling.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame.\n    \"\"\"\n</code></pre>"},{"location":"reference/data/baseprocessor/#periomod.data.BaseProcessor.save_data","title":"<code>save_data(data, path=Path('data/processed/processed_data.csv'))</code>","text":"<p>Saves the processed DataFrame to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The processed DataFrame.</p> required <code>path</code> <code>str</code> <p>Directory where dataset is saved. Defaults to Path(\"data/processed/processed_data.csv\".</p> <code>Path('data/processed/processed_data.csv')</code> Source code in <code>periomod/data/_basedata.py</code> <pre><code>def save_data(\n    self,\n    data: pd.DataFrame,\n    path: Union[str, Path] = Path(\"data/processed/processed_data.csv\"),\n) -&gt; None:\n    \"\"\"Saves the processed DataFrame to a CSV file.\n\n    Args:\n        data (pd.DataFrame): The processed DataFrame.\n        path (str, optional): Directory where dataset is saved.\n            Defaults to Path(\"data/processed/processed_data.csv\".\n    \"\"\"\n    super().save_data(data=data, path=path)\n</code></pre>"},{"location":"reference/data/datahelper/","title":"ProcessDataHelper","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Helper class for processing periodontal data with utility methods.</p> <p>This class provides methods for evaluating tooth infection status, calculating adjacent infected teeth, and imputing values for 'plaque' and 'furcationbaseline' columns based on predefined rules and conditions.</p> Inherits <ul> <li><code>BaseConfig</code>: Provides configuration settings for data processing.</li> </ul> <p>Attributes:</p> Name Type Description <code>teeth_neighbors</code> <code>dict</code> <p>Dictionary mapping each tooth to its adjacent neighbors.</p> <code>sides_with_fur</code> <code>dict</code> <p>Dictionary specifying teeth with furcations and their respective sides.</p> <p>Methods:</p> Name Description <code>check_infection</code> <p>Evaluates infection status based on pocket depth and BOP values.</p> <code>get_adjacent_infected_teeth_count</code> <p>Adds a column to indicate the count of adjacent infected teeth for each tooth.</p> <code>plaque_imputation</code> <p>Imputes values in the 'plaque' column.</p> <code>fur_imputation</code> <p>Imputes values in the 'furcationbaseline' column.</p> Example <pre><code>helper = ProcessDataHelper()\ndata = helper.plaque_imputation(data)\ndata = helper.fur_imputation(data)\ninfected_count_data = helper.get_adjacent_infected_teeth_count(\n    data, patient_col=\"id_patient\", tooth_col=\"tooth\",\n    infection_col=\"infection\"\n)\n</code></pre> Source code in <code>periomod/data/_helpers.py</code> <pre><code>class ProcessDataHelper(BaseConfig):\n    \"\"\"Helper class for processing periodontal data with utility methods.\n\n    This class provides methods for evaluating tooth infection status,\n    calculating adjacent infected teeth, and imputing values for 'plaque' and\n    'furcationbaseline' columns based on predefined rules and conditions.\n\n    Inherits:\n        - `BaseConfig`: Provides configuration settings for data processing.\n\n    Attributes:\n        teeth_neighbors (dict): Dictionary mapping each tooth to its adjacent\n            neighbors.\n        sides_with_fur (dict): Dictionary specifying teeth with furcations and\n            their respective sides.\n\n    Methods:\n        check_infection: Evaluates infection status based on pocket depth and\n            BOP values.\n        get_adjacent_infected_teeth_count: Adds a column to indicate the count\n            of adjacent infected teeth for each tooth.\n        plaque_imputation: Imputes values in the 'plaque' column.\n        fur_imputation: Imputes values in the 'furcationbaseline' column.\n\n    Example:\n        ```\n        helper = ProcessDataHelper()\n        data = helper.plaque_imputation(data)\n        data = helper.fur_imputation(data)\n        infected_count_data = helper.get_adjacent_infected_teeth_count(\n            data, patient_col=\"id_patient\", tooth_col=\"tooth\",\n            infection_col=\"infection\"\n        )\n        ```\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize Preprocessor with helper data without storing the DataFrame.\"\"\"\n        super().__init__()\n        self.teeth_neighbors = _get_teeth_neighbors()\n        self.sides_with_fur = _get_side()\n\n    @staticmethod\n    def check_infection(depth: int, boprevaluation: int) -&gt; int:\n        \"\"\"Check if a given tooth side is infected.\n\n        Args:\n            depth: the depth of the pocket before the therapy\n            boprevaluation: the value of BOP evaluation for the tooth side\n\n        Returns:\n            1 if the tooth side is infected, otherwise 0.\n        \"\"\"\n        if depth &gt; 4:\n            return 1\n        elif depth == 4 and boprevaluation == 2:\n            return 1\n        return 0\n\n    def _tooth_neighbor(self, nr: int) -&gt; Union[np.ndarray, str]:\n        \"\"\"Returns adjacent teeth for a given tooth.\n\n        Args:\n            nr (int): tooth number (11-48)\n\n        Returns:\n            Union[np.ndarray, str]: Array of adjacent teeth, or 'No tooth'\n            if input is invalid.\n        \"\"\"\n        return np.array(self.teeth_neighbors.get(nr, \"No tooth\"))\n\n    def get_adjacent_infected_teeth_count(\n        self, data: pd.DataFrame, patient_col: str, tooth_col: str, infection_col: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"Adds a new column indicating the number of adjacent infected teeth.\n\n        Args:\n            data (pd.DataFrame): Dataset to process.\n            patient_col (str): Name of column containing ID for patients.\n            tooth_col (str): Name of column containing teeth represented in numbers.\n            infection_col (str): Name of column indicating whether a tooth is healthy.\n\n        Returns:\n            pd.DataFrame: Modified dataset with new column 'infected_neighbors'.\n        \"\"\"\n        for patient_id, patient_data in data.groupby(patient_col):\n            infected_teeth = set(\n                patient_data[patient_data[infection_col] == 1][tooth_col]\n            )\n\n            data.loc[data[patient_col] == patient_id, \"infected_neighbors\"] = (\n                patient_data[tooth_col].apply(\n                    lambda tooth, infected_teeth=infected_teeth: sum(\n                        1\n                        for neighbor in self._tooth_neighbor(nr=tooth)\n                        if neighbor in infected_teeth\n                    )\n                )\n            )\n\n        return data\n\n    @staticmethod\n    def _plaque_values(row: pd.Series, modes_dict: dict) -&gt; int:\n        \"\"\"Calculate new values for the Plaque column.\n\n        Args:\n            row (pd.Series): A row from the DataFrame.\n            modes_dict (dict): Dict mapping (tooth, side, pdbaseline_grouped)\n            to the mode plaque value.\n\n        Returns:\n            int: Imputed plaque value for the given row.\n        \"\"\"\n        if row[\"plaque_all_na\"] == 1:\n            key = (row[\"tooth\"], row[\"side\"], row[\"pdbaseline_grouped\"])\n            mode_value = modes_dict.get(key, None)\n            if mode_value is not None:\n                if isinstance(mode_value, tuple) and 2 in mode_value:\n                    return 2\n                elif mode_value == 1:\n                    return 1\n                elif mode_value == 2:\n                    return 2\n        else:\n            if pd.isna(row[\"plaque\"]):\n                return 1\n            else:\n                return row[\"plaque\"]\n        return 1\n\n    def plaque_imputation(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Imputes values for Plaque without affecting other columns.\n\n        Args:\n            data (pd.DataFrame): Input DataFrame with a 'plaque' column.\n\n        Returns:\n            pd.DataFrame: The DataFrame with the imputed 'plaque' values.\n\n        Raises:\n            KeyError: If plaque column is not found in DataFrame.\n        \"\"\"\n        data.columns = [col.lower() for col in data.columns]\n\n        if \"plaque\" not in data.columns:\n            raise KeyError(\"'plaque' column not found in the DataFrame\")\n        data[\"plaque\"] = pd.to_numeric(data[\"plaque\"], errors=\"coerce\")\n\n        conditions_baseline = [\n            data[\"pdbaseline\"] &lt;= 3,\n            (data[\"pdbaseline\"] &gt;= 4) &amp; (data[\"pdbaseline\"] &lt;= 5),\n            data[\"pdbaseline\"] &gt;= 6,\n        ]\n        choices_baseline = [0, 1, 2]\n        data[\"pdbaseline_grouped\"] = np.select(\n            conditions_baseline, choices_baseline, default=-1\n        )\n\n        patients_with_all_nas = data.groupby(self.group_col)[\"plaque\"].apply(\n            lambda x: all(pd.isna(x))\n        )\n        data[\"plaque_all_na\"] = data[self.group_col].isin(\n            patients_with_all_nas[patients_with_all_nas].index\n        )\n\n        grouped_data = data.groupby([\"tooth\", \"side\", \"pdbaseline_grouped\"])\n\n        modes_dict = {}\n        for (tooth, side, baseline_grouped), group in grouped_data:\n            modes = group[\"plaque\"].mode()\n            mode_value = modes.iloc[0] if not modes.empty else None\n            modes_dict[(tooth, side, baseline_grouped)] = mode_value\n\n        data[\"plaque\"] = data.apply(\n            lambda row: self._plaque_values(row=row, modes_dict=modes_dict), axis=1\n        )\n\n        data = data.drop([\"pdbaseline_grouped\", \"plaque_all_na\"], axis=1)\n\n        return data\n\n    def _fur_side(self, nr: int) -&gt; Union[np.ndarray, str]:\n        \"\"\"Returns the sides for the input tooth that should have furcations.\n\n        Args:\n            nr (int): Tooth number.\n\n        Returns:\n            Union[np.ndarray, str]: Sides with furcations, or 'without Furkation'\n            if not applicable.\n        \"\"\"\n        for key, value in self.sides_with_fur.items():\n            if nr in key:\n                return np.array(value)\n        return \"Tooth without Furkation\"\n\n    def _fur_values(self, row: pd.Series) -&gt; int:\n        \"\"\"Calculate values for the FurcationBaseline column.\n\n        Args:\n            row (pd.Series): A row from the DataFrame.\n\n        Returns:\n            int: Imputed value for furcationbaseline.\n\n        Raises:\n            ValueError: If NaN is found in pd- or recbaseline.\n        \"\"\"\n        tooth_fur = [14, 16, 17, 18, 24, 26, 27, 28, 36, 37, 38, 46, 47, 48]\n        if pd.isna(row[\"pdbaseline\"]) or pd.isna(row[\"recbaseline\"]):\n            raise ValueError(\n                \"NaN found in pdbaseline or recbaseline. Check RecBaseline imputation.\"\n            )\n\n        if row[\"furcationbaseline_all_na\"] == 1:\n            if row[\"tooth\"] in tooth_fur:\n                if row[\"side\"] in self._fur_side(nr=row[\"tooth\"]):\n                    if (row[\"pdbaseline\"] + row[\"recbaseline\"]) &lt; 4:\n                        return 0\n                    elif 3 &lt; (row[\"pdbaseline\"] + row[\"recbaseline\"]) &lt; 6:\n                        return 1\n                    else:\n                        return 2\n                else:\n                    return 0\n            else:\n                return 0\n        else:\n            if pd.isna(row[\"furcationbaseline\"]):\n                return 0\n            else:\n                return row[\"furcationbaseline\"]\n\n    def fur_imputation(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Impute the values in the FurcationBaseline column.\n\n        Args:\n            data (pd.DataFrame): Input DataFrame.\n\n        Returns:\n            pd.DataFrame: The DataFrame with imputed values for 'furcationbaseline'.\n\n        Raises:\n            KeyError: If furcationbaseline is not found in DatFrame.\n        \"\"\"\n        if \"furcationbaseline\" not in data.columns:\n            raise KeyError(\"'furcationbaseline' column not found in the DataFrame\")\n\n        patients_with_all_nas = data.groupby(self.group_col)[\"furcationbaseline\"].apply(\n            lambda x: all(pd.isna(x))\n        )\n        data[\"furcationbaseline_all_na\"] = data[self.group_col].isin(\n            patients_with_all_nas[patients_with_all_nas].index\n        )\n\n        data[\"furcationbaseline\"] = data.apply(self._fur_values, axis=1)\n        data = data.drop([\"furcationbaseline_all_na\"], axis=1)\n\n        return data\n</code></pre>"},{"location":"reference/data/datahelper/#periomod.data.ProcessDataHelper.__init__","title":"<code>__init__()</code>","text":"<p>Initialize Preprocessor with helper data without storing the DataFrame.</p> Source code in <code>periomod/data/_helpers.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize Preprocessor with helper data without storing the DataFrame.\"\"\"\n    super().__init__()\n    self.teeth_neighbors = _get_teeth_neighbors()\n    self.sides_with_fur = _get_side()\n</code></pre>"},{"location":"reference/data/datahelper/#periomod.data.ProcessDataHelper.check_infection","title":"<code>check_infection(depth, boprevaluation)</code>  <code>staticmethod</code>","text":"<p>Check if a given tooth side is infected.</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>int</code> <p>the depth of the pocket before the therapy</p> required <code>boprevaluation</code> <code>int</code> <p>the value of BOP evaluation for the tooth side</p> required <p>Returns:</p> Type Description <code>int</code> <p>1 if the tooth side is infected, otherwise 0.</p> Source code in <code>periomod/data/_helpers.py</code> <pre><code>@staticmethod\ndef check_infection(depth: int, boprevaluation: int) -&gt; int:\n    \"\"\"Check if a given tooth side is infected.\n\n    Args:\n        depth: the depth of the pocket before the therapy\n        boprevaluation: the value of BOP evaluation for the tooth side\n\n    Returns:\n        1 if the tooth side is infected, otherwise 0.\n    \"\"\"\n    if depth &gt; 4:\n        return 1\n    elif depth == 4 and boprevaluation == 2:\n        return 1\n    return 0\n</code></pre>"},{"location":"reference/data/datahelper/#periomod.data.ProcessDataHelper.fur_imputation","title":"<code>fur_imputation(data)</code>","text":"<p>Impute the values in the FurcationBaseline column.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame with imputed values for 'furcationbaseline'.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If furcationbaseline is not found in DatFrame.</p> Source code in <code>periomod/data/_helpers.py</code> <pre><code>def fur_imputation(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Impute the values in the FurcationBaseline column.\n\n    Args:\n        data (pd.DataFrame): Input DataFrame.\n\n    Returns:\n        pd.DataFrame: The DataFrame with imputed values for 'furcationbaseline'.\n\n    Raises:\n        KeyError: If furcationbaseline is not found in DatFrame.\n    \"\"\"\n    if \"furcationbaseline\" not in data.columns:\n        raise KeyError(\"'furcationbaseline' column not found in the DataFrame\")\n\n    patients_with_all_nas = data.groupby(self.group_col)[\"furcationbaseline\"].apply(\n        lambda x: all(pd.isna(x))\n    )\n    data[\"furcationbaseline_all_na\"] = data[self.group_col].isin(\n        patients_with_all_nas[patients_with_all_nas].index\n    )\n\n    data[\"furcationbaseline\"] = data.apply(self._fur_values, axis=1)\n    data = data.drop([\"furcationbaseline_all_na\"], axis=1)\n\n    return data\n</code></pre>"},{"location":"reference/data/datahelper/#periomod.data.ProcessDataHelper.get_adjacent_infected_teeth_count","title":"<code>get_adjacent_infected_teeth_count(data, patient_col, tooth_col, infection_col)</code>","text":"<p>Adds a new column indicating the number of adjacent infected teeth.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataset to process.</p> required <code>patient_col</code> <code>str</code> <p>Name of column containing ID for patients.</p> required <code>tooth_col</code> <code>str</code> <p>Name of column containing teeth represented in numbers.</p> required <code>infection_col</code> <code>str</code> <p>Name of column indicating whether a tooth is healthy.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Modified dataset with new column 'infected_neighbors'.</p> Source code in <code>periomod/data/_helpers.py</code> <pre><code>def get_adjacent_infected_teeth_count(\n    self, data: pd.DataFrame, patient_col: str, tooth_col: str, infection_col: str\n) -&gt; pd.DataFrame:\n    \"\"\"Adds a new column indicating the number of adjacent infected teeth.\n\n    Args:\n        data (pd.DataFrame): Dataset to process.\n        patient_col (str): Name of column containing ID for patients.\n        tooth_col (str): Name of column containing teeth represented in numbers.\n        infection_col (str): Name of column indicating whether a tooth is healthy.\n\n    Returns:\n        pd.DataFrame: Modified dataset with new column 'infected_neighbors'.\n    \"\"\"\n    for patient_id, patient_data in data.groupby(patient_col):\n        infected_teeth = set(\n            patient_data[patient_data[infection_col] == 1][tooth_col]\n        )\n\n        data.loc[data[patient_col] == patient_id, \"infected_neighbors\"] = (\n            patient_data[tooth_col].apply(\n                lambda tooth, infected_teeth=infected_teeth: sum(\n                    1\n                    for neighbor in self._tooth_neighbor(nr=tooth)\n                    if neighbor in infected_teeth\n                )\n            )\n        )\n\n    return data\n</code></pre>"},{"location":"reference/data/datahelper/#periomod.data.ProcessDataHelper.plaque_imputation","title":"<code>plaque_imputation(data)</code>","text":"<p>Imputes values for Plaque without affecting other columns.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input DataFrame with a 'plaque' column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame with the imputed 'plaque' values.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If plaque column is not found in DataFrame.</p> Source code in <code>periomod/data/_helpers.py</code> <pre><code>def plaque_imputation(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Imputes values for Plaque without affecting other columns.\n\n    Args:\n        data (pd.DataFrame): Input DataFrame with a 'plaque' column.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the imputed 'plaque' values.\n\n    Raises:\n        KeyError: If plaque column is not found in DataFrame.\n    \"\"\"\n    data.columns = [col.lower() for col in data.columns]\n\n    if \"plaque\" not in data.columns:\n        raise KeyError(\"'plaque' column not found in the DataFrame\")\n    data[\"plaque\"] = pd.to_numeric(data[\"plaque\"], errors=\"coerce\")\n\n    conditions_baseline = [\n        data[\"pdbaseline\"] &lt;= 3,\n        (data[\"pdbaseline\"] &gt;= 4) &amp; (data[\"pdbaseline\"] &lt;= 5),\n        data[\"pdbaseline\"] &gt;= 6,\n    ]\n    choices_baseline = [0, 1, 2]\n    data[\"pdbaseline_grouped\"] = np.select(\n        conditions_baseline, choices_baseline, default=-1\n    )\n\n    patients_with_all_nas = data.groupby(self.group_col)[\"plaque\"].apply(\n        lambda x: all(pd.isna(x))\n    )\n    data[\"plaque_all_na\"] = data[self.group_col].isin(\n        patients_with_all_nas[patients_with_all_nas].index\n    )\n\n    grouped_data = data.groupby([\"tooth\", \"side\", \"pdbaseline_grouped\"])\n\n    modes_dict = {}\n    for (tooth, side, baseline_grouped), group in grouped_data:\n        modes = group[\"plaque\"].mode()\n        mode_value = modes.iloc[0] if not modes.empty else None\n        modes_dict[(tooth, side, baseline_grouped)] = mode_value\n\n    data[\"plaque\"] = data.apply(\n        lambda row: self._plaque_values(row=row, modes_dict=modes_dict), axis=1\n    )\n\n    data = data.drop([\"pdbaseline_grouped\", \"plaque_all_na\"], axis=1)\n\n    return data\n</code></pre>"},{"location":"reference/data/dataloader/","title":"ProcessedDataLoader","text":"<p>               Bases: <code>BaseDataLoader</code></p> <p>Concrete data loader for loading, transforming, and saving processed data.</p> <p>This class implements methods for encoding categorical columns, scaling numeric columns, and transforming data based on the specified task. It supports encoding types such as 'one_hot' and 'target', with optional scaling of numeric columns.</p> Inherits <p><code>BaseDataLoader</code>: Provides core data loading, encoding, and scaling methods.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>The task column name, used to guide specific transformations. Can be 'pocketclosure', 'pocketclosureinf', 'improvement', or 'pdgrouprevaluation'.</p> required <code>encoding</code> <code>Optional[str]</code> <p>Specifies the encoding method for categorical columns. Options include 'one_hot', 'target', or None. Defaults to None.</p> <code>None</code> <code>encode</code> <code>bool</code> <p>If True, applies encoding to categorical columns. Defaults to True.</p> <code>True</code> <code>scale</code> <code>bool</code> <p>If True, applies scaling to numeric columns. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>task</code> <code>str</code> <p>Task column name used during data transformations. Can be 'pocketclosure', 'pocketclosureinf', 'improvement', or 'pdgrouprevaluation'.</p> <code>encoding</code> <code>str</code> <p>Encoding method specified for categorical columns. Options include 'one_hot' or 'target'.</p> <code>encode</code> <code>bool</code> <p>Flag to enable encoding of categorical columns.</p> <code>scale</code> <code>bool</code> <p>Flag to enable scaling of numeric columns.</p> <p>Methods:</p> Name Description <code>encode_categorical_columns</code> <p>Encodes categorical columns based on the specified encoding method.</p> <code>scale_numeric_columns</code> <p>Scales numeric columns to normalize data.</p> <code>transform_data</code> <p>Executes the complete data processing pipeline, including encoding and scaling.</p> Inherited Methods <ul> <li><code>load_data</code>: Load processed data from the specified path and file.</li> <li><code>save_data</code>: Save processed data to the specified path and file.</li> </ul> Example <pre><code>from periomod.data import ProcessedDataLoader\n\n# instantiate with one-hot encoding and scale numerical variables\ndataloader = ProcessedDataLoader(\n    task=\"pocketclosure\", encoding=\"one_hot\", encode=True, scale=True\n)\ndata = dataloader.load_data(path=\"data/processed/processed_data.csv\")\ndata = dataloader.transform_data(data=data)\ndataloader.save_data(data=data, path=\"data/training/training_data.csv\")\n</code></pre> Source code in <code>periomod/data/_dataloader.py</code> <pre><code>class ProcessedDataLoader(BaseDataLoader):\n    \"\"\"Concrete data loader for loading, transforming, and saving processed data.\n\n    This class implements methods for encoding categorical columns, scaling numeric\n    columns, and transforming data based on the specified task. It supports encoding\n    types such as 'one_hot' and 'target', with optional scaling of numeric columns.\n\n    Inherits:\n        `BaseDataLoader`: Provides core data loading, encoding, and scaling methods.\n\n    Args:\n        task (str): The task column name, used to guide specific transformations.\n            Can be 'pocketclosure', 'pocketclosureinf', 'improvement', or\n            'pdgrouprevaluation'.\n        encoding (Optional[str]): Specifies the encoding method for categorical columns.\n            Options include 'one_hot', 'target', or None. Defaults to None.\n        encode (bool, optional): If True, applies encoding to categorical columns.\n            Defaults to True.\n        scale (bool, optional): If True, applies scaling to numeric columns.\n            Defaults to True.\n\n    Attributes:\n        task (str): Task column name used during data transformations. Can be\n            'pocketclosure', 'pocketclosureinf', 'improvement', or 'pdgrouprevaluation'.\n        encoding (str): Encoding method specified for categorical columns. Options\n            include 'one_hot' or 'target'.\n        encode (bool): Flag to enable encoding of categorical columns.\n        scale (bool): Flag to enable scaling of numeric columns.\n\n    Methods:\n        encode_categorical_columns: Encodes categorical columns based on\n            the specified encoding method.\n        scale_numeric_columns: Scales numeric columns to normalize data.\n        transform_data: Executes the complete data processing pipeline,\n            including encoding and scaling.\n\n    Inherited Methods:\n        - `load_data`: Load processed data from the specified path and file.\n        - `save_data`: Save processed data to the specified path and file.\n\n    Example:\n        ```\n        from periomod.data import ProcessedDataLoader\n\n        # instantiate with one-hot encoding and scale numerical variables\n        dataloader = ProcessedDataLoader(\n            task=\"pocketclosure\", encoding=\"one_hot\", encode=True, scale=True\n        )\n        data = dataloader.load_data(path=\"data/processed/processed_data.csv\")\n        data = dataloader.transform_data(data=data)\n        dataloader.save_data(data=data, path=\"data/training/training_data.csv\")\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        encoding: Optional[str] = None,\n        encode: bool = True,\n        scale: bool = True,\n    ) -&gt; None:\n        \"\"\"Initializes the ProcessedDataLoader with the specified task column.\"\"\"\n        super().__init__(task=task, encoding=encoding, encode=encode, scale=scale)\n        if self.encoding == \"one_hot\":\n            self.one_hot_encoder = OneHotEncoder(\n                sparse_output=False, handle_unknown=\"ignore\"\n            )\n\n    def encode_categorical_columns(\n        self, data: pd.DataFrame, fit_encoder: bool = True\n    ) -&gt; pd.DataFrame:\n        \"\"\"Encodes categorical columns in the DataFrame.\n\n        Args:\n            data (pd.DataFrame): The DataFrame containing categorical columns.\n            fit_encoder (bool): Whether to fit the encoder on this dataset\n                (only for training data). Defaults to True.\n\n        Returns:\n            pd.DataFrame: The DataFrame with encoded categorical columns.\n\n        Raises:\n            ValueError: If an invalid encoding type is specified.\n        \"\"\"\n        if not self.encode:\n            return data\n\n        cat_vars = [col for col in self.all_cat_vars if col in data.columns]\n        data[cat_vars] = data[cat_vars].apply(\n            lambda col: col.astype(str) if col.dtype in [float, object] else col\n        )\n\n        if self.encoding == \"one_hot\":\n            data_reset = data.reset_index(drop=True)\n\n            if fit_encoder or self.one_hot_encoder is None:\n                encoded_arr = self.one_hot_encoder.fit_transform(data_reset[cat_vars])\n                self.encoded_feature_names = self.one_hot_encoder.get_feature_names_out(\n                    cat_vars\n                )  # Store feature names\n            else:\n                encoded_arr = self.one_hot_encoder.transform(data_reset[cat_vars])\n\n            encoded_cols = self.one_hot_encoder.get_feature_names_out(cat_vars)\n            encoded_df = pd.DataFrame(\n                encoded_arr, columns=encoded_cols, index=data.index\n            )\n\n            if not fit_encoder and hasattr(self, \"encoded_feature_names\"):\n                missing_cols = set(self.encoded_feature_names) - set(encoded_df.columns)\n                for col in missing_cols:\n                    encoded_df[col] = 0  # Add missing features as 0\n\n                encoded_df = encoded_df[list(self.encoded_feature_names)]\n\n            data_encoded = pd.concat(\n                [data_reset.drop(cat_vars, axis=1), encoded_df], axis=1\n            )\n\n        elif self.encoding == \"target\":\n            data[\"toothside\"] = (\n                data[\"tooth\"].astype(str) + \"_\" + data[\"side\"].astype(str)\n            )\n            data_encoded = data.drop(columns=[\"tooth\", \"side\"])\n\n        else:\n            raise ValueError(\n                f\"Invalid encoding '{self.encoding}' specified. \"\n                \"Choose 'one_hot', 'target', or None.\"\n            )\n\n        self._check_encoded_columns(data=data_encoded)\n        return data_encoded\n\n    def scale_numeric_columns(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Scales numeric columns in the DataFrame.\n\n        Args:\n            data (pd.DataFrame): The DataFrame containing numeric columns.\n\n        Returns:\n            data: The DataFrame with scaled numeric columns.\n        \"\"\"\n        scale_vars = [col for col in self.scale_vars if col in data.columns]\n        data[scale_vars] = data[scale_vars].apply(pd.to_numeric, errors=\"coerce\")\n        scaled_values = StandardScaler().fit_transform(data[scale_vars])\n        data[scale_vars] = pd.DataFrame(\n            scaled_values, columns=scale_vars, index=data.index\n        )\n        self._check_scaled_columns(data=data)\n        return data\n\n    def transform_data(\n        self, data: pd.DataFrame, fit_encoder: bool = True\n    ) -&gt; pd.DataFrame:\n        \"\"\"Select task column and optionally, scale and encode.\n\n        Args:\n            data (pd.DataFrame): The DataFrame to transform.\n            fit_encoder (bool): Whether to fit the encoder on this dataset\n                (only for training data). Defaults to True.\n\n        Returns:\n            pd.DataFrame: DataFrame with the selected task 'y'.\n\n        Raises:\n            ValueError: If self.task is invalid.\n        \"\"\"\n        if self.encode:\n            data = self.encode_categorical_columns(data=data, fit_encoder=fit_encoder)\n        if self.scale:\n            data = self.scale_numeric_columns(data=data)\n\n        if self.task not in self.task_cols:\n            raise ValueError(f\"Task '{self.task}' not supported.\")\n\n        if (\n            self.task in [\"improvement\", \"pocketclosureinf\"]\n            and \"pdgroupbase\" in data.columns\n        ):\n            data = data.query(\"pdgroupbase in [1, 2]\")\n            if self.task == \"pocketclosureinf\":\n                self.task = \"pocketclosure\"\n\n        cols_to_drop = [\n            col for col in self.task_cols if col != self.task and col in data.columns\n        ] + self.no_train_cols\n\n        data = data.drop(columns=cols_to_drop, errors=\"ignore\").rename(\n            columns={self.task: \"y\"}\n        )\n\n        if \"y\" not in data.columns:\n            raise ValueError(f\"task column '{self.task}' was not renamed to 'y'.\")\n\n        return data\n</code></pre>"},{"location":"reference/data/dataloader/#periomod.data.ProcessedDataLoader.__init__","title":"<code>__init__(task, encoding=None, encode=True, scale=True)</code>","text":"<p>Initializes the ProcessedDataLoader with the specified task column.</p> Source code in <code>periomod/data/_dataloader.py</code> <pre><code>def __init__(\n    self,\n    task: str,\n    encoding: Optional[str] = None,\n    encode: bool = True,\n    scale: bool = True,\n) -&gt; None:\n    \"\"\"Initializes the ProcessedDataLoader with the specified task column.\"\"\"\n    super().__init__(task=task, encoding=encoding, encode=encode, scale=scale)\n    if self.encoding == \"one_hot\":\n        self.one_hot_encoder = OneHotEncoder(\n            sparse_output=False, handle_unknown=\"ignore\"\n        )\n</code></pre>"},{"location":"reference/data/dataloader/#periomod.data.ProcessedDataLoader.encode_categorical_columns","title":"<code>encode_categorical_columns(data, fit_encoder=True)</code>","text":"<p>Encodes categorical columns in the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame containing categorical columns.</p> required <code>fit_encoder</code> <code>bool</code> <p>Whether to fit the encoder on this dataset (only for training data). Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame with encoded categorical columns.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid encoding type is specified.</p> Source code in <code>periomod/data/_dataloader.py</code> <pre><code>def encode_categorical_columns(\n    self, data: pd.DataFrame, fit_encoder: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"Encodes categorical columns in the DataFrame.\n\n    Args:\n        data (pd.DataFrame): The DataFrame containing categorical columns.\n        fit_encoder (bool): Whether to fit the encoder on this dataset\n            (only for training data). Defaults to True.\n\n    Returns:\n        pd.DataFrame: The DataFrame with encoded categorical columns.\n\n    Raises:\n        ValueError: If an invalid encoding type is specified.\n    \"\"\"\n    if not self.encode:\n        return data\n\n    cat_vars = [col for col in self.all_cat_vars if col in data.columns]\n    data[cat_vars] = data[cat_vars].apply(\n        lambda col: col.astype(str) if col.dtype in [float, object] else col\n    )\n\n    if self.encoding == \"one_hot\":\n        data_reset = data.reset_index(drop=True)\n\n        if fit_encoder or self.one_hot_encoder is None:\n            encoded_arr = self.one_hot_encoder.fit_transform(data_reset[cat_vars])\n            self.encoded_feature_names = self.one_hot_encoder.get_feature_names_out(\n                cat_vars\n            )  # Store feature names\n        else:\n            encoded_arr = self.one_hot_encoder.transform(data_reset[cat_vars])\n\n        encoded_cols = self.one_hot_encoder.get_feature_names_out(cat_vars)\n        encoded_df = pd.DataFrame(\n            encoded_arr, columns=encoded_cols, index=data.index\n        )\n\n        if not fit_encoder and hasattr(self, \"encoded_feature_names\"):\n            missing_cols = set(self.encoded_feature_names) - set(encoded_df.columns)\n            for col in missing_cols:\n                encoded_df[col] = 0  # Add missing features as 0\n\n            encoded_df = encoded_df[list(self.encoded_feature_names)]\n\n        data_encoded = pd.concat(\n            [data_reset.drop(cat_vars, axis=1), encoded_df], axis=1\n        )\n\n    elif self.encoding == \"target\":\n        data[\"toothside\"] = (\n            data[\"tooth\"].astype(str) + \"_\" + data[\"side\"].astype(str)\n        )\n        data_encoded = data.drop(columns=[\"tooth\", \"side\"])\n\n    else:\n        raise ValueError(\n            f\"Invalid encoding '{self.encoding}' specified. \"\n            \"Choose 'one_hot', 'target', or None.\"\n        )\n\n    self._check_encoded_columns(data=data_encoded)\n    return data_encoded\n</code></pre>"},{"location":"reference/data/dataloader/#periomod.data.ProcessedDataLoader.scale_numeric_columns","title":"<code>scale_numeric_columns(data)</code>","text":"<p>Scales numeric columns in the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame containing numeric columns.</p> required <p>Returns:</p> Name Type Description <code>data</code> <code>DataFrame</code> <p>The DataFrame with scaled numeric columns.</p> Source code in <code>periomod/data/_dataloader.py</code> <pre><code>def scale_numeric_columns(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Scales numeric columns in the DataFrame.\n\n    Args:\n        data (pd.DataFrame): The DataFrame containing numeric columns.\n\n    Returns:\n        data: The DataFrame with scaled numeric columns.\n    \"\"\"\n    scale_vars = [col for col in self.scale_vars if col in data.columns]\n    data[scale_vars] = data[scale_vars].apply(pd.to_numeric, errors=\"coerce\")\n    scaled_values = StandardScaler().fit_transform(data[scale_vars])\n    data[scale_vars] = pd.DataFrame(\n        scaled_values, columns=scale_vars, index=data.index\n    )\n    self._check_scaled_columns(data=data)\n    return data\n</code></pre>"},{"location":"reference/data/dataloader/#periomod.data.ProcessedDataLoader.transform_data","title":"<code>transform_data(data, fit_encoder=True)</code>","text":"<p>Select task column and optionally, scale and encode.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame to transform.</p> required <code>fit_encoder</code> <code>bool</code> <p>Whether to fit the encoder on this dataset (only for training data). Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with the selected task 'y'.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If self.task is invalid.</p> Source code in <code>periomod/data/_dataloader.py</code> <pre><code>def transform_data(\n    self, data: pd.DataFrame, fit_encoder: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"Select task column and optionally, scale and encode.\n\n    Args:\n        data (pd.DataFrame): The DataFrame to transform.\n        fit_encoder (bool): Whether to fit the encoder on this dataset\n            (only for training data). Defaults to True.\n\n    Returns:\n        pd.DataFrame: DataFrame with the selected task 'y'.\n\n    Raises:\n        ValueError: If self.task is invalid.\n    \"\"\"\n    if self.encode:\n        data = self.encode_categorical_columns(data=data, fit_encoder=fit_encoder)\n    if self.scale:\n        data = self.scale_numeric_columns(data=data)\n\n    if self.task not in self.task_cols:\n        raise ValueError(f\"Task '{self.task}' not supported.\")\n\n    if (\n        self.task in [\"improvement\", \"pocketclosureinf\"]\n        and \"pdgroupbase\" in data.columns\n    ):\n        data = data.query(\"pdgroupbase in [1, 2]\")\n        if self.task == \"pocketclosureinf\":\n            self.task = \"pocketclosure\"\n\n    cols_to_drop = [\n        col for col in self.task_cols if col != self.task and col in data.columns\n    ] + self.no_train_cols\n\n    data = data.drop(columns=cols_to_drop, errors=\"ignore\").rename(\n        columns={self.task: \"y\"}\n    )\n\n    if \"y\" not in data.columns:\n        raise ValueError(f\"task column '{self.task}' was not renamed to 'y'.\")\n\n    return data\n</code></pre>"},{"location":"reference/data/staticengine/","title":"StaticProcessEngine","text":"<p>               Bases: <code>BaseProcessor</code></p> <p>Concrete implementation for preprocessing a periodontal dataset for ML.</p> <p>This class extends <code>BaseProcessor</code> and provides specific implementations for imputing missing values, creating tooth-related features, and generating outcome variables tailored for periodontal data analysis.</p> Inherits <ul> <li><code>BaseProcessor</code>: Provides core data processing methods and abstract method     definitions for required preprocessing steps.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>behavior</code> <code>bool</code> <p>If True, includes behavioral columns in processing. Defaults to False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Enables verbose logging of data processing steps if True. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>behavior</code> <code>bool</code> <p>Indicates whether to include behavior columns in processing.</p> <code>verbose</code> <code>bool</code> <p>Flag to enable or disable verbose logging.</p> <p>Methods:</p> Name Description <code>impute_missing_values</code> <p>Impute missing values specifically for periodontal data.</p> <code>create_tooth_features</code> <p>Generate tooth-related features, leveraging domain knowledge of periodontal data.</p> <code>create_outcome_variables</code> <p>Create variables representing clinical outcomes.</p> <code>process_data</code> <p>Execute a full processing pipeline including cleaning, imputing, scaling, and feature creation.</p> Inherited Methods <ul> <li><code>load_data</code>: Load processed data from the specified path and file.</li> <li><code>save_data</code>: Save processed data to the specified path and file.</li> </ul> Example <pre><code>from periomod.data import StaticProcessEngine\n\nengine = StaticProcessEngine()\ndata = engine.load_data(path=\"data/raw/raw_data.xlsx\")\ndata = engine.process_data(data)\nengine.save_data(data=data, path=\"data/processed/processed_data.csv\")\n</code></pre> Source code in <code>periomod/data/_preprocessing.py</code> <pre><code>class StaticProcessEngine(BaseProcessor):\n    \"\"\"Concrete implementation for preprocessing a periodontal dataset for ML.\n\n    This class extends `BaseProcessor` and provides specific implementations\n    for imputing missing values, creating tooth-related features, and generating\n    outcome variables tailored for periodontal data analysis.\n\n    Inherits:\n        - `BaseProcessor`: Provides core data processing methods and abstract method\n            definitions for required preprocessing steps.\n\n    Args:\n        behavior (bool): If True, includes behavioral columns in processing.\n            Defaults to False.\n        verbose (bool): Enables verbose logging of data processing steps if True.\n            Defaults to True.\n\n    Attributes:\n        behavior (bool): Indicates whether to include behavior columns in processing.\n        verbose (bool): Flag to enable or disable verbose logging.\n\n    Methods:\n        impute_missing_values: Impute missing values specifically for\n            periodontal data.\n        create_tooth_features: Generate tooth-related features, leveraging\n            domain knowledge of periodontal data.\n        create_outcome_variables: Create variables representing clinical\n            outcomes.\n        process_data: Execute a full processing pipeline including cleaning,\n            imputing, scaling, and feature creation.\n\n    Inherited Methods:\n        - `load_data`: Load processed data from the specified path and file.\n        - `save_data`: Save processed data to the specified path and file.\n\n    Example:\n        ```\n        from periomod.data import StaticProcessEngine\n\n        engine = StaticProcessEngine()\n        data = engine.load_data(path=\"data/raw/raw_data.xlsx\")\n        data = engine.process_data(data)\n        engine.save_data(data=data, path=\"data/processed/processed_data.csv\")\n        ```\n    \"\"\"\n\n    def __init__(self, behavior: bool = False, verbose: bool = True) -&gt; None:\n        \"\"\"Initializes the StaticProcessEngine.\"\"\"\n        super().__init__(behavior=behavior)\n        self.verbose = verbose\n        self.helper = ProcessDataHelper()\n\n    @staticmethod\n    def impute_missing_values(data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Imputes missing values in the DataFrame.\n\n        Imputation rules exist for a predefined set of variables.\n        The method will only impute the columns present in the dataframe.\n\n        Args:\n            data (pd.DataFrame): The DataFrame with missing values.\n\n        Returns:\n            data: The DataFrame with imputed missing values.\n        \"\"\"\n        pd.set_option(\"future.no_silent_downcasting\", True)\n        if data.isna().to_numpy().any():\n            missing_values = data.isna().sum()\n            warnings.warn(\n                f\"Missing values found: \\n{missing_values[missing_values &gt; 0]}\",\n                stacklevel=2,\n            )\n\n        imputation_rules = {\n            \"boprevaluation\": lambda x: x.replace([\"\", \"NA\", \"-\", \" \"], np.nan)\n            .fillna(1)\n            .astype(float),\n            \"recbaseline\": lambda x: x.fillna(1).astype(float),\n            \"bop\": lambda x: x.fillna(1).astype(float),\n            \"percussion-sensitivity\": lambda x: x.fillna(1).astype(float),\n            \"sensitivity\": lambda x: x.fillna(1).astype(float),\n            \"bodymassindex\": lambda x: pd.to_numeric(x, errors=\"coerce\")\n            .fillna(pd.to_numeric(x, errors=\"coerce\").mean())\n            .astype(float),\n            \"periofamilyhistory\": lambda x: x.fillna(2).astype(int),\n            \"restoration\": lambda x: x.fillna(0).astype(int),\n            \"smokingtype\": lambda x: x.fillna(1).astype(int),\n            \"cigarettenumber\": lambda x: x.fillna(0).astype(float),\n            \"diabetes\": lambda x: x.fillna(1).astype(int),\n            \"stresslvl\": lambda x: np.select(\n                [\n                    (x - 1).fillna(x.median()).astype(float) &lt;= 3,\n                    ((x - 1).fillna(x.median()).astype(float) &gt;= 4)\n                    &amp; ((x - 1).fillna(x.median()).astype(float) &lt;= 6),\n                    (x - 1).fillna(x.median()).astype(float) &gt;= 7,\n                ],\n                [0, 1, 2],\n                default=-1,\n            ).astype(int),\n        }\n\n        for column, impute_func in imputation_rules.items():\n            if column in data.columns:\n                data[column] = impute_func(data[column])\n            else:\n                warnings.warn(\n                    f\"Column '{column}' is missing from DataFrame and was not imputed.\",\n                    stacklevel=2,\n                )\n        missing_or_incorrect_tooth_rows = data[\n            data[\"toothtype\"].isna()\n            | data[\"rootnumber\"].isna()\n            | data.apply(\n                lambda row: (row[\"toothtype\"], row[\"rootnumber\"])\n                != _impute_tooth_features(row),\n                axis=1,\n            )\n        ]\n\n        data.loc[missing_or_incorrect_tooth_rows.index, [\"toothtype\", \"rootnumber\"]] = (\n            missing_or_incorrect_tooth_rows.apply(\n                lambda row: _impute_tooth_features(row), axis=1\n            ).tolist()\n        )\n\n        return data\n\n    def create_tooth_features(\n        self, data: pd.DataFrame, neighbors: bool = True, patient_id: bool = True\n    ) -&gt; pd.DataFrame:\n        \"\"\"Creates side_infected, tooth_infected, and infected_neighbors columns.\n\n        Args:\n            data (pd.DataFrame): The input dataframe containing patient data.\n            neighbors (bool): Compute the count of adjacent infected teeth.\n                Defaults to True.\n            patient_id (bool): Flag to indicate whether 'id_patient' is required\n                when creating the 'tooth_infected' column. If True, 'id_patient' is\n                included in the grouping; otherwise, it is not. Defaults to True.\n\n        Returns:\n            data: The dataframe with additional tooth-related features.\n        \"\"\"\n        data[\"side_infected\"] = data.apply(\n            lambda row: self.helper.check_infection(\n                depth=row[\"pdbaseline\"], boprevaluation=row[\"bop\"]\n            ),\n            axis=1,\n        )\n        if patient_id:\n            data[\"tooth_infected\"] = (\n                data.groupby([self.group_col, \"tooth\"])[\"side_infected\"]\n                .transform(lambda x: (x == 1).any())\n                .astype(int)\n            )\n        else:\n            data[\"tooth_infected\"] = (\n                data.groupby(\"tooth\")[\"side_infected\"]\n                .transform(lambda x: (x == 1).any())\n                .astype(int)\n            )\n        if neighbors:\n            data = self.helper.get_adjacent_infected_teeth_count(\n                data=data,\n                patient_col=self.group_col,\n                tooth_col=\"tooth\",\n                infection_col=\"tooth_infected\",\n            )\n\n        return data\n\n    @staticmethod\n    def create_outcome_variables(data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Adds outcome variables to the DataFrame.\n\n        Args:\n            data (pd.DataFrame): The input DataFrame.\n\n        Returns:\n            data: The DataFrame with new outcome variables.\n        \"\"\"\n        data[\"pocketclosure\"] = data.apply(\n            lambda row: (\n                0\n                if row[\"pdrevaluation\"] == 4\n                and row[\"boprevaluation\"] == 2\n                or row[\"pdrevaluation\"] &gt; 4\n                else 1\n            ),\n            axis=1,\n        )\n        data[\"pdgroupbase\"] = data[\"pdbaseline\"].apply(\n            lambda x: 0 if x &lt;= 3 else (1 if x in [4, 5] else 2)\n        )\n        data[\"pdgrouprevaluation\"] = data[\"pdrevaluation\"].apply(\n            lambda x: 0 if x &lt;= 3 else (1 if x in [4, 5] else 2)\n        )\n        data[\"improvement\"] = (data[\"pdrevaluation\"] &lt; data[\"pdbaseline\"]).astype(int)\n        return data\n\n    def process_data(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Processes dataset with data cleaning, imputation and transformation.\n\n        Args:\n            data (pd.DataFrame): The input DataFrame.\n\n        Returns:\n            data: The imputed Dataframe with added feature and target columns.\n        \"\"\"\n        pd.set_option(\"future.no_silent_downcasting\", True)\n        data.columns = [col.lower() for col in data.columns]\n        initial_patients = data[self.group_col].nunique()\n        initial_rows = len(data)\n        if \"age\" in data.columns and \"pregnant\" in data.columns:\n            under_age_or_pregnant = data[(data[\"age\"] &lt; 18) | (data[\"pregnant\"] == 2)]\n            removed_patients = under_age_or_pregnant[self.group_col].nunique()\n            removed_rows = len(under_age_or_pregnant)\n\n            data = (\n                data[data[\"age\"] &gt;= 18]\n                .replace(\" \", pd.NA)\n                .loc[data[\"pregnant\"] != 2]\n                .drop(columns=[\"pregnant\"])\n            )\n        else:\n            warnings.warn(\n                \"Columns 'age' and/or 'pregnant' missing from the dataset.\",\n                stacklevel=2,\n            )\n            removed_patients = removed_rows = 0\n\n        if self.verbose:\n            print(\n                f\"Initial number of patients: {initial_patients}\\n\"\n                f\"Initial number of rows: {initial_rows}\\n\"\n                f\"Number of unique patients removed: {removed_patients}\\n\"\n                f\"Number of rows removed: {removed_rows}\\n\"\n                f\"Remaining number of patients: {data[self.group_col].nunique()}\\n\"\n                f\"Remaining number of rows: {len(data)}\\n\"\n            )\n\n        data = self.create_outcome_variables(\n            self.create_tooth_features(self.impute_missing_values(data=data))\n        )\n\n        if self.behavior:\n            self.bin_vars += [col.lower() for col in self.behavior_columns[\"binary\"]]\n        bin_vars = [col for col in self.bin_vars if col in data.columns]\n        data[bin_vars] = data[bin_vars].replace({1: 0, 2: 1})\n\n        data = data.replace([\"\", \" \"], np.nan)\n        data = self.helper.fur_imputation(self.helper.plaque_imputation(data=data))\n\n        if data.isna().to_numpy().any():\n            missing_values = data.isna().sum()\n            warnings.warn(\n                f\"Missing values: \\n{missing_values[missing_values &gt; 0]}\", stacklevel=2\n            )\n            for col in data.columns:\n                if data[col].isna().sum() &gt; 0:\n                    missing_patients = (\n                        data[data[col].isna()][self.group_col].unique().tolist()\n                    )\n                    if self.verbose:\n                        print(f\"Patients with missing {col}: {missing_patients}\")\n        else:\n            if self.verbose:\n                print(\"No missing values after imputation.\")\n\n        return data\n</code></pre>"},{"location":"reference/data/staticengine/#periomod.data.StaticProcessEngine.__init__","title":"<code>__init__(behavior=False, verbose=True)</code>","text":"<p>Initializes the StaticProcessEngine.</p> Source code in <code>periomod/data/_preprocessing.py</code> <pre><code>def __init__(self, behavior: bool = False, verbose: bool = True) -&gt; None:\n    \"\"\"Initializes the StaticProcessEngine.\"\"\"\n    super().__init__(behavior=behavior)\n    self.verbose = verbose\n    self.helper = ProcessDataHelper()\n</code></pre>"},{"location":"reference/data/staticengine/#periomod.data.StaticProcessEngine.create_outcome_variables","title":"<code>create_outcome_variables(data)</code>  <code>staticmethod</code>","text":"<p>Adds outcome variables to the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>data</code> <code>DataFrame</code> <p>The DataFrame with new outcome variables.</p> Source code in <code>periomod/data/_preprocessing.py</code> <pre><code>@staticmethod\ndef create_outcome_variables(data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Adds outcome variables to the DataFrame.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame.\n\n    Returns:\n        data: The DataFrame with new outcome variables.\n    \"\"\"\n    data[\"pocketclosure\"] = data.apply(\n        lambda row: (\n            0\n            if row[\"pdrevaluation\"] == 4\n            and row[\"boprevaluation\"] == 2\n            or row[\"pdrevaluation\"] &gt; 4\n            else 1\n        ),\n        axis=1,\n    )\n    data[\"pdgroupbase\"] = data[\"pdbaseline\"].apply(\n        lambda x: 0 if x &lt;= 3 else (1 if x in [4, 5] else 2)\n    )\n    data[\"pdgrouprevaluation\"] = data[\"pdrevaluation\"].apply(\n        lambda x: 0 if x &lt;= 3 else (1 if x in [4, 5] else 2)\n    )\n    data[\"improvement\"] = (data[\"pdrevaluation\"] &lt; data[\"pdbaseline\"]).astype(int)\n    return data\n</code></pre>"},{"location":"reference/data/staticengine/#periomod.data.StaticProcessEngine.create_tooth_features","title":"<code>create_tooth_features(data, neighbors=True, patient_id=True)</code>","text":"<p>Creates side_infected, tooth_infected, and infected_neighbors columns.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataframe containing patient data.</p> required <code>neighbors</code> <code>bool</code> <p>Compute the count of adjacent infected teeth. Defaults to True.</p> <code>True</code> <code>patient_id</code> <code>bool</code> <p>Flag to indicate whether 'id_patient' is required when creating the 'tooth_infected' column. If True, 'id_patient' is included in the grouping; otherwise, it is not. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>data</code> <code>DataFrame</code> <p>The dataframe with additional tooth-related features.</p> Source code in <code>periomod/data/_preprocessing.py</code> <pre><code>def create_tooth_features(\n    self, data: pd.DataFrame, neighbors: bool = True, patient_id: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"Creates side_infected, tooth_infected, and infected_neighbors columns.\n\n    Args:\n        data (pd.DataFrame): The input dataframe containing patient data.\n        neighbors (bool): Compute the count of adjacent infected teeth.\n            Defaults to True.\n        patient_id (bool): Flag to indicate whether 'id_patient' is required\n            when creating the 'tooth_infected' column. If True, 'id_patient' is\n            included in the grouping; otherwise, it is not. Defaults to True.\n\n    Returns:\n        data: The dataframe with additional tooth-related features.\n    \"\"\"\n    data[\"side_infected\"] = data.apply(\n        lambda row: self.helper.check_infection(\n            depth=row[\"pdbaseline\"], boprevaluation=row[\"bop\"]\n        ),\n        axis=1,\n    )\n    if patient_id:\n        data[\"tooth_infected\"] = (\n            data.groupby([self.group_col, \"tooth\"])[\"side_infected\"]\n            .transform(lambda x: (x == 1).any())\n            .astype(int)\n        )\n    else:\n        data[\"tooth_infected\"] = (\n            data.groupby(\"tooth\")[\"side_infected\"]\n            .transform(lambda x: (x == 1).any())\n            .astype(int)\n        )\n    if neighbors:\n        data = self.helper.get_adjacent_infected_teeth_count(\n            data=data,\n            patient_col=self.group_col,\n            tooth_col=\"tooth\",\n            infection_col=\"tooth_infected\",\n        )\n\n    return data\n</code></pre>"},{"location":"reference/data/staticengine/#periomod.data.StaticProcessEngine.impute_missing_values","title":"<code>impute_missing_values(data)</code>  <code>staticmethod</code>","text":"<p>Imputes missing values in the DataFrame.</p> <p>Imputation rules exist for a predefined set of variables. The method will only impute the columns present in the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame with missing values.</p> required <p>Returns:</p> Name Type Description <code>data</code> <code>DataFrame</code> <p>The DataFrame with imputed missing values.</p> Source code in <code>periomod/data/_preprocessing.py</code> <pre><code>@staticmethod\ndef impute_missing_values(data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Imputes missing values in the DataFrame.\n\n    Imputation rules exist for a predefined set of variables.\n    The method will only impute the columns present in the dataframe.\n\n    Args:\n        data (pd.DataFrame): The DataFrame with missing values.\n\n    Returns:\n        data: The DataFrame with imputed missing values.\n    \"\"\"\n    pd.set_option(\"future.no_silent_downcasting\", True)\n    if data.isna().to_numpy().any():\n        missing_values = data.isna().sum()\n        warnings.warn(\n            f\"Missing values found: \\n{missing_values[missing_values &gt; 0]}\",\n            stacklevel=2,\n        )\n\n    imputation_rules = {\n        \"boprevaluation\": lambda x: x.replace([\"\", \"NA\", \"-\", \" \"], np.nan)\n        .fillna(1)\n        .astype(float),\n        \"recbaseline\": lambda x: x.fillna(1).astype(float),\n        \"bop\": lambda x: x.fillna(1).astype(float),\n        \"percussion-sensitivity\": lambda x: x.fillna(1).astype(float),\n        \"sensitivity\": lambda x: x.fillna(1).astype(float),\n        \"bodymassindex\": lambda x: pd.to_numeric(x, errors=\"coerce\")\n        .fillna(pd.to_numeric(x, errors=\"coerce\").mean())\n        .astype(float),\n        \"periofamilyhistory\": lambda x: x.fillna(2).astype(int),\n        \"restoration\": lambda x: x.fillna(0).astype(int),\n        \"smokingtype\": lambda x: x.fillna(1).astype(int),\n        \"cigarettenumber\": lambda x: x.fillna(0).astype(float),\n        \"diabetes\": lambda x: x.fillna(1).astype(int),\n        \"stresslvl\": lambda x: np.select(\n            [\n                (x - 1).fillna(x.median()).astype(float) &lt;= 3,\n                ((x - 1).fillna(x.median()).astype(float) &gt;= 4)\n                &amp; ((x - 1).fillna(x.median()).astype(float) &lt;= 6),\n                (x - 1).fillna(x.median()).astype(float) &gt;= 7,\n            ],\n            [0, 1, 2],\n            default=-1,\n        ).astype(int),\n    }\n\n    for column, impute_func in imputation_rules.items():\n        if column in data.columns:\n            data[column] = impute_func(data[column])\n        else:\n            warnings.warn(\n                f\"Column '{column}' is missing from DataFrame and was not imputed.\",\n                stacklevel=2,\n            )\n    missing_or_incorrect_tooth_rows = data[\n        data[\"toothtype\"].isna()\n        | data[\"rootnumber\"].isna()\n        | data.apply(\n            lambda row: (row[\"toothtype\"], row[\"rootnumber\"])\n            != _impute_tooth_features(row),\n            axis=1,\n        )\n    ]\n\n    data.loc[missing_or_incorrect_tooth_rows.index, [\"toothtype\", \"rootnumber\"]] = (\n        missing_or_incorrect_tooth_rows.apply(\n            lambda row: _impute_tooth_features(row), axis=1\n        ).tolist()\n    )\n\n    return data\n</code></pre>"},{"location":"reference/data/staticengine/#periomod.data.StaticProcessEngine.process_data","title":"<code>process_data(data)</code>","text":"<p>Processes dataset with data cleaning, imputation and transformation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>data</code> <code>DataFrame</code> <p>The imputed Dataframe with added feature and target columns.</p> Source code in <code>periomod/data/_preprocessing.py</code> <pre><code>def process_data(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Processes dataset with data cleaning, imputation and transformation.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame.\n\n    Returns:\n        data: The imputed Dataframe with added feature and target columns.\n    \"\"\"\n    pd.set_option(\"future.no_silent_downcasting\", True)\n    data.columns = [col.lower() for col in data.columns]\n    initial_patients = data[self.group_col].nunique()\n    initial_rows = len(data)\n    if \"age\" in data.columns and \"pregnant\" in data.columns:\n        under_age_or_pregnant = data[(data[\"age\"] &lt; 18) | (data[\"pregnant\"] == 2)]\n        removed_patients = under_age_or_pregnant[self.group_col].nunique()\n        removed_rows = len(under_age_or_pregnant)\n\n        data = (\n            data[data[\"age\"] &gt;= 18]\n            .replace(\" \", pd.NA)\n            .loc[data[\"pregnant\"] != 2]\n            .drop(columns=[\"pregnant\"])\n        )\n    else:\n        warnings.warn(\n            \"Columns 'age' and/or 'pregnant' missing from the dataset.\",\n            stacklevel=2,\n        )\n        removed_patients = removed_rows = 0\n\n    if self.verbose:\n        print(\n            f\"Initial number of patients: {initial_patients}\\n\"\n            f\"Initial number of rows: {initial_rows}\\n\"\n            f\"Number of unique patients removed: {removed_patients}\\n\"\n            f\"Number of rows removed: {removed_rows}\\n\"\n            f\"Remaining number of patients: {data[self.group_col].nunique()}\\n\"\n            f\"Remaining number of rows: {len(data)}\\n\"\n        )\n\n    data = self.create_outcome_variables(\n        self.create_tooth_features(self.impute_missing_values(data=data))\n    )\n\n    if self.behavior:\n        self.bin_vars += [col.lower() for col in self.behavior_columns[\"binary\"]]\n    bin_vars = [col for col in self.bin_vars if col in data.columns]\n    data[bin_vars] = data[bin_vars].replace({1: 0, 2: 1})\n\n    data = data.replace([\"\", \" \"], np.nan)\n    data = self.helper.fur_imputation(self.helper.plaque_imputation(data=data))\n\n    if data.isna().to_numpy().any():\n        missing_values = data.isna().sum()\n        warnings.warn(\n            f\"Missing values: \\n{missing_values[missing_values &gt; 0]}\", stacklevel=2\n        )\n        for col in data.columns:\n            if data[col].isna().sum() &gt; 0:\n                missing_patients = (\n                    data[data[col].isna()][self.group_col].unique().tolist()\n                )\n                if self.verbose:\n                    print(f\"Patients with missing {col}: {missing_patients}\")\n    else:\n        if self.verbose:\n            print(\"No missing values after imputation.\")\n\n    return data\n</code></pre>"},{"location":"reference/descriptives/","title":"periomod.descriptives Overview","text":"<p>The <code>periomod.descriptives</code> module provides tools for descriptive analysis, including plotting and data summaries.</p>"},{"location":"reference/descriptives/#available-components","title":"Available Components","text":"Component Description DescriptivesPlotter Class for generating descriptive statistics and plots."},{"location":"reference/descriptives/descriptives/","title":"DescriptivesPlotter","text":"<p>Class for creating various descriptive plots based on periodontal data.</p> <p>This class provides methods for visualizing data through heatmaps, bar plots, 2D histograms, and other descriptive plots to analyze pocket depth and therapy outcomes.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the data for plotting.</p> required <p>Attributes:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Stores the input DataFrame for use in plotting.</p> <p>Methods:</p> Name Description <code>plt_matrix</code> <p>Plots a heatmap/confusion matrix based on two columns.</p> <code>pocket_comparison</code> <p>Creates bar plots to compare values before and after therapy.</p> <code>pocket_group_comparison</code> <p>Generates side-by-side bar plots for pocket depth categories before and after therapy.</p> <code>histogram_2d</code> <p>Creates a 2D histogram plot based on two columns, visualizing values before and after therapy.</p> <code>outcome_descriptive</code> <p>Creates a bar plot for an outcome variable, useful for examining therapy outcomes.</p> Example <pre><code>from periomod.data import ProcessedDataLoader\nfrom periomod.descriptives import DescriptivesPlotter\n\ndf = dataloader.load_data(path=\"data/processed/processed_data.csv\")\n\n# instantiate plotter with dataframe\nplotter = DescriptivesPlotter(df)\nplotter.plt_matrix(vertical=\"pdgrouprevaluation\", horizontal=\"pdgroupbase\")\nplotter.pocket_comparison(col1=\"pdbaseline\", col2=\"pdrevaluation\")\nplotter.histogram_2d(col_before=\"pdbaseline\", col_after=\"pdrevaluation\")\n</code></pre> Source code in <code>periomod/descriptives/_descriptives.py</code> <pre><code>class DescriptivesPlotter:\n    \"\"\"Class for creating various descriptive plots based on periodontal data.\n\n    This class provides methods for visualizing data through heatmaps, bar plots,\n    2D histograms, and other descriptive plots to analyze pocket depth and therapy\n    outcomes.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the data for plotting.\n\n    Attributes:\n        df (pd.DataFrame): Stores the input DataFrame for use in plotting.\n\n    Methods:\n        plt_matrix: Plots a heatmap/confusion matrix based on two columns.\n        pocket_comparison: Creates bar plots to compare values before\n            and after therapy.\n        pocket_group_comparison: Generates side-by-side bar plots for pocket\n            depth categories before and after therapy.\n        histogram_2d: Creates a 2D histogram plot based on two columns, visualizing\n            values before and after therapy.\n        outcome_descriptive: Creates a bar plot for an outcome variable, useful\n            for examining therapy outcomes.\n\n    Example:\n        ```\n        from periomod.data import ProcessedDataLoader\n        from periomod.descriptives import DescriptivesPlotter\n\n        df = dataloader.load_data(path=\"data/processed/processed_data.csv\")\n\n        # instantiate plotter with dataframe\n        plotter = DescriptivesPlotter(df)\n        plotter.plt_matrix(vertical=\"pdgrouprevaluation\", horizontal=\"pdgroupbase\")\n        plotter.pocket_comparison(col1=\"pdbaseline\", col2=\"pdrevaluation\")\n        plotter.histogram_2d(col_before=\"pdbaseline\", col_after=\"pdrevaluation\")\n        ```\n    \"\"\"\n\n    def __init__(self, df: pd.DataFrame) -&gt; None:\n        \"\"\"Initializes DescriptivesPlotter with pd.DataFrame.\n\n        Args:\n            df (pd.DataFrame): DataFrame containing data for plotting.\n        \"\"\"\n        self.df = df\n        plt.rcParams[\"svg.fonttype\"] = \"none\"\n        plt.rcParams[\"font.family\"] = \"Arial\"\n        plt.rcParams[\"axes.labelsize\"] = 12\n\n    def plt_matrix(\n        self,\n        vertical: str,\n        horizontal: str,\n        x_label: str = \"Pocket depth before therapy\",\n        y_label: str = \"Pocket depth after therapy\",\n        name: Optional[str] = None,\n        normalize: str = \"rows\",\n        save: bool = False,\n        color: str = \"#078294\",\n    ) -&gt; None:\n        \"\"\"Plots a heatmap/confusion matrix.\n\n        Args:\n            vertical (str): Column name for the vertical axis.\n            horizontal (str): Column name for the horizontal axis.\n            x_label (str): Label for x-axis. Defaults to \"Pocket depth before therapy\".\n            y_label (str): Label for y-axis. Defaults to \"Pocket depth after therapy\".\n            name (str): Title of the plot and name for saving the plot.\n            normalize (str, optional): Normalization method ('rows' or 'columns').\n                Defaults to 'rows'.\n            save (bool, optional): Save the plot as an SVG. Defaults to False.\n            color: Custom Color mapping for confusion matrix based on #.\n                Defaults to teal.\n\n        Raises:\n            ValueError: If `normalize` is not 'rows' or 'columns'.\n            ValueError: If 'name' argument is not defined when 'save' is True.\"\n        \"\"\"\n        vertical_data = self.df[vertical]\n        horizontal_data = self.df[horizontal]\n        cm = confusion_matrix(vertical_data, horizontal_data)\n        custom_cmap = LinearSegmentedColormap.from_list(\n            \"custom_cmap\", [\"#FFFFFF\", color]\n        )\n\n        if normalize == \"rows\":\n            row_sums = cm.sum(axis=1)\n            normalized_cm = (cm / row_sums[:, np.newaxis]) * 100\n        elif normalize == \"columns\":\n            col_sums = cm.sum(axis=0)\n            normalized_cm = (cm / col_sums) * 100\n        else:\n            raise ValueError(\"Invalid value for 'normalize'. Use 'rows' or 'columns'.\")\n\n        plt.figure(figsize=(6, 4), dpi=300)\n        sns.heatmap(\n            normalized_cm,\n            cmap=custom_cmap,\n            fmt=\"g\",\n            linewidths=0.5,\n            square=True,\n            cbar_kws={\"label\": \"Percent\"},\n        )\n\n        for i in range(len(cm)):\n            for j in range(len(cm)):\n                if normalized_cm[i, j] &gt; 50:\n                    plt.text(\n                        j + 0.5,\n                        i + 0.5,\n                        cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"white\",\n                    )\n                else:\n                    plt.text(j + 0.5, i + 0.5, cm[i, j], ha=\"center\", va=\"center\")\n\n        title = \"Data Overview\"\n\n        plt.title(title, fontsize=12)\n\n        ax = plt.gca()\n        ax.xaxis.set_ticks_position(\"top\")\n        ax.xaxis.set_label_position(\"top\")\n\n        cbar = ax.collections[0].colorbar\n        cbar.outline.set_edgecolor(\"black\")\n        cbar.outline.set_linewidth(1)\n\n        ax.add_patch(\n            Rectangle(\n                (0, 0), cm.shape[1], cm.shape[0], fill=False, edgecolor=\"black\", lw=2\n            )\n        )\n\n        plt.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n        plt.xlabel(x_label, fontsize=12)\n        plt.ylabel(y_label, fontsize=12)\n\n        if save:\n            if name is None:\n                raise ValueError(\"'name' argument required when 'save' is True.\")\n            plt.savefig(name + \".svg\", format=\"svg\", dpi=300)\n\n        plt.show()\n\n    def pocket_comparison(\n        self,\n        col1: str,\n        col2: str,\n        title_1: str = \"Pocket depth before therapy\",\n        title_2: str = \"Pocket depth after therapy\",\n        name: Optional[str] = None,\n        save: bool = False,\n    ) -&gt; None:\n        \"\"\"Creates two bar plots for comparing pocket depth before and after therapy.\n\n        Args:\n            col1 (str): Column name for the first plot (before therapy).\n            col2 (str): Column name for the second plot (after therapy).\n            title_1 (str): Label for x-axis. Defaults to \"Pocket depth before therapy\".\n            title_2 (str): Label for y-axis. Defaults to \"Pocket depth after therapy\".\n            name (str): Name for saving the plot.\n            save (bool, optional): Save the plot as an SVG. Defaults to False.\n\n        Raises:\n            ValueError: If 'name' argument is not defined when 'save' is True.\"\n        \"\"\"\n        value_counts_1 = self.df[col1].value_counts()\n        x_values_1 = value_counts_1.index\n        heights_1 = value_counts_1.to_numpy()\n\n        value_counts_2 = self.df[col2].value_counts()\n        x_values_2 = value_counts_2.index\n        heights_2 = value_counts_2.to_numpy()\n\n        fig, (ax1, ax2) = plt.subplots(\n            1, 2, figsize=(8, 5), sharex=True, sharey=True, dpi=300\n        )\n\n        ax1.bar(x_values_1, heights_1, edgecolor=\"black\", color=\"#078294\", linewidth=1)\n        ax1.set_ylabel(\"Number of sites\", fontsize=12)\n        ax1.set_title(title_1, fontsize=12, pad=10)\n        ax1.set_yticks(np.arange(0, 90001, 10000))\n        ax1.set_xticks(np.arange(1, 12.5, 1))\n        ax1.tick_params(axis=\"both\", labelsize=12)\n\n        ax1.axvline(x=3.5, color=\"red\", linestyle=\"--\", linewidth=1, alpha=0.3)\n        ax1.axvline(x=5.5, color=\"red\", linestyle=\"--\", linewidth=1, alpha=0.3)\n\n        ax1.grid(True, axis=\"y\", color=\"black\", linestyle=\"--\", linewidth=1, alpha=0.3)\n        ax1.spines[\"top\"].set_visible(False)\n        ax1.spines[\"right\"].set_visible(False)\n\n        ax1.tick_params(width=1)\n        for spine in ax1.spines.values():\n            spine.set_linewidth(1)\n\n        ax2.bar(x_values_2, heights_2, edgecolor=\"black\", color=\"#078294\", linewidth=1)\n        ax2.set_title(title_2, fontsize=12, pad=10)\n        ax2.tick_params(axis=\"both\", labelsize=12)\n\n        ax2.axvline(x=3.5, color=\"red\", linestyle=\"--\", linewidth=1, alpha=0.3)\n        ax2.axvline(x=5.5, color=\"red\", linestyle=\"--\", linewidth=1, alpha=0.3)\n\n        ax2.grid(True, axis=\"y\", color=\"black\", linestyle=\"--\", linewidth=1, alpha=0.3)\n        ax2.spines[\"top\"].set_visible(False)\n        ax2.spines[\"right\"].set_visible(False)\n        for spine in ax2.spines.values():\n            spine.set_linewidth(1)\n\n        ax2.tick_params(width=1)\n        for spine in ax2.spines.values():\n            spine.set_linewidth(1)\n\n        fig.supxlabel(\"Pocket Depth [mm]\", fontsize=12)\n        plt.tight_layout()\n\n        if save:\n            if name is None:\n                raise ValueError(\"'name' argument must required when 'save' is True.\")\n            plt.savefig(name + \".svg\", format=\"svg\", dpi=300)\n\n        plt.show()\n\n    def pocket_group_comparison(\n        self,\n        col_before: str,\n        col_after: str,\n        title_1: str = \"Pocket depth before therapy\",\n        title_2: str = \"Pocket depth after therapy\",\n        name: Optional[str] = None,\n        save: bool = False,\n    ) -&gt; None:\n        \"\"\"Creates side-by-side bar plots for pocket depth before and after therapy.\n\n        Args:\n            col_before (str): Column name for the first plot (before therapy).\n            col_after (str): Column name for the second plot (after therapy).\n            title_1 (str): Label for x-axis. Defaults to \"Pocket depth before therapy\".\n            title_2 (str): Label for y-axis. Defaults to \"Pocket depth after therapy\".\n            name (str): Name for saving the plot.\n            save (bool, optional): Save the plot as an SVG. Defaults to False.\n\n        Raises:\n            ValueError: If 'name' argument is not defined when 'save' is True.\"\n        \"\"\"\n        value_counts = self.df[col_before].value_counts()\n        x_values = value_counts.index\n        heights = value_counts.to_numpy()\n        total_values = sum(heights)\n\n        value_counts2 = self.df[col_after].value_counts()\n        x_values2 = value_counts2.index\n        heights2 = value_counts2.to_numpy()\n        total_values2 = sum(heights2)\n\n        fig, (ax1, ax2) = plt.subplots(\n            1, 2, figsize=(8, 4), sharex=True, sharey=True, dpi=300\n        )\n\n        bars1 = ax1.bar(\n            x_values, heights, edgecolor=\"black\", color=\"#078294\", linewidth=1\n        )\n        ax1.set_ylabel(\"Number of sites\", fontsize=12)\n        ax1.set_title(f\"{title_1} (n={total_values})\", fontsize=12, pad=10)\n        ax1.set_yticks(np.arange(0, 100001, 10000))\n        ax1.set_xticks(np.arange(0, 2.1, 1))\n\n        ax1.spines[\"top\"].set_visible(False)\n        ax1.spines[\"right\"].set_visible(False)\n        for spine in ax1.spines.values():\n            spine.set_linewidth(1)\n        ax1.tick_params(axis=\"both\", labelsize=12)\n\n        for bar in bars1:\n            height = bar.get_height()\n            ax1.annotate(\n                \"{}\".format(height),\n                xy=(bar.get_x() + bar.get_width() / 2, height),\n                xytext=(0, 3),\n                textcoords=\"offset points\",\n                ha=\"center\",\n                va=\"bottom\",\n                fontsize=12,\n            )\n\n        bars2 = ax2.bar(\n            x_values2, heights2, edgecolor=\"black\", color=\"#078294\", linewidth=1\n        )\n        ax2.set_title(f\"{title_2} (n={total_values2})\", fontsize=12, pad=10)\n        ax2.spines[\"top\"].set_visible(False)\n        ax2.spines[\"right\"].set_visible(False)\n        for spine in ax2.spines.values():\n            spine.set_linewidth(1)\n\n        ax2.tick_params(axis=\"both\", labelsize=12, width=1)\n\n        for bar in bars2:\n            height = bar.get_height()\n            ax2.annotate(\n                \"{}\".format(height),\n                xy=(bar.get_x() + bar.get_width() / 2, height),\n                xytext=(0, 3),\n                textcoords=\"offset points\",\n                ha=\"center\",\n                va=\"bottom\",\n                fontsize=12,\n            )\n        fig.supxlabel(\"Pocket depth categories\", fontsize=12)\n        plt.tight_layout()\n\n        if save:\n            if name is None:\n                raise ValueError(\"'name' argument must required when 'save' is True.\")\n            plt.savefig(name + \".svg\", format=\"svg\", dpi=300)\n\n        plt.show()\n\n    def histogram_2d(\n        self,\n        col_before: str,\n        col_after: str,\n        x_label: str = \"Pocket depth before therapy [mm]\",\n        y_label: str = \"Pocket depth after therapy [mm]\",\n        name: Optional[str] = None,\n        save: bool = False,\n    ) -&gt; None:\n        \"\"\"Creates a 2D histogram plot based on two columns.\n\n        Args:\n            col_before (str): Column name for pocket depth before therapy.\n            col_after (str): Column name for pocket depth after therapy.\n            x_label (str): Label for x-axis. Defaults to\n                \"Pocket depth before therapy [mm]\".\n            y_label (str): Label for y-axis. Defaults to\n                \"Pocket depth after therapy [mm]\".\n            name (str): Name for saving the plot.\n            save (bool, optional): Save the plot as an SVG. Defaults to False.\n\n        Raises:\n            ValueError: If 'name' argument is not defined when 'save' is True.\"\n        \"\"\"\n        heatmap, _, _ = np.histogram2d(\n            self.df[col_before], self.df[col_after], bins=(12, 12)\n        )\n\n        plt.figure(figsize=(8, 6), dpi=300)\n\n        plt.imshow(heatmap.T, origin=\"lower\", cmap=\"viridis\", interpolation=\"nearest\")\n        cbar = plt.colorbar()\n        cbar.set_label(\"Frequency\", fontsize=12)\n        cbar.outline.set_linewidth(1)\n\n        plt.xlabel(xlabel=x_label, fontsize=12)\n        plt.ylabel(ylabel=y_label, fontsize=12)\n        plt.xticks(np.arange(12), np.arange(1, 13), fontsize=12)\n        plt.yticks(np.arange(12), np.arange(1, 13), fontsize=12)\n\n        ax = plt.gca()\n        for spine in ax.spines.values():\n            spine.set_linewidth(1)\n        ax.tick_params(width=1)\n\n        cbar.ax.tick_params(labelsize=12)\n\n        plt.plot([-0.5, 2.5], [2.5, 2.5], \"r--\", lw=2)\n        plt.plot([2.5, 2.5], [-0.5, 2.5], \"r--\", lw=2)\n\n        if save:\n            if name is None:\n                raise ValueError(\"'name' argument must required when 'save' is True.\")\n            plt.savefig(name + \".svg\", format=\"svg\", dpi=300)\n\n        plt.tight_layout()\n        plt.show()\n\n    def outcome_descriptive(\n        self, outcome: str, title: str, name: Optional[str] = None, save: bool = False\n    ) -&gt; None:\n        \"\"\"Creates a bar plot for the outcome variable.\n\n        Args:\n            outcome (str): Column name for the outcome variable.\n            title (str): Title of the plot.\n            name (str): Filename for saving the plot.\n            save (bool, optional): Save the plot as an SVG. Defaults to False.\n\n        Raises:\n            ValueError: If 'name' argument is not defined when 'save' is True.\"\n        \"\"\"\n        df_temp = self.df\n        if outcome == \"improvement\" and \"pdgroupbase\" in self.df.columns:\n            df_temp = df_temp.query(\"pdgroupbase in [1, 2]\")\n\n        value_counts = df_temp[outcome].value_counts()\n        x_values = value_counts.index.astype(str)\n        heights = value_counts.to_numpy()\n\n        plt.figure(figsize=(4, 4), dpi=300)\n        bars = plt.bar(\n            x_values, heights, edgecolor=\"black\", color=\"#078294\", linewidth=1\n        )\n        plt.ylabel(\"Number of sites\", fontsize=12)\n        plt.title(title, fontsize=12, pad=10)\n\n        for bar in bars:\n            height = bar.get_height()\n            plt.annotate(\n                \"{}\".format(height),\n                xy=(bar.get_x() + bar.get_width() / 2, height),\n                xytext=(0, 3),\n                textcoords=\"offset points\",\n                ha=\"center\",\n                va=\"bottom\",\n                fontsize=12,\n            )\n\n        ax = plt.gca()\n        for spine in ax.spines.values():\n            spine.set_linewidth(1)\n        ax.tick_params(width=1)\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n\n        if save:\n            if name is None:\n                raise ValueError(\"'name' argument is required when 'save' is True.\")\n            plt.savefig(name + \".svg\", format=\"svg\", dpi=300)\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"reference/descriptives/descriptives/#periomod.descriptives.DescriptivesPlotter.__init__","title":"<code>__init__(df)</code>","text":"<p>Initializes DescriptivesPlotter with pd.DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing data for plotting.</p> required Source code in <code>periomod/descriptives/_descriptives.py</code> <pre><code>def __init__(self, df: pd.DataFrame) -&gt; None:\n    \"\"\"Initializes DescriptivesPlotter with pd.DataFrame.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing data for plotting.\n    \"\"\"\n    self.df = df\n    plt.rcParams[\"svg.fonttype\"] = \"none\"\n    plt.rcParams[\"font.family\"] = \"Arial\"\n    plt.rcParams[\"axes.labelsize\"] = 12\n</code></pre>"},{"location":"reference/descriptives/descriptives/#periomod.descriptives.DescriptivesPlotter.histogram_2d","title":"<code>histogram_2d(col_before, col_after, x_label='Pocket depth before therapy [mm]', y_label='Pocket depth after therapy [mm]', name=None, save=False)</code>","text":"<p>Creates a 2D histogram plot based on two columns.</p> <p>Parameters:</p> Name Type Description Default <code>col_before</code> <code>str</code> <p>Column name for pocket depth before therapy.</p> required <code>col_after</code> <code>str</code> <p>Column name for pocket depth after therapy.</p> required <code>x_label</code> <code>str</code> <p>Label for x-axis. Defaults to \"Pocket depth before therapy [mm]\".</p> <code>'Pocket depth before therapy [mm]'</code> <code>y_label</code> <code>str</code> <p>Label for y-axis. Defaults to \"Pocket depth after therapy [mm]\".</p> <code>'Pocket depth after therapy [mm]'</code> <code>name</code> <code>str</code> <p>Name for saving the plot.</p> <code>None</code> <code>save</code> <code>bool</code> <p>Save the plot as an SVG. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'name' argument is not defined when 'save' is True.\"</p> Source code in <code>periomod/descriptives/_descriptives.py</code> <pre><code>def histogram_2d(\n    self,\n    col_before: str,\n    col_after: str,\n    x_label: str = \"Pocket depth before therapy [mm]\",\n    y_label: str = \"Pocket depth after therapy [mm]\",\n    name: Optional[str] = None,\n    save: bool = False,\n) -&gt; None:\n    \"\"\"Creates a 2D histogram plot based on two columns.\n\n    Args:\n        col_before (str): Column name for pocket depth before therapy.\n        col_after (str): Column name for pocket depth after therapy.\n        x_label (str): Label for x-axis. Defaults to\n            \"Pocket depth before therapy [mm]\".\n        y_label (str): Label for y-axis. Defaults to\n            \"Pocket depth after therapy [mm]\".\n        name (str): Name for saving the plot.\n        save (bool, optional): Save the plot as an SVG. Defaults to False.\n\n    Raises:\n        ValueError: If 'name' argument is not defined when 'save' is True.\"\n    \"\"\"\n    heatmap, _, _ = np.histogram2d(\n        self.df[col_before], self.df[col_after], bins=(12, 12)\n    )\n\n    plt.figure(figsize=(8, 6), dpi=300)\n\n    plt.imshow(heatmap.T, origin=\"lower\", cmap=\"viridis\", interpolation=\"nearest\")\n    cbar = plt.colorbar()\n    cbar.set_label(\"Frequency\", fontsize=12)\n    cbar.outline.set_linewidth(1)\n\n    plt.xlabel(xlabel=x_label, fontsize=12)\n    plt.ylabel(ylabel=y_label, fontsize=12)\n    plt.xticks(np.arange(12), np.arange(1, 13), fontsize=12)\n    plt.yticks(np.arange(12), np.arange(1, 13), fontsize=12)\n\n    ax = plt.gca()\n    for spine in ax.spines.values():\n        spine.set_linewidth(1)\n    ax.tick_params(width=1)\n\n    cbar.ax.tick_params(labelsize=12)\n\n    plt.plot([-0.5, 2.5], [2.5, 2.5], \"r--\", lw=2)\n    plt.plot([2.5, 2.5], [-0.5, 2.5], \"r--\", lw=2)\n\n    if save:\n        if name is None:\n            raise ValueError(\"'name' argument must required when 'save' is True.\")\n        plt.savefig(name + \".svg\", format=\"svg\", dpi=300)\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/descriptives/descriptives/#periomod.descriptives.DescriptivesPlotter.outcome_descriptive","title":"<code>outcome_descriptive(outcome, title, name=None, save=False)</code>","text":"<p>Creates a bar plot for the outcome variable.</p> <p>Parameters:</p> Name Type Description Default <code>outcome</code> <code>str</code> <p>Column name for the outcome variable.</p> required <code>title</code> <code>str</code> <p>Title of the plot.</p> required <code>name</code> <code>str</code> <p>Filename for saving the plot.</p> <code>None</code> <code>save</code> <code>bool</code> <p>Save the plot as an SVG. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'name' argument is not defined when 'save' is True.\"</p> Source code in <code>periomod/descriptives/_descriptives.py</code> <pre><code>def outcome_descriptive(\n    self, outcome: str, title: str, name: Optional[str] = None, save: bool = False\n) -&gt; None:\n    \"\"\"Creates a bar plot for the outcome variable.\n\n    Args:\n        outcome (str): Column name for the outcome variable.\n        title (str): Title of the plot.\n        name (str): Filename for saving the plot.\n        save (bool, optional): Save the plot as an SVG. Defaults to False.\n\n    Raises:\n        ValueError: If 'name' argument is not defined when 'save' is True.\"\n    \"\"\"\n    df_temp = self.df\n    if outcome == \"improvement\" and \"pdgroupbase\" in self.df.columns:\n        df_temp = df_temp.query(\"pdgroupbase in [1, 2]\")\n\n    value_counts = df_temp[outcome].value_counts()\n    x_values = value_counts.index.astype(str)\n    heights = value_counts.to_numpy()\n\n    plt.figure(figsize=(4, 4), dpi=300)\n    bars = plt.bar(\n        x_values, heights, edgecolor=\"black\", color=\"#078294\", linewidth=1\n    )\n    plt.ylabel(\"Number of sites\", fontsize=12)\n    plt.title(title, fontsize=12, pad=10)\n\n    for bar in bars:\n        height = bar.get_height()\n        plt.annotate(\n            \"{}\".format(height),\n            xy=(bar.get_x() + bar.get_width() / 2, height),\n            xytext=(0, 3),\n            textcoords=\"offset points\",\n            ha=\"center\",\n            va=\"bottom\",\n            fontsize=12,\n        )\n\n    ax = plt.gca()\n    for spine in ax.spines.values():\n        spine.set_linewidth(1)\n    ax.tick_params(width=1)\n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"right\"].set_visible(False)\n\n    if save:\n        if name is None:\n            raise ValueError(\"'name' argument is required when 'save' is True.\")\n        plt.savefig(name + \".svg\", format=\"svg\", dpi=300)\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/descriptives/descriptives/#periomod.descriptives.DescriptivesPlotter.plt_matrix","title":"<code>plt_matrix(vertical, horizontal, x_label='Pocket depth before therapy', y_label='Pocket depth after therapy', name=None, normalize='rows', save=False, color='#078294')</code>","text":"<p>Plots a heatmap/confusion matrix.</p> <p>Parameters:</p> Name Type Description Default <code>vertical</code> <code>str</code> <p>Column name for the vertical axis.</p> required <code>horizontal</code> <code>str</code> <p>Column name for the horizontal axis.</p> required <code>x_label</code> <code>str</code> <p>Label for x-axis. Defaults to \"Pocket depth before therapy\".</p> <code>'Pocket depth before therapy'</code> <code>y_label</code> <code>str</code> <p>Label for y-axis. Defaults to \"Pocket depth after therapy\".</p> <code>'Pocket depth after therapy'</code> <code>name</code> <code>str</code> <p>Title of the plot and name for saving the plot.</p> <code>None</code> <code>normalize</code> <code>str</code> <p>Normalization method ('rows' or 'columns'). Defaults to 'rows'.</p> <code>'rows'</code> <code>save</code> <code>bool</code> <p>Save the plot as an SVG. Defaults to False.</p> <code>False</code> <code>color</code> <code>str</code> <p>Custom Color mapping for confusion matrix based on #. Defaults to teal.</p> <code>'#078294'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>normalize</code> is not 'rows' or 'columns'.</p> <code>ValueError</code> <p>If 'name' argument is not defined when 'save' is True.\"</p> Source code in <code>periomod/descriptives/_descriptives.py</code> <pre><code>def plt_matrix(\n    self,\n    vertical: str,\n    horizontal: str,\n    x_label: str = \"Pocket depth before therapy\",\n    y_label: str = \"Pocket depth after therapy\",\n    name: Optional[str] = None,\n    normalize: str = \"rows\",\n    save: bool = False,\n    color: str = \"#078294\",\n) -&gt; None:\n    \"\"\"Plots a heatmap/confusion matrix.\n\n    Args:\n        vertical (str): Column name for the vertical axis.\n        horizontal (str): Column name for the horizontal axis.\n        x_label (str): Label for x-axis. Defaults to \"Pocket depth before therapy\".\n        y_label (str): Label for y-axis. Defaults to \"Pocket depth after therapy\".\n        name (str): Title of the plot and name for saving the plot.\n        normalize (str, optional): Normalization method ('rows' or 'columns').\n            Defaults to 'rows'.\n        save (bool, optional): Save the plot as an SVG. Defaults to False.\n        color: Custom Color mapping for confusion matrix based on #.\n            Defaults to teal.\n\n    Raises:\n        ValueError: If `normalize` is not 'rows' or 'columns'.\n        ValueError: If 'name' argument is not defined when 'save' is True.\"\n    \"\"\"\n    vertical_data = self.df[vertical]\n    horizontal_data = self.df[horizontal]\n    cm = confusion_matrix(vertical_data, horizontal_data)\n    custom_cmap = LinearSegmentedColormap.from_list(\n        \"custom_cmap\", [\"#FFFFFF\", color]\n    )\n\n    if normalize == \"rows\":\n        row_sums = cm.sum(axis=1)\n        normalized_cm = (cm / row_sums[:, np.newaxis]) * 100\n    elif normalize == \"columns\":\n        col_sums = cm.sum(axis=0)\n        normalized_cm = (cm / col_sums) * 100\n    else:\n        raise ValueError(\"Invalid value for 'normalize'. Use 'rows' or 'columns'.\")\n\n    plt.figure(figsize=(6, 4), dpi=300)\n    sns.heatmap(\n        normalized_cm,\n        cmap=custom_cmap,\n        fmt=\"g\",\n        linewidths=0.5,\n        square=True,\n        cbar_kws={\"label\": \"Percent\"},\n    )\n\n    for i in range(len(cm)):\n        for j in range(len(cm)):\n            if normalized_cm[i, j] &gt; 50:\n                plt.text(\n                    j + 0.5,\n                    i + 0.5,\n                    cm[i, j],\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"white\",\n                )\n            else:\n                plt.text(j + 0.5, i + 0.5, cm[i, j], ha=\"center\", va=\"center\")\n\n    title = \"Data Overview\"\n\n    plt.title(title, fontsize=12)\n\n    ax = plt.gca()\n    ax.xaxis.set_ticks_position(\"top\")\n    ax.xaxis.set_label_position(\"top\")\n\n    cbar = ax.collections[0].colorbar\n    cbar.outline.set_edgecolor(\"black\")\n    cbar.outline.set_linewidth(1)\n\n    ax.add_patch(\n        Rectangle(\n            (0, 0), cm.shape[1], cm.shape[0], fill=False, edgecolor=\"black\", lw=2\n        )\n    )\n\n    plt.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n    plt.xlabel(x_label, fontsize=12)\n    plt.ylabel(y_label, fontsize=12)\n\n    if save:\n        if name is None:\n            raise ValueError(\"'name' argument required when 'save' is True.\")\n        plt.savefig(name + \".svg\", format=\"svg\", dpi=300)\n\n    plt.show()\n</code></pre>"},{"location":"reference/descriptives/descriptives/#periomod.descriptives.DescriptivesPlotter.pocket_comparison","title":"<code>pocket_comparison(col1, col2, title_1='Pocket depth before therapy', title_2='Pocket depth after therapy', name=None, save=False)</code>","text":"<p>Creates two bar plots for comparing pocket depth before and after therapy.</p> <p>Parameters:</p> Name Type Description Default <code>col1</code> <code>str</code> <p>Column name for the first plot (before therapy).</p> required <code>col2</code> <code>str</code> <p>Column name for the second plot (after therapy).</p> required <code>title_1</code> <code>str</code> <p>Label for x-axis. Defaults to \"Pocket depth before therapy\".</p> <code>'Pocket depth before therapy'</code> <code>title_2</code> <code>str</code> <p>Label for y-axis. Defaults to \"Pocket depth after therapy\".</p> <code>'Pocket depth after therapy'</code> <code>name</code> <code>str</code> <p>Name for saving the plot.</p> <code>None</code> <code>save</code> <code>bool</code> <p>Save the plot as an SVG. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'name' argument is not defined when 'save' is True.\"</p> Source code in <code>periomod/descriptives/_descriptives.py</code> <pre><code>def pocket_comparison(\n    self,\n    col1: str,\n    col2: str,\n    title_1: str = \"Pocket depth before therapy\",\n    title_2: str = \"Pocket depth after therapy\",\n    name: Optional[str] = None,\n    save: bool = False,\n) -&gt; None:\n    \"\"\"Creates two bar plots for comparing pocket depth before and after therapy.\n\n    Args:\n        col1 (str): Column name for the first plot (before therapy).\n        col2 (str): Column name for the second plot (after therapy).\n        title_1 (str): Label for x-axis. Defaults to \"Pocket depth before therapy\".\n        title_2 (str): Label for y-axis. Defaults to \"Pocket depth after therapy\".\n        name (str): Name for saving the plot.\n        save (bool, optional): Save the plot as an SVG. Defaults to False.\n\n    Raises:\n        ValueError: If 'name' argument is not defined when 'save' is True.\"\n    \"\"\"\n    value_counts_1 = self.df[col1].value_counts()\n    x_values_1 = value_counts_1.index\n    heights_1 = value_counts_1.to_numpy()\n\n    value_counts_2 = self.df[col2].value_counts()\n    x_values_2 = value_counts_2.index\n    heights_2 = value_counts_2.to_numpy()\n\n    fig, (ax1, ax2) = plt.subplots(\n        1, 2, figsize=(8, 5), sharex=True, sharey=True, dpi=300\n    )\n\n    ax1.bar(x_values_1, heights_1, edgecolor=\"black\", color=\"#078294\", linewidth=1)\n    ax1.set_ylabel(\"Number of sites\", fontsize=12)\n    ax1.set_title(title_1, fontsize=12, pad=10)\n    ax1.set_yticks(np.arange(0, 90001, 10000))\n    ax1.set_xticks(np.arange(1, 12.5, 1))\n    ax1.tick_params(axis=\"both\", labelsize=12)\n\n    ax1.axvline(x=3.5, color=\"red\", linestyle=\"--\", linewidth=1, alpha=0.3)\n    ax1.axvline(x=5.5, color=\"red\", linestyle=\"--\", linewidth=1, alpha=0.3)\n\n    ax1.grid(True, axis=\"y\", color=\"black\", linestyle=\"--\", linewidth=1, alpha=0.3)\n    ax1.spines[\"top\"].set_visible(False)\n    ax1.spines[\"right\"].set_visible(False)\n\n    ax1.tick_params(width=1)\n    for spine in ax1.spines.values():\n        spine.set_linewidth(1)\n\n    ax2.bar(x_values_2, heights_2, edgecolor=\"black\", color=\"#078294\", linewidth=1)\n    ax2.set_title(title_2, fontsize=12, pad=10)\n    ax2.tick_params(axis=\"both\", labelsize=12)\n\n    ax2.axvline(x=3.5, color=\"red\", linestyle=\"--\", linewidth=1, alpha=0.3)\n    ax2.axvline(x=5.5, color=\"red\", linestyle=\"--\", linewidth=1, alpha=0.3)\n\n    ax2.grid(True, axis=\"y\", color=\"black\", linestyle=\"--\", linewidth=1, alpha=0.3)\n    ax2.spines[\"top\"].set_visible(False)\n    ax2.spines[\"right\"].set_visible(False)\n    for spine in ax2.spines.values():\n        spine.set_linewidth(1)\n\n    ax2.tick_params(width=1)\n    for spine in ax2.spines.values():\n        spine.set_linewidth(1)\n\n    fig.supxlabel(\"Pocket Depth [mm]\", fontsize=12)\n    plt.tight_layout()\n\n    if save:\n        if name is None:\n            raise ValueError(\"'name' argument must required when 'save' is True.\")\n        plt.savefig(name + \".svg\", format=\"svg\", dpi=300)\n\n    plt.show()\n</code></pre>"},{"location":"reference/descriptives/descriptives/#periomod.descriptives.DescriptivesPlotter.pocket_group_comparison","title":"<code>pocket_group_comparison(col_before, col_after, title_1='Pocket depth before therapy', title_2='Pocket depth after therapy', name=None, save=False)</code>","text":"<p>Creates side-by-side bar plots for pocket depth before and after therapy.</p> <p>Parameters:</p> Name Type Description Default <code>col_before</code> <code>str</code> <p>Column name for the first plot (before therapy).</p> required <code>col_after</code> <code>str</code> <p>Column name for the second plot (after therapy).</p> required <code>title_1</code> <code>str</code> <p>Label for x-axis. Defaults to \"Pocket depth before therapy\".</p> <code>'Pocket depth before therapy'</code> <code>title_2</code> <code>str</code> <p>Label for y-axis. Defaults to \"Pocket depth after therapy\".</p> <code>'Pocket depth after therapy'</code> <code>name</code> <code>str</code> <p>Name for saving the plot.</p> <code>None</code> <code>save</code> <code>bool</code> <p>Save the plot as an SVG. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'name' argument is not defined when 'save' is True.\"</p> Source code in <code>periomod/descriptives/_descriptives.py</code> <pre><code>def pocket_group_comparison(\n    self,\n    col_before: str,\n    col_after: str,\n    title_1: str = \"Pocket depth before therapy\",\n    title_2: str = \"Pocket depth after therapy\",\n    name: Optional[str] = None,\n    save: bool = False,\n) -&gt; None:\n    \"\"\"Creates side-by-side bar plots for pocket depth before and after therapy.\n\n    Args:\n        col_before (str): Column name for the first plot (before therapy).\n        col_after (str): Column name for the second plot (after therapy).\n        title_1 (str): Label for x-axis. Defaults to \"Pocket depth before therapy\".\n        title_2 (str): Label for y-axis. Defaults to \"Pocket depth after therapy\".\n        name (str): Name for saving the plot.\n        save (bool, optional): Save the plot as an SVG. Defaults to False.\n\n    Raises:\n        ValueError: If 'name' argument is not defined when 'save' is True.\"\n    \"\"\"\n    value_counts = self.df[col_before].value_counts()\n    x_values = value_counts.index\n    heights = value_counts.to_numpy()\n    total_values = sum(heights)\n\n    value_counts2 = self.df[col_after].value_counts()\n    x_values2 = value_counts2.index\n    heights2 = value_counts2.to_numpy()\n    total_values2 = sum(heights2)\n\n    fig, (ax1, ax2) = plt.subplots(\n        1, 2, figsize=(8, 4), sharex=True, sharey=True, dpi=300\n    )\n\n    bars1 = ax1.bar(\n        x_values, heights, edgecolor=\"black\", color=\"#078294\", linewidth=1\n    )\n    ax1.set_ylabel(\"Number of sites\", fontsize=12)\n    ax1.set_title(f\"{title_1} (n={total_values})\", fontsize=12, pad=10)\n    ax1.set_yticks(np.arange(0, 100001, 10000))\n    ax1.set_xticks(np.arange(0, 2.1, 1))\n\n    ax1.spines[\"top\"].set_visible(False)\n    ax1.spines[\"right\"].set_visible(False)\n    for spine in ax1.spines.values():\n        spine.set_linewidth(1)\n    ax1.tick_params(axis=\"both\", labelsize=12)\n\n    for bar in bars1:\n        height = bar.get_height()\n        ax1.annotate(\n            \"{}\".format(height),\n            xy=(bar.get_x() + bar.get_width() / 2, height),\n            xytext=(0, 3),\n            textcoords=\"offset points\",\n            ha=\"center\",\n            va=\"bottom\",\n            fontsize=12,\n        )\n\n    bars2 = ax2.bar(\n        x_values2, heights2, edgecolor=\"black\", color=\"#078294\", linewidth=1\n    )\n    ax2.set_title(f\"{title_2} (n={total_values2})\", fontsize=12, pad=10)\n    ax2.spines[\"top\"].set_visible(False)\n    ax2.spines[\"right\"].set_visible(False)\n    for spine in ax2.spines.values():\n        spine.set_linewidth(1)\n\n    ax2.tick_params(axis=\"both\", labelsize=12, width=1)\n\n    for bar in bars2:\n        height = bar.get_height()\n        ax2.annotate(\n            \"{}\".format(height),\n            xy=(bar.get_x() + bar.get_width() / 2, height),\n            xytext=(0, 3),\n            textcoords=\"offset points\",\n            ha=\"center\",\n            va=\"bottom\",\n            fontsize=12,\n        )\n    fig.supxlabel(\"Pocket depth categories\", fontsize=12)\n    plt.tight_layout()\n\n    if save:\n        if name is None:\n            raise ValueError(\"'name' argument must required when 'save' is True.\")\n        plt.savefig(name + \".svg\", format=\"svg\", dpi=300)\n\n    plt.show()\n</code></pre>"},{"location":"reference/evaluation/","title":"periomod.evaluation Overview","text":"<p>The <code>periomod.evaluation</code> module includes evaluative components for assessing model performance and analyzing results.</p>"},{"location":"reference/evaluation/#available-components","title":"Available Components","text":"Component Description BaseModelEvaluator Base class for model evaluation and plotting. EvaluatorMethods Base class for metric calculations and aggregation. ModelEvaluator Comprehensive model evaluation class for metrics analysis."},{"location":"reference/evaluation/basemodelevaluator/","title":"BaseModelEvaluator","text":"<p>               Bases: <code>EvaluatorMethods</code>, <code>ABC</code></p> <p>Abstract base class for evaluating machine learning model performance.</p> <p>This class provides methods for calculating model performance metrics, plotting confusion matrices, and evaluating feature importance, with options for handling one-hot encoded features and aggregating SHAP values.</p> Inherits <ul> <li><code>EvaluatorMethods</code>: Provides prediction and encoding methods.</li> <li><code>ABC</code>: Specifies abstract methods for subclasses to implement.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The test dataset features.</p> required <code>y</code> <code>Series</code> <p>The test dataset labels.</p> required <code>model</code> <code>BaseEstimator</code> <p>A trained sklearn model instance for single-model evaluation.</p> required <code>encoding</code> <code>Optional[str]</code> <p>Encoding type for categorical features, e.g., 'one_hot' or 'target', used for labeling and grouping in plots.</p> required <code>aggregate</code> <code>bool</code> <p>If True, aggregates the importance values of multi-category encoded features for interpretability.</p> required <p>Attributes:</p> Name Type Description <code>X</code> <code>DataFrame</code> <p>Holds the test dataset features for evaluation.</p> <code>y</code> <code>Series</code> <p>Holds the test dataset labels for evaluation.</p> <code>model</code> <code>Optional[BaseEstimator]</code> <p>The primary model instance used for evaluation, if single-model evaluation is performed.</p> <code>encoding</code> <code>Optional[str]</code> <p>Indicates the encoding type used, which impacts plot titles and feature grouping in evaluations.</p> <code>aggregate</code> <code>bool</code> <p>Indicates whether to aggregate importance values of multi-category encoded features, enhancing interpretability in feature importance plots.</p> <p>Methods:</p> Name Description <code>calibration_plot</code> <p>Plots calibration plot for model probabilities.</p> <code>brier_score_groups</code> <p>Calculates Brier score within specified groups.</p> <code>bss_comparison</code> <p>Compares Brier Skill Score of model with baseline.</p> <code>plot_confusion_matrix</code> <p>Generates a styled confusion matrix heatmap for the test data and model predictions.</p> Inherited Methods <ul> <li><code>brier_scores</code>: Calculates Brier score for each instance in the evaluator's     dataset based on the model's predicted probabilities. Returns series of     Brier scores indexed by instance.</li> <li><code>model_predictions</code>: Generates model predictions for evaluator's feature     set, applying threshold-based binarization if specified, and returns     predictions as a series indexed by instance.</li> </ul> Abstract Methods <ul> <li><code>evaluate_feature_importance</code>: Abstract method for evaluating feature     importance across models.</li> <li><code>analyze_brier_within_clusters</code>: Abstract method for analyzing Brier     score distribution within clusters.</li> </ul> Source code in <code>periomod/evaluation/_baseeval.py</code> <pre><code>class BaseModelEvaluator(EvaluatorMethods, ABC):\n    \"\"\"Abstract base class for evaluating machine learning model performance.\n\n    This class provides methods for calculating model performance metrics,\n    plotting confusion matrices, and evaluating feature importance, with options\n    for handling one-hot encoded features and aggregating SHAP values.\n\n    Inherits:\n        - `EvaluatorMethods`: Provides prediction and encoding methods.\n        - `ABC`: Specifies abstract methods for subclasses to implement.\n\n    Args:\n        X (pd.DataFrame): The test dataset features.\n        y (pd.Series): The test dataset labels.\n        model (sklearn.base.BaseEstimator): A trained sklearn model instance\n            for single-model evaluation.\n        encoding (Optional[str]): Encoding type for categorical features, e.g.,\n            'one_hot' or 'target', used for labeling and grouping in plots.\n        aggregate (bool): If True, aggregates the importance values of multi-category\n            encoded features for interpretability.\n\n    Attributes:\n        X (pd.DataFrame): Holds the test dataset features for evaluation.\n        y (pd.Series): Holds the test dataset labels for evaluation.\n        model (Optional[sklearn.base.BaseEstimator]): The primary model instance used\n            for evaluation, if single-model evaluation is performed.\n        encoding (Optional[str]): Indicates the encoding type used, which impacts\n            plot titles and feature grouping in evaluations.\n        aggregate (bool): Indicates whether to aggregate importance values of\n            multi-category encoded features, enhancing interpretability in feature\n            importance plots.\n\n    Methods:\n        calibration_plot: Plots calibration plot for model probabilities.\n        brier_score_groups: Calculates Brier score within specified groups.\n        bss_comparison: Compares Brier Skill Score of model with baseline.\n        plot_confusion_matrix: Generates a styled confusion matrix heatmap\n            for the test data and model predictions.\n\n    Inherited Methods:\n        - `brier_scores`: Calculates Brier score for each instance in the evaluator's\n            dataset based on the model's predicted probabilities. Returns series of\n            Brier scores indexed by instance.\n        - `model_predictions`: Generates model predictions for evaluator's feature\n            set, applying threshold-based binarization if specified, and returns\n            predictions as a series indexed by instance.\n\n    Abstract Methods:\n        - `evaluate_feature_importance`: Abstract method for evaluating feature\n            importance across models.\n        - `analyze_brier_within_clusters`: Abstract method for analyzing Brier\n            score distribution within clusters.\n    \"\"\"\n\n    def __init__(\n        self,\n        X: pd.DataFrame,\n        y: pd.Series,\n        model: Union[\n            RandomForestClassifier,\n            LogisticRegression,\n            MLPClassifier,\n            XGBClassifier,\n        ],\n        encoding: Optional[str],\n        aggregate: bool,\n    ) -&gt; None:\n        \"\"\"Initialize the FeatureImportance class.\"\"\"\n        super().__init__(X=X, y=y, model=model, encoding=encoding, aggregate=aggregate)\n\n    def calibration_plot(\n        self,\n        n_bins: int = 10,\n        tight_layout: bool = False,\n        task: Optional[str] = None,\n        save: bool = False,\n        name: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Generates calibration plots for the model's predicted probabilities.\n\n        Creates a calibration plot for binary classification or one for each\n        class in a multiclass setup.\n\n        Args:\n            n_bins (int): Number of bins for plotting.\n            tight_layout (bool): If True, applies tight layout to the plot.\n                Defaults to False.\n            task (Optional[str]): Task name to apply label mapping for the plot.\n                Defaults to None.\n            save (bool): If True, saves the plot as a .svg file. Defaults to False.\n            name (Optional[str]): Name of the file to save. Defaults to None.\n        \"\"\"\n        classification = \"binary\" if self.y.nunique() == 2 else \"multiclass\"\n        probas = get_probs(self.model, classification=classification, X=self.X)\n\n        if task is not None:\n            label_mapping = _label_mapping(task)\n            plot_title = (\n                f\"Calibration Plot \\n {task_map.get(task, 'Binary')}\"\n                if classification == \"binary\"\n                else f\"Calibration Plot \\n{task_map.get(task, 'Multiclass Task')}\"\n            )\n        else:\n            plot_title = \"Calibration Plot\"\n            label_name = None\n\n        if classification == \"multiclass\":\n            num_classes = probas.shape[1]\n            plt.figure(figsize=(4, 4), dpi=300)\n            for class_idx in range(num_classes):\n                class_probas = probas[:, class_idx]\n                binarized_y = (self.y == class_idx).astype(int)\n                prob_true, prob_pred = calibration_curve(\n                    binarized_y, class_probas, n_bins=n_bins, strategy=\"uniform\"\n                )\n                if task is not None:\n                    label_name = label_mapping.get(class_idx, f\"Class {class_idx}\")\n                plt.plot(prob_pred, prob_true, marker=\"o\", label=label_name)\n            plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect Calibration\")\n            plt.xlabel(\"Mean Predicted Probability\", fontsize=12)\n            plt.ylabel(\"Fraction of Positives\", fontsize=12)\n            ax = plt.gca()\n            ax.set_aspect(\"equal\", adjustable=\"box\")\n            ax.spines[\"top\"].set_visible(False)\n            ax.spines[\"right\"].set_visible(False)\n            plt.title(plot_title, fontsize=12)\n            plt.xticks(fontsize=12)\n            plt.yticks(fontsize=12)\n            plt.ylim(0, 1)\n            plt.xlim(0, 1)\n            plt.legend(frameon=False)\n            if tight_layout:\n                plt.tight_layout()\n            if save:\n                filename = (\n                    f\"{name}_calibration_plot.svg\"\n                    if name is not None\n                    else \"calibration_plot.svg\"\n                )\n                plt.savefig(filename, format=\"svg\")\n            plt.show()\n        else:\n            prob_true, prob_pred = calibration_curve(\n                self.y, probas, n_bins=n_bins, strategy=\"uniform\"\n            )\n            plt.figure(figsize=(4, 4), dpi=300)\n            plt.plot(prob_pred, prob_true, marker=\"o\", label=\"Model\", color=\"#078294\")\n            plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect Calibration\")\n            plt.xlabel(\"Mean Predicted Probability\", fontsize=12)\n            plt.ylabel(\"Fraction of Positives\", fontsize=12)\n            ax = plt.gca()\n            ax.set_aspect(\"equal\", adjustable=\"box\")\n            ax.spines[\"top\"].set_visible(False)\n            ax.spines[\"right\"].set_visible(False)\n            plt.xticks(fontsize=12)\n            plt.yticks(fontsize=12)\n            plt.title(plot_title, fontsize=12)\n            plt.ylim(0, 1)\n            plt.xlim(0, 1)\n            plt.legend(frameon=False)\n\n            if tight_layout:\n                plt.tight_layout()\n            if save:\n                filename = (\n                    f\"{name}_calibration_plot.svg\"\n                    if name is not None\n                    else \"calibration_plot.svg\"\n                )\n                plt.savefig(filename, format=\"svg\")\n            plt.show()\n\n    def brier_score_groups(\n        self,\n        group_by: str = \"y\",\n        task: Optional[str] = None,\n        tight_layout: bool = False,\n        save: bool = False,\n        name: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Calculates and displays Brier score within groups.\n\n        Args:\n            group_by (str): Grouping variable for calculating Brier scores.\n                Defaults to \"y\".\n            tight_layout (bool): If True, applies tight layout to the plot.\n                Defaults to False.\n            task (Optional[str]): Task name to apply label mapping for the plot.\n                Defaults to None.\n            save (bool): If True, saves the plot as a .svg file. Defaults to False.\n            name (Optional[str]): Name of the file to save. Defaults to None.\n        \"\"\"\n        data = pd.DataFrame({group_by: self.y, \"Brier_Score\": self.brier_scores()})\n        if task is not None:\n            data[group_by] = data[group_by].map(_label_mapping(task))\n        data_grouped = data.groupby(group_by)\n        summary = data_grouped[\"Brier_Score\"].agg([\"mean\", \"median\"]).reset_index()\n        print(f\"Average and Median Brier Scores by {group_by}:\\n{summary}\")\n\n        plt.figure(figsize=(4, 4), dpi=300)\n        plt.figure(figsize=(4, 4), dpi=300)\n        sns.violinplot(\n            x=group_by,\n            y=\"Brier_Score\",\n            data=data,\n            linewidth=0.5,\n            color=\"#078294\",\n            inner_kws={\"box_width\": 4, \"whis_width\": 0.5},\n        )\n        sns.despine(top=True, right=True)\n        plt.title(\"Distribution of Brier Scores\", fontsize=12)\n        plt.xlabel(f\"{'y' if group_by == 'y' else group_by}\", fontsize=12)\n        plt.ylabel(\"Brier Score\", fontsize=12)\n        plt.ylim(0, 1)\n        plt.xticks(fontsize=12)\n        plt.yticks(fontsize=12)\n        if tight_layout:\n            plt.tight_layout()\n        if save:\n            filename = (\n                f\"{name}_brier_score_groups.svg\"\n                if name is not None\n                else \"brier_score_groups.svg\"\n            )\n            plt.savefig(filename, format=\"svg\")\n        plt.show()\n\n    def bss_comparison(\n        self,\n        baseline_models: Dict[Tuple[str, str], Any],\n        classification: str,\n        tight_layout: bool = False,\n        num_patients: Optional[int] = None,\n        save: bool = False,\n        name: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Compares the Brier Skill Scores (BSS) of the model with baseline models.\n\n        Args:\n            baseline_models (Dict[Tuple[str, str], Any]): A dictionary containing\n                the baseline models. Keys are tuples of model name and type, and\n                values are the trained model objects.\n            classification (str): Classification type ('binary' or 'multiclass').\n            tight_layout (bool): If True, applies tight layout to the plot.\n                Defaults to False.\n            num_patients (Optional[int]): Number of unique patients in the test set.\n                Defaults to None.\n            save (bool): If True, saves the plot as a .svg file. Defaults to False.\n            name (Optional[str]): Name of the file to save. Defaults to None.\n\n        Raises:\n            ValueError: If the model or any baseline model cannot predict probabilities.\n        \"\"\"\n        trained_probs = (\n            get_probs(self.model, classification=classification, X=self.X)\n            if hasattr(self.model, \"predict_proba\")\n            else None\n        )\n\n        if classification == \"binary\":\n            trained_brier = brier_score_loss(y_true=self.y, y_proba=trained_probs)\n        elif classification == \"multiclass\":\n            trained_brier = brier_loss_multi(y=self.y, probs=trained_probs)\n        else:\n            raise ValueError(f\"Unsupported classification type: {classification}\")\n\n        bss_data = []\n        brier_scores = [{\"Model\": \"Tuned Model\", \"Brier Score\": trained_brier}]\n\n        for model_name, model in baseline_models.items():\n            baseline_probs = (\n                get_probs(model, classification=classification, X=self.X)\n                if hasattr(model, \"predict_proba\")\n                else None\n            )\n            if classification == \"binary\":\n                baseline_brier = brier_score_loss(y_true=self.y, y_proba=baseline_probs)\n            else:\n                baseline_brier = brier_loss_multi(y=self.y, probs=baseline_probs)\n\n            bss = 1 - (trained_brier / baseline_brier)\n            bss_data.append({\"Model\": model_name[0], \"Brier Skill Score\": bss})\n            brier_scores.append({\"Model\": model_name[0], \"Brier Score\": baseline_brier})\n\n        bss_df, brier_df = pd.DataFrame(bss_data), pd.DataFrame(brier_scores)\n        df1_melted = brier_df.melt(\n            id_vars=[\"Model\"], var_name=\"Score\", value_name=\"Value\"\n        )\n        df2_melted = bss_df.melt(\n            id_vars=[\"Model\"], var_name=\"Score\", value_name=\"Value\"\n        )\n\n        combined_df = pd.concat([df1_melted, df2_melted], ignore_index=True)\n\n        plt.subplots(figsize=(8, 4), dpi=300)\n\n        model_order = [\n            \"Tuned Model\",\n            \"Dummy Classifier\",\n            \"Logistic Regression\",\n            \"Random Forest\",\n        ]\n\n        g = sns.barplot(\n            data=combined_df,\n            x=\"Model\",\n            y=\"Value\",\n            hue=\"Score\",\n            order=model_order,\n            linewidth=1,\n            edgecolor=\"black\",\n        )\n\n        plt.axvline(x=0.5, color=\"gray\", linestyle=\"--\", linewidth=1)\n        plt.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=1)\n\n        plt.gca().spines[\"top\"].set_visible(False)\n        plt.gca().spines[\"bottom\"].set_visible(False)\n        plt.gca().spines[\"right\"].set_visible(False)\n\n        for _, e in enumerate(g.patches):\n            if e.get_height() &gt; 0:\n                g.annotate(\n                    f\"{e.get_height():.2f}\",\n                    (e.get_x() + e.get_width() / 2, e.get_height()),\n                    ha=\"center\",\n                    va=\"center\",\n                    fontsize=8,\n                    color=\"black\",\n                    xytext=(0, 5),\n                    textcoords=\"offset points\",\n                )\n            if e.get_height() &lt; 0:\n                g.annotate(\n                    f\"{e.get_height():.2f}\",\n                    (e.get_x() + e.get_width() / 2, e.get_height()),\n                    ha=\"center\",\n                    va=\"center\",\n                    fontsize=8,\n                    color=\"black\",\n                    xytext=(0, -10),\n                    textcoords=\"offset points\",\n                )\n\n        plt.legend(\n            title=\"Metric\",\n            frameon=False,\n            fontsize=10,\n            bbox_to_anchor=(1.05, 0.5),\n            loc=\"upper left\",\n        )\n        if num_patients is not None:\n            plt.title(\n                f\"Baseline Comparison \\n\"\n                f\"Number of Patients {num_patients}; Number of Sites: {len(self.y)}\"\n            )\n        else:\n            plt.title(f\"Baseline Comparison \\n Number of Sites: {len(self.y)}\")\n\n        labels = [label.get_text() for label in g.get_xticklabels()]\n        g.set_xticklabels([label.replace(\" \", \"\\n\") for label in labels])\n\n        if tight_layout:\n            plt.tight_layout()\n        if save:\n            filename = (\n                f\"{name}_bss_comparison.svg\"\n                if name is not None\n                else \"bss_comparison.svg\"\n            )\n            plt.savefig(filename, format=\"svg\")\n        plt.show()\n\n    def plot_confusion_matrix(\n        self,\n        col: Optional[pd.Series] = None,\n        y_label: str = \"True\",\n        normalize: str = \"rows\",\n        tight_layout: bool = False,\n        task: Optional[str] = None,\n        save: bool = False,\n        name: Optional[str] = None,\n    ) -&gt; plt.Figure:\n        \"\"\"Generates a styled confusion matrix for the given model and test data.\n\n        Args:\n            col (Optional[pd.Series]): Column for y label. Defaults to None.\n            y_label (str): Description of y label. Defaults to \"True\".\n            normalize (str, optional): Normalization method ('rows' or 'columns').\n                Defaults to 'rows'.\n            tight_layout (bool): If True, applies tight layout to the plot.\n                Defaults to False.\n            task (Optional[str]): Task name to apply label mapping for the plot.\n                Defaults to None.\n            save (bool): If True, saves the plot as a .svg file. Defaults to False.\n            name (Optional[str]): Name of the file to save. Defaults to None.\n\n        Raises:\n            ValueError: If invalid value for normalize is selected.\n        \"\"\"\n        y_true = pd.Series(col if col is not None else self.y)\n        pred = self.model_predictions()\n\n        if task is not None:\n            label_mapping = _label_mapping(task)\n            y_true = y_true.map(label_mapping)\n            pred = pd.Series(pred).map(label_mapping).to_numpy()\n            labels = list(label_mapping.values())\n        else:\n            labels = None\n\n        cm = confusion_matrix(y_true=y_true, y_pred=pred, labels=labels)\n\n        if normalize == \"rows\":\n            row_sums = cm.sum(axis=1, keepdims=True)\n            normalized_cm = (cm / row_sums) * 100\n        elif normalize == \"columns\":\n            col_sums = cm.sum(axis=0, keepdims=True)\n            normalized_cm = (cm / col_sums) * 100\n        else:\n            raise ValueError(\"Invalid value for 'normalize'. Use 'rows' or 'columns'.\")\n\n        custom_cmap = LinearSegmentedColormap.from_list(\n            \"teal_cmap\", [\"#FFFFFF\", \"#078294\"]\n        )\n\n        plt.figure(figsize=(6, 4), dpi=300)\n        sns.heatmap(\n            normalized_cm,\n            cmap=custom_cmap,\n            fmt=\"g\",\n            linewidths=0.5,\n            square=True,\n            annot=False,\n            cbar_kws={\"label\": \"Percent\"},\n            xticklabels=labels if labels else range(cm.shape[1]),\n            yticklabels=labels if labels else range(cm.shape[0]),\n        )\n\n        for i in range(len(cm)):\n            for j in range(len(cm)):\n                plt.text(\n                    j + 0.5,\n                    i + 0.5,\n                    cm[i, j],\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"white\" if normalized_cm[i, j] &gt; 50 else \"black\",\n                )\n\n        plt.title(\"Confusion Matrix\", fontsize=12)\n        plt.xlabel(\"Predicted\", fontsize=12)\n        plt.ylabel(y_label, fontsize=12)\n\n        ax = plt.gca()\n        ax.xaxis.set_ticks_position(\"top\")\n        ax.xaxis.set_label_position(\"top\")\n        cbar = ax.collections[0].colorbar\n        cbar.outline.set_edgecolor(\"black\")\n        cbar.outline.set_linewidth(1)\n\n        ax.add_patch(\n            Rectangle(\n                (0, 0), cm.shape[1], cm.shape[0], fill=False, edgecolor=\"black\", lw=2\n            )\n        )\n\n        plt.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n        if tight_layout:\n            plt.tight_layout()\n        if save:\n            filename = (\n                f\"{name}_confusion_matrix.svg\"\n                if name is not None\n                else \"confusion_matrix.svg\"\n            )\n            plt.savefig(filename, format=\"svg\")\n        plt.show()\n\n    @abstractmethod\n    def evaluate_feature_importance(self, importance_types: List[str]):\n        \"\"\"Evaluate the feature importance for a list of trained models.\n\n        Args:\n            importance_types (List[str]): Methods of feature importance evaluation:\n                'shap', 'permutation', 'standard'.\n        \"\"\"\n\n    @abstractmethod\n    def analyze_brier_within_clusters(\n        self,\n        clustering_algorithm: Type,\n        n_clusters: int,\n    ):\n        \"\"\"Analyze distribution of Brier scores within clusters formed by input data.\n\n        Args:\n            clustering_algorithm (Type): Clustering algorithm class from sklearn to use\n                for clustering.\n            n_clusters (int): Number of clusters to form.\n        \"\"\"\n</code></pre>"},{"location":"reference/evaluation/basemodelevaluator/#periomod.evaluation.BaseModelEvaluator.__init__","title":"<code>__init__(X, y, model, encoding, aggregate)</code>","text":"<p>Initialize the FeatureImportance class.</p> Source code in <code>periomod/evaluation/_baseeval.py</code> <pre><code>def __init__(\n    self,\n    X: pd.DataFrame,\n    y: pd.Series,\n    model: Union[\n        RandomForestClassifier,\n        LogisticRegression,\n        MLPClassifier,\n        XGBClassifier,\n    ],\n    encoding: Optional[str],\n    aggregate: bool,\n) -&gt; None:\n    \"\"\"Initialize the FeatureImportance class.\"\"\"\n    super().__init__(X=X, y=y, model=model, encoding=encoding, aggregate=aggregate)\n</code></pre>"},{"location":"reference/evaluation/basemodelevaluator/#periomod.evaluation.BaseModelEvaluator.analyze_brier_within_clusters","title":"<code>analyze_brier_within_clusters(clustering_algorithm, n_clusters)</code>  <code>abstractmethod</code>","text":"<p>Analyze distribution of Brier scores within clusters formed by input data.</p> <p>Parameters:</p> Name Type Description Default <code>clustering_algorithm</code> <code>Type</code> <p>Clustering algorithm class from sklearn to use for clustering.</p> required <code>n_clusters</code> <code>int</code> <p>Number of clusters to form.</p> required Source code in <code>periomod/evaluation/_baseeval.py</code> <pre><code>@abstractmethod\ndef analyze_brier_within_clusters(\n    self,\n    clustering_algorithm: Type,\n    n_clusters: int,\n):\n    \"\"\"Analyze distribution of Brier scores within clusters formed by input data.\n\n    Args:\n        clustering_algorithm (Type): Clustering algorithm class from sklearn to use\n            for clustering.\n        n_clusters (int): Number of clusters to form.\n    \"\"\"\n</code></pre>"},{"location":"reference/evaluation/basemodelevaluator/#periomod.evaluation.BaseModelEvaluator.brier_score_groups","title":"<code>brier_score_groups(group_by='y', task=None, tight_layout=False, save=False, name=None)</code>","text":"<p>Calculates and displays Brier score within groups.</p> <p>Parameters:</p> Name Type Description Default <code>group_by</code> <code>str</code> <p>Grouping variable for calculating Brier scores. Defaults to \"y\".</p> <code>'y'</code> <code>tight_layout</code> <code>bool</code> <p>If True, applies tight layout to the plot. Defaults to False.</p> <code>False</code> <code>task</code> <code>Optional[str]</code> <p>Task name to apply label mapping for the plot. Defaults to None.</p> <code>None</code> <code>save</code> <code>bool</code> <p>If True, saves the plot as a .svg file. Defaults to False.</p> <code>False</code> <code>name</code> <code>Optional[str]</code> <p>Name of the file to save. Defaults to None.</p> <code>None</code> Source code in <code>periomod/evaluation/_baseeval.py</code> <pre><code>def brier_score_groups(\n    self,\n    group_by: str = \"y\",\n    task: Optional[str] = None,\n    tight_layout: bool = False,\n    save: bool = False,\n    name: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Calculates and displays Brier score within groups.\n\n    Args:\n        group_by (str): Grouping variable for calculating Brier scores.\n            Defaults to \"y\".\n        tight_layout (bool): If True, applies tight layout to the plot.\n            Defaults to False.\n        task (Optional[str]): Task name to apply label mapping for the plot.\n            Defaults to None.\n        save (bool): If True, saves the plot as a .svg file. Defaults to False.\n        name (Optional[str]): Name of the file to save. Defaults to None.\n    \"\"\"\n    data = pd.DataFrame({group_by: self.y, \"Brier_Score\": self.brier_scores()})\n    if task is not None:\n        data[group_by] = data[group_by].map(_label_mapping(task))\n    data_grouped = data.groupby(group_by)\n    summary = data_grouped[\"Brier_Score\"].agg([\"mean\", \"median\"]).reset_index()\n    print(f\"Average and Median Brier Scores by {group_by}:\\n{summary}\")\n\n    plt.figure(figsize=(4, 4), dpi=300)\n    plt.figure(figsize=(4, 4), dpi=300)\n    sns.violinplot(\n        x=group_by,\n        y=\"Brier_Score\",\n        data=data,\n        linewidth=0.5,\n        color=\"#078294\",\n        inner_kws={\"box_width\": 4, \"whis_width\": 0.5},\n    )\n    sns.despine(top=True, right=True)\n    plt.title(\"Distribution of Brier Scores\", fontsize=12)\n    plt.xlabel(f\"{'y' if group_by == 'y' else group_by}\", fontsize=12)\n    plt.ylabel(\"Brier Score\", fontsize=12)\n    plt.ylim(0, 1)\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    if tight_layout:\n        plt.tight_layout()\n    if save:\n        filename = (\n            f\"{name}_brier_score_groups.svg\"\n            if name is not None\n            else \"brier_score_groups.svg\"\n        )\n        plt.savefig(filename, format=\"svg\")\n    plt.show()\n</code></pre>"},{"location":"reference/evaluation/basemodelevaluator/#periomod.evaluation.BaseModelEvaluator.bss_comparison","title":"<code>bss_comparison(baseline_models, classification, tight_layout=False, num_patients=None, save=False, name=None)</code>","text":"<p>Compares the Brier Skill Scores (BSS) of the model with baseline models.</p> <p>Parameters:</p> Name Type Description Default <code>baseline_models</code> <code>Dict[Tuple[str, str], Any]</code> <p>A dictionary containing the baseline models. Keys are tuples of model name and type, and values are the trained model objects.</p> required <code>classification</code> <code>str</code> <p>Classification type ('binary' or 'multiclass').</p> required <code>tight_layout</code> <code>bool</code> <p>If True, applies tight layout to the plot. Defaults to False.</p> <code>False</code> <code>num_patients</code> <code>Optional[int]</code> <p>Number of unique patients in the test set. Defaults to None.</p> <code>None</code> <code>save</code> <code>bool</code> <p>If True, saves the plot as a .svg file. Defaults to False.</p> <code>False</code> <code>name</code> <code>Optional[str]</code> <p>Name of the file to save. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model or any baseline model cannot predict probabilities.</p> Source code in <code>periomod/evaluation/_baseeval.py</code> <pre><code>def bss_comparison(\n    self,\n    baseline_models: Dict[Tuple[str, str], Any],\n    classification: str,\n    tight_layout: bool = False,\n    num_patients: Optional[int] = None,\n    save: bool = False,\n    name: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Compares the Brier Skill Scores (BSS) of the model with baseline models.\n\n    Args:\n        baseline_models (Dict[Tuple[str, str], Any]): A dictionary containing\n            the baseline models. Keys are tuples of model name and type, and\n            values are the trained model objects.\n        classification (str): Classification type ('binary' or 'multiclass').\n        tight_layout (bool): If True, applies tight layout to the plot.\n            Defaults to False.\n        num_patients (Optional[int]): Number of unique patients in the test set.\n            Defaults to None.\n        save (bool): If True, saves the plot as a .svg file. Defaults to False.\n        name (Optional[str]): Name of the file to save. Defaults to None.\n\n    Raises:\n        ValueError: If the model or any baseline model cannot predict probabilities.\n    \"\"\"\n    trained_probs = (\n        get_probs(self.model, classification=classification, X=self.X)\n        if hasattr(self.model, \"predict_proba\")\n        else None\n    )\n\n    if classification == \"binary\":\n        trained_brier = brier_score_loss(y_true=self.y, y_proba=trained_probs)\n    elif classification == \"multiclass\":\n        trained_brier = brier_loss_multi(y=self.y, probs=trained_probs)\n    else:\n        raise ValueError(f\"Unsupported classification type: {classification}\")\n\n    bss_data = []\n    brier_scores = [{\"Model\": \"Tuned Model\", \"Brier Score\": trained_brier}]\n\n    for model_name, model in baseline_models.items():\n        baseline_probs = (\n            get_probs(model, classification=classification, X=self.X)\n            if hasattr(model, \"predict_proba\")\n            else None\n        )\n        if classification == \"binary\":\n            baseline_brier = brier_score_loss(y_true=self.y, y_proba=baseline_probs)\n        else:\n            baseline_brier = brier_loss_multi(y=self.y, probs=baseline_probs)\n\n        bss = 1 - (trained_brier / baseline_brier)\n        bss_data.append({\"Model\": model_name[0], \"Brier Skill Score\": bss})\n        brier_scores.append({\"Model\": model_name[0], \"Brier Score\": baseline_brier})\n\n    bss_df, brier_df = pd.DataFrame(bss_data), pd.DataFrame(brier_scores)\n    df1_melted = brier_df.melt(\n        id_vars=[\"Model\"], var_name=\"Score\", value_name=\"Value\"\n    )\n    df2_melted = bss_df.melt(\n        id_vars=[\"Model\"], var_name=\"Score\", value_name=\"Value\"\n    )\n\n    combined_df = pd.concat([df1_melted, df2_melted], ignore_index=True)\n\n    plt.subplots(figsize=(8, 4), dpi=300)\n\n    model_order = [\n        \"Tuned Model\",\n        \"Dummy Classifier\",\n        \"Logistic Regression\",\n        \"Random Forest\",\n    ]\n\n    g = sns.barplot(\n        data=combined_df,\n        x=\"Model\",\n        y=\"Value\",\n        hue=\"Score\",\n        order=model_order,\n        linewidth=1,\n        edgecolor=\"black\",\n    )\n\n    plt.axvline(x=0.5, color=\"gray\", linestyle=\"--\", linewidth=1)\n    plt.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=1)\n\n    plt.gca().spines[\"top\"].set_visible(False)\n    plt.gca().spines[\"bottom\"].set_visible(False)\n    plt.gca().spines[\"right\"].set_visible(False)\n\n    for _, e in enumerate(g.patches):\n        if e.get_height() &gt; 0:\n            g.annotate(\n                f\"{e.get_height():.2f}\",\n                (e.get_x() + e.get_width() / 2, e.get_height()),\n                ha=\"center\",\n                va=\"center\",\n                fontsize=8,\n                color=\"black\",\n                xytext=(0, 5),\n                textcoords=\"offset points\",\n            )\n        if e.get_height() &lt; 0:\n            g.annotate(\n                f\"{e.get_height():.2f}\",\n                (e.get_x() + e.get_width() / 2, e.get_height()),\n                ha=\"center\",\n                va=\"center\",\n                fontsize=8,\n                color=\"black\",\n                xytext=(0, -10),\n                textcoords=\"offset points\",\n            )\n\n    plt.legend(\n        title=\"Metric\",\n        frameon=False,\n        fontsize=10,\n        bbox_to_anchor=(1.05, 0.5),\n        loc=\"upper left\",\n    )\n    if num_patients is not None:\n        plt.title(\n            f\"Baseline Comparison \\n\"\n            f\"Number of Patients {num_patients}; Number of Sites: {len(self.y)}\"\n        )\n    else:\n        plt.title(f\"Baseline Comparison \\n Number of Sites: {len(self.y)}\")\n\n    labels = [label.get_text() for label in g.get_xticklabels()]\n    g.set_xticklabels([label.replace(\" \", \"\\n\") for label in labels])\n\n    if tight_layout:\n        plt.tight_layout()\n    if save:\n        filename = (\n            f\"{name}_bss_comparison.svg\"\n            if name is not None\n            else \"bss_comparison.svg\"\n        )\n        plt.savefig(filename, format=\"svg\")\n    plt.show()\n</code></pre>"},{"location":"reference/evaluation/basemodelevaluator/#periomod.evaluation.BaseModelEvaluator.calibration_plot","title":"<code>calibration_plot(n_bins=10, tight_layout=False, task=None, save=False, name=None)</code>","text":"<p>Generates calibration plots for the model's predicted probabilities.</p> <p>Creates a calibration plot for binary classification or one for each class in a multiclass setup.</p> <p>Parameters:</p> Name Type Description Default <code>n_bins</code> <code>int</code> <p>Number of bins for plotting.</p> <code>10</code> <code>tight_layout</code> <code>bool</code> <p>If True, applies tight layout to the plot. Defaults to False.</p> <code>False</code> <code>task</code> <code>Optional[str]</code> <p>Task name to apply label mapping for the plot. Defaults to None.</p> <code>None</code> <code>save</code> <code>bool</code> <p>If True, saves the plot as a .svg file. Defaults to False.</p> <code>False</code> <code>name</code> <code>Optional[str]</code> <p>Name of the file to save. Defaults to None.</p> <code>None</code> Source code in <code>periomod/evaluation/_baseeval.py</code> <pre><code>def calibration_plot(\n    self,\n    n_bins: int = 10,\n    tight_layout: bool = False,\n    task: Optional[str] = None,\n    save: bool = False,\n    name: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Generates calibration plots for the model's predicted probabilities.\n\n    Creates a calibration plot for binary classification or one for each\n    class in a multiclass setup.\n\n    Args:\n        n_bins (int): Number of bins for plotting.\n        tight_layout (bool): If True, applies tight layout to the plot.\n            Defaults to False.\n        task (Optional[str]): Task name to apply label mapping for the plot.\n            Defaults to None.\n        save (bool): If True, saves the plot as a .svg file. Defaults to False.\n        name (Optional[str]): Name of the file to save. Defaults to None.\n    \"\"\"\n    classification = \"binary\" if self.y.nunique() == 2 else \"multiclass\"\n    probas = get_probs(self.model, classification=classification, X=self.X)\n\n    if task is not None:\n        label_mapping = _label_mapping(task)\n        plot_title = (\n            f\"Calibration Plot \\n {task_map.get(task, 'Binary')}\"\n            if classification == \"binary\"\n            else f\"Calibration Plot \\n{task_map.get(task, 'Multiclass Task')}\"\n        )\n    else:\n        plot_title = \"Calibration Plot\"\n        label_name = None\n\n    if classification == \"multiclass\":\n        num_classes = probas.shape[1]\n        plt.figure(figsize=(4, 4), dpi=300)\n        for class_idx in range(num_classes):\n            class_probas = probas[:, class_idx]\n            binarized_y = (self.y == class_idx).astype(int)\n            prob_true, prob_pred = calibration_curve(\n                binarized_y, class_probas, n_bins=n_bins, strategy=\"uniform\"\n            )\n            if task is not None:\n                label_name = label_mapping.get(class_idx, f\"Class {class_idx}\")\n            plt.plot(prob_pred, prob_true, marker=\"o\", label=label_name)\n        plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect Calibration\")\n        plt.xlabel(\"Mean Predicted Probability\", fontsize=12)\n        plt.ylabel(\"Fraction of Positives\", fontsize=12)\n        ax = plt.gca()\n        ax.set_aspect(\"equal\", adjustable=\"box\")\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n        plt.title(plot_title, fontsize=12)\n        plt.xticks(fontsize=12)\n        plt.yticks(fontsize=12)\n        plt.ylim(0, 1)\n        plt.xlim(0, 1)\n        plt.legend(frameon=False)\n        if tight_layout:\n            plt.tight_layout()\n        if save:\n            filename = (\n                f\"{name}_calibration_plot.svg\"\n                if name is not None\n                else \"calibration_plot.svg\"\n            )\n            plt.savefig(filename, format=\"svg\")\n        plt.show()\n    else:\n        prob_true, prob_pred = calibration_curve(\n            self.y, probas, n_bins=n_bins, strategy=\"uniform\"\n        )\n        plt.figure(figsize=(4, 4), dpi=300)\n        plt.plot(prob_pred, prob_true, marker=\"o\", label=\"Model\", color=\"#078294\")\n        plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect Calibration\")\n        plt.xlabel(\"Mean Predicted Probability\", fontsize=12)\n        plt.ylabel(\"Fraction of Positives\", fontsize=12)\n        ax = plt.gca()\n        ax.set_aspect(\"equal\", adjustable=\"box\")\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n        plt.xticks(fontsize=12)\n        plt.yticks(fontsize=12)\n        plt.title(plot_title, fontsize=12)\n        plt.ylim(0, 1)\n        plt.xlim(0, 1)\n        plt.legend(frameon=False)\n\n        if tight_layout:\n            plt.tight_layout()\n        if save:\n            filename = (\n                f\"{name}_calibration_plot.svg\"\n                if name is not None\n                else \"calibration_plot.svg\"\n            )\n            plt.savefig(filename, format=\"svg\")\n        plt.show()\n</code></pre>"},{"location":"reference/evaluation/basemodelevaluator/#periomod.evaluation.BaseModelEvaluator.evaluate_feature_importance","title":"<code>evaluate_feature_importance(importance_types)</code>  <code>abstractmethod</code>","text":"<p>Evaluate the feature importance for a list of trained models.</p> <p>Parameters:</p> Name Type Description Default <code>importance_types</code> <code>List[str]</code> <p>Methods of feature importance evaluation: 'shap', 'permutation', 'standard'.</p> required Source code in <code>periomod/evaluation/_baseeval.py</code> <pre><code>@abstractmethod\ndef evaluate_feature_importance(self, importance_types: List[str]):\n    \"\"\"Evaluate the feature importance for a list of trained models.\n\n    Args:\n        importance_types (List[str]): Methods of feature importance evaluation:\n            'shap', 'permutation', 'standard'.\n    \"\"\"\n</code></pre>"},{"location":"reference/evaluation/basemodelevaluator/#periomod.evaluation.BaseModelEvaluator.plot_confusion_matrix","title":"<code>plot_confusion_matrix(col=None, y_label='True', normalize='rows', tight_layout=False, task=None, save=False, name=None)</code>","text":"<p>Generates a styled confusion matrix for the given model and test data.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Optional[Series]</code> <p>Column for y label. Defaults to None.</p> <code>None</code> <code>y_label</code> <code>str</code> <p>Description of y label. Defaults to \"True\".</p> <code>'True'</code> <code>normalize</code> <code>str</code> <p>Normalization method ('rows' or 'columns'). Defaults to 'rows'.</p> <code>'rows'</code> <code>tight_layout</code> <code>bool</code> <p>If True, applies tight layout to the plot. Defaults to False.</p> <code>False</code> <code>task</code> <code>Optional[str]</code> <p>Task name to apply label mapping for the plot. Defaults to None.</p> <code>None</code> <code>save</code> <code>bool</code> <p>If True, saves the plot as a .svg file. Defaults to False.</p> <code>False</code> <code>name</code> <code>Optional[str]</code> <p>Name of the file to save. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If invalid value for normalize is selected.</p> Source code in <code>periomod/evaluation/_baseeval.py</code> <pre><code>def plot_confusion_matrix(\n    self,\n    col: Optional[pd.Series] = None,\n    y_label: str = \"True\",\n    normalize: str = \"rows\",\n    tight_layout: bool = False,\n    task: Optional[str] = None,\n    save: bool = False,\n    name: Optional[str] = None,\n) -&gt; plt.Figure:\n    \"\"\"Generates a styled confusion matrix for the given model and test data.\n\n    Args:\n        col (Optional[pd.Series]): Column for y label. Defaults to None.\n        y_label (str): Description of y label. Defaults to \"True\".\n        normalize (str, optional): Normalization method ('rows' or 'columns').\n            Defaults to 'rows'.\n        tight_layout (bool): If True, applies tight layout to the plot.\n            Defaults to False.\n        task (Optional[str]): Task name to apply label mapping for the plot.\n            Defaults to None.\n        save (bool): If True, saves the plot as a .svg file. Defaults to False.\n        name (Optional[str]): Name of the file to save. Defaults to None.\n\n    Raises:\n        ValueError: If invalid value for normalize is selected.\n    \"\"\"\n    y_true = pd.Series(col if col is not None else self.y)\n    pred = self.model_predictions()\n\n    if task is not None:\n        label_mapping = _label_mapping(task)\n        y_true = y_true.map(label_mapping)\n        pred = pd.Series(pred).map(label_mapping).to_numpy()\n        labels = list(label_mapping.values())\n    else:\n        labels = None\n\n    cm = confusion_matrix(y_true=y_true, y_pred=pred, labels=labels)\n\n    if normalize == \"rows\":\n        row_sums = cm.sum(axis=1, keepdims=True)\n        normalized_cm = (cm / row_sums) * 100\n    elif normalize == \"columns\":\n        col_sums = cm.sum(axis=0, keepdims=True)\n        normalized_cm = (cm / col_sums) * 100\n    else:\n        raise ValueError(\"Invalid value for 'normalize'. Use 'rows' or 'columns'.\")\n\n    custom_cmap = LinearSegmentedColormap.from_list(\n        \"teal_cmap\", [\"#FFFFFF\", \"#078294\"]\n    )\n\n    plt.figure(figsize=(6, 4), dpi=300)\n    sns.heatmap(\n        normalized_cm,\n        cmap=custom_cmap,\n        fmt=\"g\",\n        linewidths=0.5,\n        square=True,\n        annot=False,\n        cbar_kws={\"label\": \"Percent\"},\n        xticklabels=labels if labels else range(cm.shape[1]),\n        yticklabels=labels if labels else range(cm.shape[0]),\n    )\n\n    for i in range(len(cm)):\n        for j in range(len(cm)):\n            plt.text(\n                j + 0.5,\n                i + 0.5,\n                cm[i, j],\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if normalized_cm[i, j] &gt; 50 else \"black\",\n            )\n\n    plt.title(\"Confusion Matrix\", fontsize=12)\n    plt.xlabel(\"Predicted\", fontsize=12)\n    plt.ylabel(y_label, fontsize=12)\n\n    ax = plt.gca()\n    ax.xaxis.set_ticks_position(\"top\")\n    ax.xaxis.set_label_position(\"top\")\n    cbar = ax.collections[0].colorbar\n    cbar.outline.set_edgecolor(\"black\")\n    cbar.outline.set_linewidth(1)\n\n    ax.add_patch(\n        Rectangle(\n            (0, 0), cm.shape[1], cm.shape[0], fill=False, edgecolor=\"black\", lw=2\n        )\n    )\n\n    plt.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n    if tight_layout:\n        plt.tight_layout()\n    if save:\n        filename = (\n            f\"{name}_confusion_matrix.svg\"\n            if name is not None\n            else \"confusion_matrix.svg\"\n        )\n        plt.savefig(filename, format=\"svg\")\n    plt.show()\n</code></pre>"},{"location":"reference/evaluation/evaluatormethods/","title":"EvaluatorMethods","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Base class that contains methods for ModelEvalutor.</p> Inherits <ul> <li><code>BaseConfig</code>: Provides base configuration settings.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The test dataset features.</p> required <code>y</code> <code>Series</code> <p>The test dataset labels.</p> required <code>model</code> <code>BaseEstimator</code> <p>A trained sklearn model instance for single-model evaluation.</p> required <code>encoding</code> <code>Optional[str]</code> <p>Encoding type for categorical features, e.g., 'one_hot' or 'target', used for labeling and grouping in plots.</p> required <code>aggregate</code> <code>bool</code> <p>If True, aggregates the importance values of multi-category encoded features for interpretability.</p> required <p>Attributes:</p> Name Type Description <code>X</code> <code>DataFrame</code> <p>Holds the test dataset features for evaluation.</p> <code>y</code> <code>Series</code> <p>Holds the test dataset labels for evaluation.</p> <code>model</code> <code>Optional[BaseEstimator]</code> <p>The primary model instance used for evaluation, if single-model evaluation is performed.</p> <code>encoding</code> <code>Optional[str]</code> <p>Indicates the encoding type used, which impacts plot titles and feature grouping in evaluations.</p> <code>aggregate</code> <code>bool</code> <p>Indicates whether to aggregate importance values of multi-category encoded features, enhancing interpretability in feature importance plots.</p> <p>Methods:</p> Name Description <code>brier_scores</code> <p>Calculates Brier score for each instance in the evaluator's dataset based on the model's predicted probabilities. Returns series of Brier scores indexed by instance.</p> <code>model_predictions</code> <p>Generates model predictions for evaluator's feature set, applying threshold-based binarization if specified, and returns predictions as a series indexed by instance.</p> Source code in <code>periomod/evaluation/_baseeval.py</code> <pre><code>class EvaluatorMethods(BaseConfig):\n    \"\"\"Base class that contains methods for ModelEvalutor.\n\n    Inherits:\n        - `BaseConfig`: Provides base configuration settings.\n\n    Args:\n        X (pd.DataFrame): The test dataset features.\n        y (pd.Series): The test dataset labels.\n        model (sklearn.base.BaseEstimator): A trained sklearn model instance\n            for single-model evaluation.\n        encoding (Optional[str]): Encoding type for categorical features, e.g.,\n            'one_hot' or 'target', used for labeling and grouping in plots.\n        aggregate (bool): If True, aggregates the importance values of multi-category\n            encoded features for interpretability.\n\n    Attributes:\n        X (pd.DataFrame): Holds the test dataset features for evaluation.\n        y (pd.Series): Holds the test dataset labels for evaluation.\n        model (Optional[sklearn.base.BaseEstimator]): The primary model instance used\n            for evaluation, if single-model evaluation is performed.\n        encoding (Optional[str]): Indicates the encoding type used, which impacts\n            plot titles and feature grouping in evaluations.\n        aggregate (bool): Indicates whether to aggregate importance values of\n            multi-category encoded features, enhancing interpretability in feature\n            importance plots.\n\n    Methods:\n        brier_scores: Calculates Brier score for each instance in the evaluator's\n            dataset based on the model's predicted probabilities. Returns series of\n            Brier scores indexed by instance.\n        model_predictions: Generates model predictions for evaluator's feature\n            set, applying threshold-based binarization if specified, and returns\n            predictions as a series indexed by instance.\n    \"\"\"\n\n    def __init__(\n        self,\n        X: pd.DataFrame,\n        y: pd.Series,\n        model: Union[\n            RandomForestClassifier,\n            LogisticRegression,\n            MLPClassifier,\n            XGBClassifier,\n        ],\n        encoding: Optional[str],\n        aggregate: bool,\n    ) -&gt; None:\n        \"\"\"Initialize the FeatureImportance class.\"\"\"\n        super().__init__()\n        self.X = X\n        self.y = y\n        self.model = model\n        self.encoding = encoding\n        self.aggregate = aggregate\n        self._set_plot_style()\n\n    def _set_plot_style(self) -&gt; None:\n        plt.rcParams[\"svg.fonttype\"] = \"none\"\n        plt.rcParams[\"font.family\"] = \"Arial\"\n\n    def brier_scores(self) -&gt; pd.Series:\n        \"\"\"Calculates Brier scores for each instance in the evaluator's dataset.\n\n        Returns:\n            Series: Brier scores for each instance.\n        \"\"\"\n        probas = self.model.predict_proba(self.X)\n\n        if probas.shape[1] == 1:\n            brier_scores = [\n                brier_score_loss([true_label], [pred_proba[0]])\n                for true_label, pred_proba in zip(self.y, probas, strict=False)\n            ]\n        else:\n            brier_scores = [\n                brier_score_loss(\n                    [1 if true_label == idx else 0 for idx in range(len(proba))], proba\n                )\n                for true_label, proba in zip(self.y, probas, strict=False)\n            ]\n\n        return pd.Series(brier_scores, index=self.y.index)\n\n    def model_predictions(self) -&gt; pd.Series:\n        \"\"\"Generates model predictions for the evaluator's feature set.\n\n        Returns:\n            pred: Predicted labels as a series.\n        \"\"\"\n        if (\n            hasattr(self.model, \"best_threshold\")\n            and self.model.best_threshold is not None\n        ):\n            final_probs = get_probs(model=self.model, classification=\"binary\", X=self.X)\n            if final_probs is not None:\n                pred = pd.Series(\n                    (final_probs &gt;= self.model.best_threshold).astype(int),\n                    index=self.X.index,\n                )\n            else:\n                pred = pd.Series(self.model.predict(self.X), index=self.X.index)\n        else:\n            pred = pd.Series(self.model.predict(self.X), index=self.X.index)\n\n        return pred\n\n    def _feature_mapping(self, features: List[str]) -&gt; List[str]:\n        \"\"\"Maps a list of feature names to their original labels.\n\n        Args:\n            features (List[str]): List of feature names to be mapped.\n\n        Returns:\n            List[str]: List of mapped feature names, with original labels applied\n                where available.\n        \"\"\"\n        return [self.feature_mapping.get(feature, feature) for feature in features]\n\n    @staticmethod\n    def _aggregate_one_hot_importances(\n        fi_df: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Aggregate importance scores of one-hot encoded variables.\n\n        Args:\n            fi_df (pd.DataFrame): DataFrame with features and their\n                importance scores.\n\n        Returns:\n            pd.DataFrame: Updated DataFrame with aggregated importance scores.\n        \"\"\"\n        base_names = fi_df[\"Feature\"].apply(_get_base_name)\n        aggregated_importances = (\n            fi_df.groupby(base_names)[\"Importance\"].sum().reset_index()\n        )\n        aggregated_importances.columns = [\"Feature\", \"Importance\"]\n        original_features = fi_df[\"Feature\"][\n            ~fi_df[\"Feature\"].apply(_is_one_hot_encoded)\n        ].unique()\n\n        aggregated_or_original = (\n            pd.concat([\n                aggregated_importances,\n                fi_df[fi_df[\"Feature\"].isin(original_features)],\n            ])\n            .drop_duplicates()\n            .sort_values(by=\"Importance\", ascending=False)\n        )\n\n        return aggregated_or_original.reset_index(drop=True)\n\n    @staticmethod\n    def _aggregate_shap_one_hot(\n        shap_values: np.ndarray, feature_names: List[str]\n    ) -&gt; Tuple[np.ndarray, List[str]]:\n        \"\"\"Aggregate SHAP values of one-hot encoded variables.\n\n        Args:\n            shap_values (np.ndarray): SHAP values.\n            feature_names (List[str]): List of features corresponding to SHAP values.\n\n        Returns:\n            Tuple[np.ndarray, List[str]]: Aggregated SHAP values and updated list of\n            feature names.\n        \"\"\"\n        if shap_values.ndim == 3:\n            shap_values = shap_values.mean(axis=2)\n\n        shap_df = pd.DataFrame(shap_values, columns=feature_names)\n        base_names = [_get_base_name(feature=feature) for feature in shap_df.columns]\n        feature_mapping = dict(zip(shap_df.columns, base_names, strict=False))\n        aggregated_shap_df = shap_df.groupby(feature_mapping, axis=1).sum()\n        return aggregated_shap_df.to_numpy(), list(aggregated_shap_df.columns)\n\n    @staticmethod\n    def _aggregate_one_hot_features_for_clustering(X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Aggregate one-hot encoded features for clustering.\n\n        Args:\n            X (pd.DataFrame): Input DataFrame with one-hot encoded features.\n\n        Returns:\n            pd.DataFrame: DataFrame with aggregated one-hot encoded features.\n        \"\"\"\n        X_copy = X.copy()\n        one_hot_encoded_cols = [\n            col for col in X_copy.columns if _is_one_hot_encoded(feature=col)\n        ]\n        base_names = {col: _get_base_name(feature=col) for col in one_hot_encoded_cols}\n        aggregated_data = X_copy.groupby(base_names, axis=1).sum()\n        non_one_hot_cols = [\n            col for col in X_copy.columns if col not in one_hot_encoded_cols\n        ]\n        return pd.concat([X_copy[non_one_hot_cols], aggregated_data], axis=1)\n</code></pre>"},{"location":"reference/evaluation/evaluatormethods/#periomod.evaluation._baseeval.EvaluatorMethods.__init__","title":"<code>__init__(X, y, model, encoding, aggregate)</code>","text":"<p>Initialize the FeatureImportance class.</p> Source code in <code>periomod/evaluation/_baseeval.py</code> <pre><code>def __init__(\n    self,\n    X: pd.DataFrame,\n    y: pd.Series,\n    model: Union[\n        RandomForestClassifier,\n        LogisticRegression,\n        MLPClassifier,\n        XGBClassifier,\n    ],\n    encoding: Optional[str],\n    aggregate: bool,\n) -&gt; None:\n    \"\"\"Initialize the FeatureImportance class.\"\"\"\n    super().__init__()\n    self.X = X\n    self.y = y\n    self.model = model\n    self.encoding = encoding\n    self.aggregate = aggregate\n    self._set_plot_style()\n</code></pre>"},{"location":"reference/evaluation/evaluatormethods/#periomod.evaluation._baseeval.EvaluatorMethods.brier_scores","title":"<code>brier_scores()</code>","text":"<p>Calculates Brier scores for each instance in the evaluator's dataset.</p> <p>Returns:</p> Name Type Description <code>Series</code> <code>Series</code> <p>Brier scores for each instance.</p> Source code in <code>periomod/evaluation/_baseeval.py</code> <pre><code>def brier_scores(self) -&gt; pd.Series:\n    \"\"\"Calculates Brier scores for each instance in the evaluator's dataset.\n\n    Returns:\n        Series: Brier scores for each instance.\n    \"\"\"\n    probas = self.model.predict_proba(self.X)\n\n    if probas.shape[1] == 1:\n        brier_scores = [\n            brier_score_loss([true_label], [pred_proba[0]])\n            for true_label, pred_proba in zip(self.y, probas, strict=False)\n        ]\n    else:\n        brier_scores = [\n            brier_score_loss(\n                [1 if true_label == idx else 0 for idx in range(len(proba))], proba\n            )\n            for true_label, proba in zip(self.y, probas, strict=False)\n        ]\n\n    return pd.Series(brier_scores, index=self.y.index)\n</code></pre>"},{"location":"reference/evaluation/evaluatormethods/#periomod.evaluation._baseeval.EvaluatorMethods.model_predictions","title":"<code>model_predictions()</code>","text":"<p>Generates model predictions for the evaluator's feature set.</p> <p>Returns:</p> Name Type Description <code>pred</code> <code>Series</code> <p>Predicted labels as a series.</p> Source code in <code>periomod/evaluation/_baseeval.py</code> <pre><code>def model_predictions(self) -&gt; pd.Series:\n    \"\"\"Generates model predictions for the evaluator's feature set.\n\n    Returns:\n        pred: Predicted labels as a series.\n    \"\"\"\n    if (\n        hasattr(self.model, \"best_threshold\")\n        and self.model.best_threshold is not None\n    ):\n        final_probs = get_probs(model=self.model, classification=\"binary\", X=self.X)\n        if final_probs is not None:\n            pred = pd.Series(\n                (final_probs &gt;= self.model.best_threshold).astype(int),\n                index=self.X.index,\n            )\n        else:\n            pred = pd.Series(self.model.predict(self.X), index=self.X.index)\n    else:\n        pred = pd.Series(self.model.predict(self.X), index=self.X.index)\n\n    return pred\n</code></pre>"},{"location":"reference/evaluation/modelevaluator/","title":"ModelEvaluator","text":"<p>               Bases: <code>BaseModelEvaluator</code></p> <p>Concrete implementation for evaluating machine learning model performance.</p> <p>This class extends <code>BaseModelEvaluator</code> to provide methods for calculating feature importance using SHAP, permutation importance, and standard model importance. It also supports clustering analyses of Brier scores.</p> Inherits <ul> <li><code>BaseModelEvaluator</code>: Provides methods for model evaluation, calculating   Brier scores, plotting confusion matrices, and aggregating feature   importance for one-hot encoded features.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The dataset features used for testing the model's performance.</p> required <code>y</code> <code>Series</code> <p>The true labels for the test dataset.</p> required <code>model</code> <code>BaseEstimator</code> <p>A single trained model instance (e.g., <code>RandomForestClassifier</code> or <code>LogisticRegression</code>) for evaluation.</p> required <code>encoding</code> <code>Optional[str]</code> <p>Encoding type for categorical variables used in plot titles and feature grouping (e.g., 'one_hot' or 'target').</p> <code>None</code> <code>aggregate</code> <code>bool</code> <p>If True, aggregates the importance values of multi-category encoded features for interpretability.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>X</code> <code>DataFrame</code> <p>Stores the test dataset features for model evaluation.</p> <code>y</code> <code>Series</code> <p>Stores the test dataset labels for model evaluation.</p> <code>model</code> <code>BaseEstimator</code> <p>Primary model used for evaluation.</p> <code>encoding</code> <code>Optional[str]</code> <p>Indicates the encoding type used, which impacts plot titles and feature grouping in evaluations.</p> <code>aggregate</code> <code>bool</code> <p>Indicates whether to aggregate importance values of multi-category encoded features, enhancing interpretability in feature importance plots.</p> <p>Methods:</p> Name Description <code>evaluate_feature_importance</code> <p>Calculates feature importance scores using specified methods (<code>shap</code>, <code>permutation</code>, or <code>standard</code>).</p> <code>analyze_brier_within_clusters</code> <p>Computes Brier scores within clusters formed by a specified clustering algorithm and provides visualizations.</p> Inherited Methods <ul> <li><code>brier_scores</code>: Calculates Brier score for each instance in the evaluator's     dataset based on the model's predicted probabilities. Returns series of     Brier scores indexed by instance.</li> <li><code>calibration_plot</code>: Plots calibration plot for model probabilities.</li> <li><code>model_predictions</code>: Generates model predictions for evaluator's feature     set, applying threshold-based binarization if specified, and returns     predictions as a series indexed by instance.</li> <li><code>brier_score_groups</code>: Calculates Brier score within specified groups</li> <li><code>bss_comparison</code>: Compares Brier Skill Score of model with baseline.   based on a grouping variable (e.g., target class).</li> <li><code>plot_confusion_matrix</code>: Generates a styled confusion matrix heatmap   for model predictions, with optional normalization.</li> </ul> Example <pre><code># Use X_test, y_test obtained from Resampler\nevaluator = ModelEvaluator(\n    X=X_test, y=y_test, model=trained_rf_model, encoding=\"one_hot\"\n    )\nevaluator.evaluate_feature_importance(fi_types=[\"shap\", \"permutation\"])\nbrier_plot, heatmap_plot, clustered_data = (\n    evaluator.analyze_brier_within_clusters()\n)\n</code></pre> Source code in <code>periomod/evaluation/_eval.py</code> <pre><code>class ModelEvaluator(BaseModelEvaluator):\n    \"\"\"Concrete implementation for evaluating machine learning model performance.\n\n    This class extends `BaseModelEvaluator` to provide methods for calculating\n    feature importance using SHAP, permutation importance, and standard model\n    importance. It also supports clustering analyses of Brier scores.\n\n    Inherits:\n        - `BaseModelEvaluator`: Provides methods for model evaluation, calculating\n          Brier scores, plotting confusion matrices, and aggregating feature\n          importance for one-hot encoded features.\n\n    Args:\n        X (pd.DataFrame): The dataset features used for testing the model's\n            performance.\n        y (pd.Series): The true labels for the test dataset.\n        model (sklearn.base.BaseEstimator): A single trained model instance\n            (e.g., `RandomForestClassifier` or `LogisticRegression`) for evaluation.\n        encoding (Optional[str]): Encoding type for categorical variables used in plot\n            titles and feature grouping (e.g., 'one_hot' or 'target').\n        aggregate (bool): If True, aggregates the importance values of multi-category\n            encoded features for interpretability.\n\n    Attributes:\n        X (pd.DataFrame): Stores the test dataset features for model evaluation.\n        y (pd.Series): Stores the test dataset labels for model evaluation.\n        model (sklearn.base.BaseEstimator): Primary model used for evaluation.\n        encoding (Optional[str]): Indicates the encoding type used, which impacts\n            plot titles and feature grouping in evaluations.\n        aggregate (bool): Indicates whether to aggregate importance values of\n            multi-category encoded features, enhancing interpretability in feature\n            importance plots.\n\n    Methods:\n        evaluate_feature_importance: Calculates feature importance scores using\n            specified methods (`shap`, `permutation`, or `standard`).\n        analyze_brier_within_clusters: Computes Brier scores within clusters formed by a\n            specified clustering algorithm and provides visualizations.\n\n    Inherited Methods:\n        - `brier_scores`: Calculates Brier score for each instance in the evaluator's\n            dataset based on the model's predicted probabilities. Returns series of\n            Brier scores indexed by instance.\n        - `calibration_plot`: Plots calibration plot for model probabilities.\n        - `model_predictions`: Generates model predictions for evaluator's feature\n            set, applying threshold-based binarization if specified, and returns\n            predictions as a series indexed by instance.\n        - `brier_score_groups`: Calculates Brier score within specified groups\n        - `bss_comparison`: Compares Brier Skill Score of model with baseline.\n          based on a grouping variable (e.g., target class).\n        - `plot_confusion_matrix`: Generates a styled confusion matrix heatmap\n          for model predictions, with optional normalization.\n\n    Example:\n        ```\n        # Use X_test, y_test obtained from Resampler\n        evaluator = ModelEvaluator(\n            X=X_test, y=y_test, model=trained_rf_model, encoding=\"one_hot\"\n            )\n        evaluator.evaluate_feature_importance(fi_types=[\"shap\", \"permutation\"])\n        brier_plot, heatmap_plot, clustered_data = (\n            evaluator.analyze_brier_within_clusters()\n        )\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        X: pd.DataFrame,\n        y: pd.Series,\n        model: Union[\n            RandomForestClassifier,\n            LogisticRegression,\n            MLPClassifier,\n            XGBClassifier,\n        ],\n        encoding: Optional[str] = None,\n        aggregate: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the FeatureImportance class.\"\"\"\n        super().__init__(X=X, y=y, model=model, encoding=encoding, aggregate=aggregate)\n\n    def evaluate_feature_importance(\n        self, fi_types: List[str], save: bool = False, name: Optional[str] = None\n    ) -&gt; None:\n        \"\"\"Evaluate the feature importance for a list of trained models.\n\n        Args:\n            fi_types (List[str]): Methods of feature importance evaluation:\n                'shap', 'permutation', 'standard'.\n            save (bool): If True, saves the plot as an SVG file. Defaults to False.\n            name (Optional[str]): Name of the file to save the plot. Required when\n\n        Raises:\n            ValueError: If invalid fi_type: {fi_type} is selected.\n        \"\"\"\n        feature_names = self.X.columns.tolist()\n        importance_dict = {}\n\n        for fi_type in fi_types:\n            model_name = type(self.model).__name__\n\n            if fi_type == \"shap\":\n                if isinstance(self.model, MLPClassifier):\n                    explainer = shap.Explainer(self.model.predict_proba, self.X)\n                else:\n                    explainer = shap.Explainer(self.model, self.X)\n\n                if isinstance(self.model, (RandomForestClassifier, XGBClassifier)):\n                    shap_values = explainer.shap_values(self.X, check_additivity=False)\n                    if (\n                        isinstance(self.model, RandomForestClassifier)\n                        and len(shap_values.shape) == 3\n                    ):\n                        shap_values = np.abs(shap_values).mean(axis=-1)\n                else:\n                    shap_values = explainer.shap_values(self.X)\n\n                if isinstance(shap_values, list):\n                    shap_values_stacked = np.stack(shap_values, axis=-1)\n                    shap_values = np.abs(shap_values_stacked).mean(axis=-1)\n                else:\n                    shap_values = np.abs(shap_values)\n\n            elif fi_type == \"permutation\":\n                result = permutation_importance(\n                    estimator=self.model,\n                    X=self.X,\n                    y=self.y,\n                    n_repeats=10,\n                    random_state=0,\n                )\n                fi_df = pd.DataFrame({\n                    \"Feature\": feature_names,\n                    \"Importance\": result.importances_mean,\n                })\n\n            elif fi_type == \"standard\":\n                if isinstance(self.model, (RandomForestClassifier, XGBClassifier)):\n                    importances = self.model.feature_importances_\n                elif isinstance(self.model, LogisticRegression):\n                    importances = abs(self.model.coef_[0])\n                else:\n                    print(f\"Standard FI not supported for model type {model_name}.\")\n                    continue\n                fi_df = pd.DataFrame({\n                    \"Feature\": feature_names,\n                    \"Importance\": importances,\n                })\n\n            else:\n                raise ValueError(f\"Invalid fi_type: {fi_type}\")\n\n            if self.aggregate:\n                if fi_type == \"shap\":\n                    aggregated_shap_values, aggregated_feature_names = (\n                        self._aggregate_shap_one_hot(\n                            shap_values=shap_values, feature_names=feature_names\n                        )\n                    )\n                    aggregated_feature_names = self._feature_mapping(\n                        aggregated_feature_names\n                    )\n\n                    aggregated_shap_df = pd.DataFrame(\n                        aggregated_shap_values, columns=aggregated_feature_names\n                    )\n                    importance_dict[f\"{model_name}_{fi_type}\"] = aggregated_shap_df\n\n                    plt.figure(figsize=(4, 4), dpi=300)\n                    shap.summary_plot(\n                        aggregated_shap_values,\n                        feature_names=aggregated_feature_names,\n                        plot_type=\"bar\",\n                        show=False,\n                    )\n\n                    ax = plt.gca()\n                    for bar in ax.patches:\n                        bar.set_edgecolor(\"black\")\n                        bar.set_linewidth(1)\n\n                    ax.spines[\"left\"].set_visible(True)\n                    ax.spines[\"left\"].set_color(\"black\")\n                    ax.spines[\"bottom\"].set_color(\"black\")\n                    ax.tick_params(axis=\"y\", colors=\"black\")\n\n                    plt.title(f\"{model_name}: SHAP Feature Importance\")\n                    plt.tight_layout()\n\n                    if save:\n                        if name is None:\n                            raise ValueError(\n                                \"'name' argument must required when 'save' is True.\"\n                            )\n                        plt.savefig(name + fi_type + \".svg\", format=\"svg\", dpi=300)\n\n                else:\n                    fi_df_aggregated = self._aggregate_one_hot_importances(fi_df=fi_df)\n                    fi_df_aggregated = fi_df_aggregated.sort_values(\n                        by=\"Importance\", ascending=False\n                    )\n\n                    fi_df_aggregated[\"Feature\"] = self._feature_mapping(\n                        fi_df_aggregated[\"Feature\"]\n                    )\n\n                    importance_dict[f\"{model_name}_{fi_type}\"] = fi_df_aggregated\n\n                    top10_fi_df_aggregated = fi_df_aggregated.head(10)\n                    bottom10_fi_df_aggregated = fi_df_aggregated.tail(10)\n\n                    placeholder = pd.DataFrame(\n                        [[\"[...]\", 0]], columns=[\"Feature\", \"Importance\"]\n                    )\n                    selected_fi_df_aggregated = pd.concat(\n                        [\n                            top10_fi_df_aggregated,\n                            placeholder,\n                            bottom10_fi_df_aggregated,\n                        ],\n                        ignore_index=True,\n                    )\n\n            else:\n                if fi_type == \"shap\":\n                    feature_names = self._feature_mapping(feature_names)\n\n                    plt.figure(figsize=(4, 4), dpi=300)\n                    shap.summary_plot(\n                        shap_values,\n                        self.X,\n                        plot_type=\"bar\",\n                        feature_names=feature_names,\n                        show=False,\n                    )\n                    ax = plt.gca()\n                    for bar in ax.patches:\n                        bar.set_edgecolor(\"black\")\n                        bar.set_linewidth(1)\n\n                    ax.spines[\"left\"].set_visible(True)\n                    ax.spines[\"left\"].set_color(\"black\")\n                    ax.spines[\"bottom\"].set_color(\"black\")\n                    ax.tick_params(axis=\"y\", colors=\"black\")\n                    plt.title(f\"{model_name}: SHAP Feature Importance\")\n                    plt.tight_layout()\n\n                    if save:\n                        if name is None:\n                            raise ValueError(\n                                \"'name' argument must required when 'save' is True.\"\n                            )\n                        plt.savefig(name + fi_type + \".svg\", format=\"svg\", dpi=300)\n\n                else:\n                    fi_df = fi_df.sort_values(by=\"Importance\", ascending=False)\n                    fi_df[\"Feature\"] = self._feature_mapping(fi_df[\"Feature\"])\n                    importance_dict[model_name] = fi_df\n\n            if fi_type != \"shap\":\n                plt.figure(figsize=(8, 6), dpi=300)\n\n                if self.aggregate:\n                    plt.bar(\n                        selected_fi_df_aggregated[\"Feature\"],\n                        selected_fi_df_aggregated[\"Importance\"],\n                        edgecolor=\"black\",\n                        linewidth=1,\n                        color=\"#078294\",\n                    )\n                else:\n                    plt.bar(fi_df[\"Feature\"], fi_df[\"Importance\"])\n\n                plt.title(f\"{model_name}: {fi_type.title()} Feature Importance\")\n                plt.xticks(rotation=90, fontsize=12)\n                plt.yticks(fontsize=12)\n                plt.axhline(y=0, color=\"black\", linewidth=1)\n                ax = plt.gca()\n                ax.spines[\"top\"].set_visible(False)\n                ax.spines[\"right\"].set_visible(False)\n                ax.spines[\"bottom\"].set_visible(False)\n                plt.ylabel(\"Importance\", fontsize=12)\n                plt.tight_layout()\n                if save:\n                    if name is None:\n                        raise ValueError(\n                            \"'name' argument must required when 'save' is True.\"\n                        )\n                    plt.savefig(name + fi_type + \".svg\", format=\"svg\", dpi=300)\n                plt.show()\n\n    def analyze_brier_within_clusters(\n        self,\n        clustering_algorithm: Type = AgglomerativeClustering,\n        n_clusters: int = 3,\n        tight_layout: bool = False,\n    ) -&gt; Union[None, Tuple[plt.Figure, plt.Figure, pd.DataFrame]]:\n        \"\"\"Analyze distribution of Brier scores within clusters formed by input data.\n\n        Args:\n            clustering_algorithm (Type): Clustering algorithm class from sklearn to use\n                for clustering.\n            n_clusters (int): Number of clusters to form.\n            tight_layout (bool): If True, applies tight layout to the plots. Defaults\n                to False.\n\n        Returns:\n            Union: Tuple containing the Brier score plot, heatmap plot, and clustered\n                DataFrame with 'Cluster' and 'Brier_Score' columns.\n        \"\"\"\n        probas = self.model.predict_proba(self.X)[:, 1]\n        brier_scores = [\n            brier_score_loss(y_true=[true], y_proba=[proba])\n            for true, proba in zip(self.y, probas, strict=False)\n        ]\n\n        X_cluster_input = (\n            self._aggregate_one_hot_features_for_clustering(X=self.X)\n            if self.aggregate\n            else self.X\n        )\n\n        clustering_algo = clustering_algorithm(n_clusters=n_clusters)\n        cluster_labels = clustering_algo.fit_predict(X_cluster_input)\n        X_clustered = X_cluster_input.assign(\n            Cluster=cluster_labels, Brier_Score=brier_scores\n        )\n        mean_brier_scores = X_clustered.groupby(\"Cluster\")[\"Brier_Score\"].mean()\n        cluster_counts = X_clustered[\"Cluster\"].value_counts().sort_index()\n\n        print(\n            \"\\nMean Brier Score per cluster:\\n\",\n            mean_brier_scores,\n            \"\\n\\nNumber of observations per cluster:\\n\",\n            cluster_counts,\n        )\n\n        feature_averages = (\n            X_clustered.drop([\"Cluster\", \"Brier_Score\"], axis=1)\n            .groupby(X_clustered[\"Cluster\"])\n            .mean()\n        )\n\n        feature_averages.columns = self._feature_mapping(\n            features=feature_averages.columns\n        )\n\n        plt.figure(figsize=(8, 4), dpi=300)\n        plt.rcParams.update({\"font.size\": 12})\n        sns.violinplot(\n            x=\"Cluster\",\n            y=\"Brier_Score\",\n            data=X_clustered,\n            linewidth=0.5,\n            color=\"#078294\",\n            inner_kws={\"box_width\": 6, \"whis_width\": 0.5},\n        )\n        sns.pointplot(\n            x=\"Cluster\",\n            y=\"Brier_Score\",\n            data=mean_brier_scores.reset_index(),\n            color=\"darkred\",\n            markers=\"D\",\n            scale=0.75,\n            ci=None,\n        )\n        sns.despine(top=True, right=True)\n        plt.ylabel(\"Brier Score\")\n        plt.xlabel(\"Cluster\", fontsize=12)\n        plt.title(\"Brier Score Distribution in Clusters\", fontsize=12)\n        plt.xticks(fontsize=12)\n        plt.yticks(fontsize=12)\n\n        if tight_layout:\n            plt.tight_layout()\n\n        brier_plot = plt.gcf()\n\n        plt.figure(figsize=(8, 4), dpi=300)\n        annot_array = np.around(feature_averages.values, decimals=1)\n        sns.heatmap(\n            feature_averages,\n            cmap=\"viridis\",\n            annot=annot_array,\n            fmt=\".1f\",\n            annot_kws={\"size\": 5, \"rotation\": 90},\n        )\n        if tight_layout:\n            plt.tight_layout()\n\n        ax = plt.gca()\n        for spine in ax.spines.values():\n            spine.set_visible(True)\n            spine.set_color(\"black\")\n            spine.set_linewidth(1)\n\n        cbar = ax.collections[0].colorbar\n        cbar.outline.set_visible(True)\n        cbar.outline.set_edgecolor(\"black\")\n        cbar.outline.set_linewidth(1)\n        heatmap_plot = plt.gcf()\n\n        return brier_plot, heatmap_plot, X_clustered\n</code></pre>"},{"location":"reference/evaluation/modelevaluator/#periomod.evaluation.ModelEvaluator.__init__","title":"<code>__init__(X, y, model, encoding=None, aggregate=True)</code>","text":"<p>Initialize the FeatureImportance class.</p> Source code in <code>periomod/evaluation/_eval.py</code> <pre><code>def __init__(\n    self,\n    X: pd.DataFrame,\n    y: pd.Series,\n    model: Union[\n        RandomForestClassifier,\n        LogisticRegression,\n        MLPClassifier,\n        XGBClassifier,\n    ],\n    encoding: Optional[str] = None,\n    aggregate: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the FeatureImportance class.\"\"\"\n    super().__init__(X=X, y=y, model=model, encoding=encoding, aggregate=aggregate)\n</code></pre>"},{"location":"reference/evaluation/modelevaluator/#periomod.evaluation.ModelEvaluator.analyze_brier_within_clusters","title":"<code>analyze_brier_within_clusters(clustering_algorithm=AgglomerativeClustering, n_clusters=3, tight_layout=False)</code>","text":"<p>Analyze distribution of Brier scores within clusters formed by input data.</p> <p>Parameters:</p> Name Type Description Default <code>clustering_algorithm</code> <code>Type</code> <p>Clustering algorithm class from sklearn to use for clustering.</p> <code>AgglomerativeClustering</code> <code>n_clusters</code> <code>int</code> <p>Number of clusters to form.</p> <code>3</code> <code>tight_layout</code> <code>bool</code> <p>If True, applies tight layout to the plots. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Union</code> <code>Union[None, Tuple[Figure, Figure, DataFrame]]</code> <p>Tuple containing the Brier score plot, heatmap plot, and clustered DataFrame with 'Cluster' and 'Brier_Score' columns.</p> Source code in <code>periomod/evaluation/_eval.py</code> <pre><code>def analyze_brier_within_clusters(\n    self,\n    clustering_algorithm: Type = AgglomerativeClustering,\n    n_clusters: int = 3,\n    tight_layout: bool = False,\n) -&gt; Union[None, Tuple[plt.Figure, plt.Figure, pd.DataFrame]]:\n    \"\"\"Analyze distribution of Brier scores within clusters formed by input data.\n\n    Args:\n        clustering_algorithm (Type): Clustering algorithm class from sklearn to use\n            for clustering.\n        n_clusters (int): Number of clusters to form.\n        tight_layout (bool): If True, applies tight layout to the plots. Defaults\n            to False.\n\n    Returns:\n        Union: Tuple containing the Brier score plot, heatmap plot, and clustered\n            DataFrame with 'Cluster' and 'Brier_Score' columns.\n    \"\"\"\n    probas = self.model.predict_proba(self.X)[:, 1]\n    brier_scores = [\n        brier_score_loss(y_true=[true], y_proba=[proba])\n        for true, proba in zip(self.y, probas, strict=False)\n    ]\n\n    X_cluster_input = (\n        self._aggregate_one_hot_features_for_clustering(X=self.X)\n        if self.aggregate\n        else self.X\n    )\n\n    clustering_algo = clustering_algorithm(n_clusters=n_clusters)\n    cluster_labels = clustering_algo.fit_predict(X_cluster_input)\n    X_clustered = X_cluster_input.assign(\n        Cluster=cluster_labels, Brier_Score=brier_scores\n    )\n    mean_brier_scores = X_clustered.groupby(\"Cluster\")[\"Brier_Score\"].mean()\n    cluster_counts = X_clustered[\"Cluster\"].value_counts().sort_index()\n\n    print(\n        \"\\nMean Brier Score per cluster:\\n\",\n        mean_brier_scores,\n        \"\\n\\nNumber of observations per cluster:\\n\",\n        cluster_counts,\n    )\n\n    feature_averages = (\n        X_clustered.drop([\"Cluster\", \"Brier_Score\"], axis=1)\n        .groupby(X_clustered[\"Cluster\"])\n        .mean()\n    )\n\n    feature_averages.columns = self._feature_mapping(\n        features=feature_averages.columns\n    )\n\n    plt.figure(figsize=(8, 4), dpi=300)\n    plt.rcParams.update({\"font.size\": 12})\n    sns.violinplot(\n        x=\"Cluster\",\n        y=\"Brier_Score\",\n        data=X_clustered,\n        linewidth=0.5,\n        color=\"#078294\",\n        inner_kws={\"box_width\": 6, \"whis_width\": 0.5},\n    )\n    sns.pointplot(\n        x=\"Cluster\",\n        y=\"Brier_Score\",\n        data=mean_brier_scores.reset_index(),\n        color=\"darkred\",\n        markers=\"D\",\n        scale=0.75,\n        ci=None,\n    )\n    sns.despine(top=True, right=True)\n    plt.ylabel(\"Brier Score\")\n    plt.xlabel(\"Cluster\", fontsize=12)\n    plt.title(\"Brier Score Distribution in Clusters\", fontsize=12)\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n\n    if tight_layout:\n        plt.tight_layout()\n\n    brier_plot = plt.gcf()\n\n    plt.figure(figsize=(8, 4), dpi=300)\n    annot_array = np.around(feature_averages.values, decimals=1)\n    sns.heatmap(\n        feature_averages,\n        cmap=\"viridis\",\n        annot=annot_array,\n        fmt=\".1f\",\n        annot_kws={\"size\": 5, \"rotation\": 90},\n    )\n    if tight_layout:\n        plt.tight_layout()\n\n    ax = plt.gca()\n    for spine in ax.spines.values():\n        spine.set_visible(True)\n        spine.set_color(\"black\")\n        spine.set_linewidth(1)\n\n    cbar = ax.collections[0].colorbar\n    cbar.outline.set_visible(True)\n    cbar.outline.set_edgecolor(\"black\")\n    cbar.outline.set_linewidth(1)\n    heatmap_plot = plt.gcf()\n\n    return brier_plot, heatmap_plot, X_clustered\n</code></pre>"},{"location":"reference/evaluation/modelevaluator/#periomod.evaluation.ModelEvaluator.evaluate_feature_importance","title":"<code>evaluate_feature_importance(fi_types, save=False, name=None)</code>","text":"<p>Evaluate the feature importance for a list of trained models.</p> <p>Parameters:</p> Name Type Description Default <code>fi_types</code> <code>List[str]</code> <p>Methods of feature importance evaluation: 'shap', 'permutation', 'standard'.</p> required <code>save</code> <code>bool</code> <p>If True, saves the plot as an SVG file. Defaults to False.</p> <code>False</code> <code>name</code> <code>Optional[str]</code> <p>Name of the file to save the plot. Required when</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If invalid fi_type: {fi_type} is selected.</p> Source code in <code>periomod/evaluation/_eval.py</code> <pre><code>def evaluate_feature_importance(\n    self, fi_types: List[str], save: bool = False, name: Optional[str] = None\n) -&gt; None:\n    \"\"\"Evaluate the feature importance for a list of trained models.\n\n    Args:\n        fi_types (List[str]): Methods of feature importance evaluation:\n            'shap', 'permutation', 'standard'.\n        save (bool): If True, saves the plot as an SVG file. Defaults to False.\n        name (Optional[str]): Name of the file to save the plot. Required when\n\n    Raises:\n        ValueError: If invalid fi_type: {fi_type} is selected.\n    \"\"\"\n    feature_names = self.X.columns.tolist()\n    importance_dict = {}\n\n    for fi_type in fi_types:\n        model_name = type(self.model).__name__\n\n        if fi_type == \"shap\":\n            if isinstance(self.model, MLPClassifier):\n                explainer = shap.Explainer(self.model.predict_proba, self.X)\n            else:\n                explainer = shap.Explainer(self.model, self.X)\n\n            if isinstance(self.model, (RandomForestClassifier, XGBClassifier)):\n                shap_values = explainer.shap_values(self.X, check_additivity=False)\n                if (\n                    isinstance(self.model, RandomForestClassifier)\n                    and len(shap_values.shape) == 3\n                ):\n                    shap_values = np.abs(shap_values).mean(axis=-1)\n            else:\n                shap_values = explainer.shap_values(self.X)\n\n            if isinstance(shap_values, list):\n                shap_values_stacked = np.stack(shap_values, axis=-1)\n                shap_values = np.abs(shap_values_stacked).mean(axis=-1)\n            else:\n                shap_values = np.abs(shap_values)\n\n        elif fi_type == \"permutation\":\n            result = permutation_importance(\n                estimator=self.model,\n                X=self.X,\n                y=self.y,\n                n_repeats=10,\n                random_state=0,\n            )\n            fi_df = pd.DataFrame({\n                \"Feature\": feature_names,\n                \"Importance\": result.importances_mean,\n            })\n\n        elif fi_type == \"standard\":\n            if isinstance(self.model, (RandomForestClassifier, XGBClassifier)):\n                importances = self.model.feature_importances_\n            elif isinstance(self.model, LogisticRegression):\n                importances = abs(self.model.coef_[0])\n            else:\n                print(f\"Standard FI not supported for model type {model_name}.\")\n                continue\n            fi_df = pd.DataFrame({\n                \"Feature\": feature_names,\n                \"Importance\": importances,\n            })\n\n        else:\n            raise ValueError(f\"Invalid fi_type: {fi_type}\")\n\n        if self.aggregate:\n            if fi_type == \"shap\":\n                aggregated_shap_values, aggregated_feature_names = (\n                    self._aggregate_shap_one_hot(\n                        shap_values=shap_values, feature_names=feature_names\n                    )\n                )\n                aggregated_feature_names = self._feature_mapping(\n                    aggregated_feature_names\n                )\n\n                aggregated_shap_df = pd.DataFrame(\n                    aggregated_shap_values, columns=aggregated_feature_names\n                )\n                importance_dict[f\"{model_name}_{fi_type}\"] = aggregated_shap_df\n\n                plt.figure(figsize=(4, 4), dpi=300)\n                shap.summary_plot(\n                    aggregated_shap_values,\n                    feature_names=aggregated_feature_names,\n                    plot_type=\"bar\",\n                    show=False,\n                )\n\n                ax = plt.gca()\n                for bar in ax.patches:\n                    bar.set_edgecolor(\"black\")\n                    bar.set_linewidth(1)\n\n                ax.spines[\"left\"].set_visible(True)\n                ax.spines[\"left\"].set_color(\"black\")\n                ax.spines[\"bottom\"].set_color(\"black\")\n                ax.tick_params(axis=\"y\", colors=\"black\")\n\n                plt.title(f\"{model_name}: SHAP Feature Importance\")\n                plt.tight_layout()\n\n                if save:\n                    if name is None:\n                        raise ValueError(\n                            \"'name' argument must required when 'save' is True.\"\n                        )\n                    plt.savefig(name + fi_type + \".svg\", format=\"svg\", dpi=300)\n\n            else:\n                fi_df_aggregated = self._aggregate_one_hot_importances(fi_df=fi_df)\n                fi_df_aggregated = fi_df_aggregated.sort_values(\n                    by=\"Importance\", ascending=False\n                )\n\n                fi_df_aggregated[\"Feature\"] = self._feature_mapping(\n                    fi_df_aggregated[\"Feature\"]\n                )\n\n                importance_dict[f\"{model_name}_{fi_type}\"] = fi_df_aggregated\n\n                top10_fi_df_aggregated = fi_df_aggregated.head(10)\n                bottom10_fi_df_aggregated = fi_df_aggregated.tail(10)\n\n                placeholder = pd.DataFrame(\n                    [[\"[...]\", 0]], columns=[\"Feature\", \"Importance\"]\n                )\n                selected_fi_df_aggregated = pd.concat(\n                    [\n                        top10_fi_df_aggregated,\n                        placeholder,\n                        bottom10_fi_df_aggregated,\n                    ],\n                    ignore_index=True,\n                )\n\n        else:\n            if fi_type == \"shap\":\n                feature_names = self._feature_mapping(feature_names)\n\n                plt.figure(figsize=(4, 4), dpi=300)\n                shap.summary_plot(\n                    shap_values,\n                    self.X,\n                    plot_type=\"bar\",\n                    feature_names=feature_names,\n                    show=False,\n                )\n                ax = plt.gca()\n                for bar in ax.patches:\n                    bar.set_edgecolor(\"black\")\n                    bar.set_linewidth(1)\n\n                ax.spines[\"left\"].set_visible(True)\n                ax.spines[\"left\"].set_color(\"black\")\n                ax.spines[\"bottom\"].set_color(\"black\")\n                ax.tick_params(axis=\"y\", colors=\"black\")\n                plt.title(f\"{model_name}: SHAP Feature Importance\")\n                plt.tight_layout()\n\n                if save:\n                    if name is None:\n                        raise ValueError(\n                            \"'name' argument must required when 'save' is True.\"\n                        )\n                    plt.savefig(name + fi_type + \".svg\", format=\"svg\", dpi=300)\n\n            else:\n                fi_df = fi_df.sort_values(by=\"Importance\", ascending=False)\n                fi_df[\"Feature\"] = self._feature_mapping(fi_df[\"Feature\"])\n                importance_dict[model_name] = fi_df\n\n        if fi_type != \"shap\":\n            plt.figure(figsize=(8, 6), dpi=300)\n\n            if self.aggregate:\n                plt.bar(\n                    selected_fi_df_aggregated[\"Feature\"],\n                    selected_fi_df_aggregated[\"Importance\"],\n                    edgecolor=\"black\",\n                    linewidth=1,\n                    color=\"#078294\",\n                )\n            else:\n                plt.bar(fi_df[\"Feature\"], fi_df[\"Importance\"])\n\n            plt.title(f\"{model_name}: {fi_type.title()} Feature Importance\")\n            plt.xticks(rotation=90, fontsize=12)\n            plt.yticks(fontsize=12)\n            plt.axhline(y=0, color=\"black\", linewidth=1)\n            ax = plt.gca()\n            ax.spines[\"top\"].set_visible(False)\n            ax.spines[\"right\"].set_visible(False)\n            ax.spines[\"bottom\"].set_visible(False)\n            plt.ylabel(\"Importance\", fontsize=12)\n            plt.tight_layout()\n            if save:\n                if name is None:\n                    raise ValueError(\n                        \"'name' argument must required when 'save' is True.\"\n                    )\n                plt.savefig(name + fi_type + \".svg\", format=\"svg\", dpi=300)\n            plt.show()\n</code></pre>"},{"location":"reference/inference/","title":"periomod.inference","text":"<p>This module provides inference-related classes for model evaluation and prediction.</p> Class Description BaseModelInference Base class for inference functionality. ModelInference Handles model-specific inference logic."},{"location":"reference/inference/basemodelinference/","title":"BaseModelInference","text":"<p>               Bases: <code>BaseConfig</code>, <code>ABC</code></p> <p>Abstract base class for performing model inference and jackknife resampling.</p> <p>This class defines methods for generating predictions, preparing data for inference, and implementing jackknife resampling with confidence intervals. It is designed to handle binary and multiclass classification tasks and allows encoding configurations for model compatibility.</p> Inherits <ul> <li><code>BaseConfig</code>: Provides configuration settings for data processing.</li> <li><code>ABC</code>: Specifies abstract methods for subclasses to implement.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>classification</code> <code>str</code> <p>The type of classification task, either 'binary' or 'multiclass', used to configure the inference process.</p> required <code>model</code> <code>Any</code> <p>A trained model instance that implements a <code>predict_proba</code> method for generating class probabilities.</p> required <code>verbose</code> <code>bool</code> <p>If True, enables detailed logging of inference steps.</p> required <p>Attributes:</p> Name Type Description <code>classification</code> <code>str</code> <p>Stores the classification type ('binary' or 'multiclass') for model compatibility.</p> <code>model</code> <p>The trained model used to make predictions during inference.</p> <code>verbose</code> <code>bool</code> <p>Indicates if verbose logging is enabled during inference.</p> <p>Methods:</p> Name Description <code>predict</code> <p>Run predictions on a batch of input data, returning predicted classes and probabilities.</p> <code>create_predict_data</code> <p>Prepare and encode data for inference based on raw data and patient data, supporting one-hot or target encoding formats.</p> <code>prepare_inference</code> <p>Prepares data for inference, performing any necessary preprocessing and scaling.</p> <code>patient_inference</code> <p>Runs predictions on specific patient data, returning results with predicted classes and probabilities.</p> <code>process_patient</code> <p>Processes a patient\u2019s data for jackknife resampling, retraining the model while excluding the patient from training.</p> Abstract Methods <ul> <li><code>jackknife_resampling</code>: Performs jackknife resampling by retraining   the model on various patient subsets.</li> <li><code>jackknife_confidence_intervals</code>: Computes confidence intervals   based on jackknife resampling results.</li> <li><code>plot_jackknife_intervals</code>: Visualizes jackknife confidence intervals   for predictions.</li> <li><code>jackknife_inference</code>: Executes full jackknife inference, including   interval computation and optional plotting.</li> </ul> Source code in <code>periomod/inference/_baseinference.py</code> <pre><code>class BaseModelInference(BaseConfig, ABC):\n    \"\"\"Abstract base class for performing model inference and jackknife resampling.\n\n    This class defines methods for generating predictions, preparing data for\n    inference, and implementing jackknife resampling with confidence intervals.\n    It is designed to handle binary and multiclass classification tasks and\n    allows encoding configurations for model compatibility.\n\n    Inherits:\n        - `BaseConfig`: Provides configuration settings for data processing.\n        - `ABC`: Specifies abstract methods for subclasses to implement.\n\n    Args:\n        classification (str): The type of classification task, either 'binary'\n            or 'multiclass', used to configure the inference process.\n        model: A trained model instance that implements a `predict_proba` method\n            for generating class probabilities.\n        verbose (bool): If True, enables detailed logging of inference steps.\n\n    Attributes:\n        classification (str): Stores the classification type ('binary' or 'multiclass')\n            for model compatibility.\n        model: The trained model used to make predictions during inference.\n        verbose (bool): Indicates if verbose logging is enabled during inference.\n\n    Methods:\n        predict: Run predictions on a batch of input data, returning\n            predicted classes and probabilities.\n        create_predict_data: Prepare and encode data for inference based on raw data\n            and patient data, supporting one-hot or target encoding formats.\n        prepare_inference: Prepares data for inference, performing any\n            necessary preprocessing and scaling.\n        patient_inference: Runs predictions on specific patient data,\n            returning results with predicted classes and probabilities.\n        process_patient: Processes a patient\u2019s data for jackknife resampling,\n            retraining the model while excluding the patient from training.\n\n    Abstract Methods:\n        - `jackknife_resampling`: Performs jackknife resampling by retraining\n          the model on various patient subsets.\n        - `jackknife_confidence_intervals`: Computes confidence intervals\n          based on jackknife resampling results.\n        - `plot_jackknife_intervals`: Visualizes jackknife confidence intervals\n          for predictions.\n        - `jackknife_inference`: Executes full jackknife inference, including\n          interval computation and optional plotting.\n    \"\"\"\n\n    def __init__(self, classification: str, model: Any, verbose: bool):\n        \"\"\"Initialize the ModelInference class with a trained model.\"\"\"\n        super().__init__()\n        self.classification = classification\n        self.model = model\n        self.verbose = verbose\n\n    def predict(self, input_data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Run prediction on a batch of input data.\n\n        Args:\n            input_data (pd.DataFrame): DataFrame containing feature values.\n\n        Returns:\n            probs_df: DataFrame with predictions and probabilities for each class.\n        \"\"\"\n        probs = self.model.predict_proba(input_data)\n\n        if self.classification == \"binary\":\n            if (\n                hasattr(self.model, \"best_threshold\")\n                and self.model.best_threshold is not None\n            ):\n                preds = (probs[:, 1] &gt;= self.model.best_threshold).astype(int)\n        preds = self.model.predict(input_data)\n        classes = [str(cls) for cls in self.model.classes_]\n        probs_df = pd.DataFrame(probs, columns=classes, index=input_data.index)\n        probs_df[\"prediction\"] = preds\n        return probs_df\n\n    def create_predict_data(\n        self,\n        raw_data: pd.DataFrame,\n        patient_data: pd.DataFrame,\n        encoding: str,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Creates prediction data for model inference.\n\n        Args:\n            raw_data (pd.DataFrame): The raw, preprocessed data.\n            patient_data (pd.DataFrame): Original patient data before preprocessing.\n            encoding (str): Type of encoding used ('one_hot' or 'target').\n\n        Returns:\n            predict_data: A DataFrame containing the prepared data for model prediction.\n\n        Raises:\n            ValueError: If encoding is invalid.\n            ValueError: If model type is not supported for feature extraction.\n        \"\"\"\n        base_data = raw_data.copy()\n\n        if encoding == \"one_hot\":\n            drop_columns = self.cat_vars + self.infect_vars\n            base_data = base_data.drop(columns=drop_columns, errors=\"ignore\")\n            encoded_data = pd.DataFrame(index=base_data.index)\n\n            for tooth_num in range(11, 49):\n                if tooth_num % 10 == 0 or tooth_num % 10 == 9:\n                    continue\n                encoded_data[f\"tooth_{tooth_num}\"] = 0\n\n            for feature, max_val in self.cat_map.items():\n                for i in range(0, max_val + 1):\n                    encoded_data[f\"{feature}_{i}\"] = 0\n\n            for idx, row in patient_data.iterrows():\n                encoded_data.loc[idx, f\"tooth_{row['tooth']}\"] = 1\n                for feature in self.cat_map:\n                    encoded_data.loc[idx, f\"{feature}_{row[feature]}\"] = 1\n\n                complete_data = pd.concat(\n                    [\n                        base_data.reset_index(drop=True),\n                        encoded_data.reset_index(drop=True),\n                    ],\n                    axis=1,\n                )\n\n            complete_data = complete_data.loc[:, ~complete_data.columns.duplicated()]\n            duplicates = complete_data.columns[\n                complete_data.columns.duplicated()\n            ].unique()\n            if len(duplicates) &gt; 0:\n                print(\"Duplicate columns found:\", duplicates)\n\n        elif encoding == \"target\":\n            complete_data = base_data.copy()\n            for column in self.target_cols:\n                if column in patient_data.columns:\n                    complete_data[column] = patient_data[column].to_numpy()\n        else:\n            raise ValueError(f\"Unsupported encoding type: {encoding}\")\n\n        if hasattr(self.model, \"get_booster\"):\n            model_features = self.model.get_booster().feature_names\n        elif hasattr(self.model, \"feature_names_in_\"):\n            model_features = self.model.feature_names_in_\n        else:\n            raise ValueError(\"Model type not supported for feature extraction\")\n\n        for feature in model_features:\n            if feature not in complete_data.columns:\n                complete_data[feature] = 0\n\n        predict_data = complete_data[model_features]\n\n        return predict_data\n\n    def prepare_inference(\n        self,\n        task: str,\n        patient_data: pd.DataFrame,\n        encoding: str,\n        X_train: pd.DataFrame,\n        y_train: pd.Series,\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"Prepares the data for inference.\n\n        Args:\n            task (str): The task name for which the model was trained.\n            patient_data (pd.DataFrame): The patient's data as a DataFrame.\n            encoding (str): Encoding type (\"one_hot\" or \"target\").\n            X_train (pd.DataFrame): Training features for target encoding.\n            y_train (pd.Series): Training target for target encoding.\n\n        Returns:\n            Tuple: Transformed patient data for prediction and patient data.\n\n        Raises:\n            ValueError: If patient data is empty.\n        \"\"\"\n        if patient_data.empty:\n            raise ValueError(\n                \"Patient data empty. Please submit data before running inference.\"\n            )\n        if self.verbose:\n            print(\"Patient Data Received for Inference:\\n\", patient_data)\n\n        engine = StaticProcessEngine()\n        dataloader = ProcessedDataLoader(task, encoding)\n        patient_data[self.group_col] = \"inference_patient\"\n        raw_data = engine.create_tooth_features(\n            data=patient_data, neighbors=True, patient_id=False\n        )\n\n        if encoding == \"target\":\n            raw_data = dataloader.encode_categorical_columns(data=raw_data)\n            resampler = Resampler(self.classification, encoding)\n            _, raw_data = resampler.apply_target_encoding(\n                X=X_train, X_val=raw_data, y=y_train\n            )\n\n            for key in raw_data.columns:\n                if key not in self.cat_vars and key in patient_data.columns:\n                    raw_data[key] = patient_data[key].to_numpy()\n        else:\n            raw_data = self.create_predict_data(\n                raw_data=raw_data, patient_data=patient_data, encoding=encoding\n            )\n\n        predict_data = self.create_predict_data(\n            raw_data=raw_data, patient_data=patient_data, encoding=encoding\n        )\n        predict_data = dataloader.scale_numeric_columns(data=predict_data)\n\n        return predict_data, patient_data\n\n    def patient_inference(\n        self, predict_data: pd.DataFrame, patient_data: pd.DataFrame\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n        \"\"\"Run inference on the patient's data.\n\n        Args:\n            predict_data (pd.DataFrame): Transformed patient data for prediction.\n            patient_data (pd.DataFrame): The patient's data as a DataFrame.\n\n        Returns:\n            Tuple:\n                - predict_data: Transformed patient data for prediction.\n                - output_data: DataFrame with columns \"tooth\", \"side\",\n                transformed \"prediction\", and \"probability\".\n                - results: Original results from the model inference.\n        \"\"\"\n        results = self.predict(predict_data)\n        output_data = patient_data[[\"tooth\", \"side\"]].copy()\n        output_data[\"prediction\"] = results[\"prediction\"]\n        output_data[\"probability\"] = results.drop(columns=[\"prediction\"]).max(axis=1)\n        return predict_data, output_data, results\n\n    def process_patient(\n        self,\n        patient_id: int,\n        train_df: pd.DataFrame,\n        patient_data: pd.DataFrame,\n        encoding: str,\n        model_params: dict,\n        resampler: Resampler,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Processes a single patient's data for jackknife resampling.\n\n        Args:\n            patient_id (int): ID of the patient to exclude from training.\n            train_df (pd.DataFrame): Full training dataset.\n            patient_data (pd.DataFrame): The data for the patient(s) to predict on.\n            encoding (str): Encoding type used ('one_hot' or 'target').\n            model_params (dict): Parameters for the model initialization.\n            resampler (Resampler): Instance of the Resampler class for encoding.\n\n        Returns:\n            predictions_df: DataFrame containing patient predictions and probabilities.\n        \"\"\"\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=UserWarning)\n            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n        train_data = train_df[train_df[self.group_col] != patient_id]\n        X_train = train_data.drop(columns=[self.y])\n        y_train = train_data[self.y]\n\n        if encoding == \"target\":\n            X_train = X_train.drop(columns=[self.group_col], errors=\"ignore\")\n            X_train_enc, _ = resampler.apply_target_encoding(\n                X=X_train, X_val=None, y=y_train, jackknife=True\n            )\n        else:\n            X_train_enc = X_train.drop(columns=[self.group_col], errors=\"ignore\")\n\n        predictor = clone(self.model)\n        predictor.set_params(**model_params)\n        predictor.fit(X_train_enc, y_train)\n\n        if self.classification == \"binary\" and hasattr(predictor, \"best_threshold\"):\n            probs = get_probs(\n                model=predictor, classification=self.classification, X=patient_data\n            )\n            if probs is not None:\n                val_pred_classes = (probs &gt;= predictor.best_threshold).astype(int)\n            else:\n                val_pred_classes = predictor.predict(patient_data)\n        else:\n            val_pred_classes = predictor.predict(patient_data)\n            probs = predictor.predict_proba(patient_data)\n\n        predictions_df = pd.DataFrame(\n            probs,\n            columns=[str(cls) for cls in predictor.classes_],\n            index=patient_data.index,\n        )\n        return predictions_df.assign(\n            prediction=val_pred_classes,\n            iteration=patient_id,\n            data_index=patient_data.index,\n        )\n\n    @abstractmethod\n    def jackknife_resampling(\n        self,\n        train_df: pd.DataFrame,\n        patient_data: pd.DataFrame,\n        encoding: str,\n        model_params: dict,\n        sample_fraction: float,\n        n_jobs: int,\n    ):\n        \"\"\"Perform jackknife resampling with retraining for each patient.\n\n        Args:\n            train_df (pd.DataFrame): Full training dataset.\n            patient_data (pd.DataFrame): The data for the patient(s) to predict on.\n            encoding (str): Encoding type used ('one_hot' or 'target').\n            model_params (dict): Parameters for the model initialization.\n            sample_fraction (float, optional): Proportion of patient IDs to use for\n                jackknife resampling.\n            n_jobs (int, optional): Number of jobs to run in parallel.\n        \"\"\"\n\n    @abstractmethod\n    def jackknife_confidence_intervals(\n        self, jackknife_results: pd.DataFrame, alpha: float\n    ):\n        \"\"\"Compute confidence intervals from jackknife results.\n\n        Args:\n            jackknife_results (pd.DataFrame): DataFrame with jackknife predictions.\n            alpha (float, optional): Significance level for confidence intervals.\n        \"\"\"\n\n    @abstractmethod\n    def plot_jackknife_intervals(\n        self,\n        ci_dict: Dict[int, Dict[str, Dict[str, float]]],\n        data_indices: List[int],\n        original_preds: pd.DataFrame,\n    ):\n        \"\"\"Plot Jackknife confidence intervals.\n\n        Args:\n            ci_dict (Dict[int, Dict[str, Dict[str, float]]]): Confidence intervals for\n                each data index and class.\n            data_indices (List[int]): List of data indices to plot.\n            original_preds (pd.DataFrame): DataFrame containing original predictions and\n                probabilities for each data point.\n        \"\"\"\n\n    @abstractmethod\n    def jackknife_inference(\n        self,\n        model: Any,\n        train_df: pd.DataFrame,\n        patient_data: pd.DataFrame,\n        encoding: str,\n        inference_results: pd.DataFrame,\n        alpha: float,\n        sample_fraction: float,\n        n_jobs: int,\n        max_plots: int,\n    ):\n        \"\"\"Run jackknife inference and generate confidence intervals and plots.\n\n        Args:\n            model (Any): Trained model instance.\n            train_df (pd.DataFrame): Training DataFrame.\n            patient_data (pd.DataFrame): Patient data to predict on.\n            encoding (str): Encoding type.\n            inference_results (pd.DataFrame): Original inference results.\n            alpha (float, optional): Significance level for confidence intervals.\n            sample_fraction (float, optional): Fraction of patient IDs for jackknife.\n            n_jobs (int, optional): Number of parallel jobs.\n            max_plots (int): Maximum number of plots for jackknife intervals.\n        \"\"\"\n</code></pre>"},{"location":"reference/inference/basemodelinference/#periomod.inference.BaseModelInference.__init__","title":"<code>__init__(classification, model, verbose)</code>","text":"<p>Initialize the ModelInference class with a trained model.</p> Source code in <code>periomod/inference/_baseinference.py</code> <pre><code>def __init__(self, classification: str, model: Any, verbose: bool):\n    \"\"\"Initialize the ModelInference class with a trained model.\"\"\"\n    super().__init__()\n    self.classification = classification\n    self.model = model\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/inference/basemodelinference/#periomod.inference.BaseModelInference.create_predict_data","title":"<code>create_predict_data(raw_data, patient_data, encoding)</code>","text":"<p>Creates prediction data for model inference.</p> <p>Parameters:</p> Name Type Description Default <code>raw_data</code> <code>DataFrame</code> <p>The raw, preprocessed data.</p> required <code>patient_data</code> <code>DataFrame</code> <p>Original patient data before preprocessing.</p> required <code>encoding</code> <code>str</code> <p>Type of encoding used ('one_hot' or 'target').</p> required <p>Returns:</p> Name Type Description <code>predict_data</code> <code>DataFrame</code> <p>A DataFrame containing the prepared data for model prediction.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If encoding is invalid.</p> <code>ValueError</code> <p>If model type is not supported for feature extraction.</p> Source code in <code>periomod/inference/_baseinference.py</code> <pre><code>def create_predict_data(\n    self,\n    raw_data: pd.DataFrame,\n    patient_data: pd.DataFrame,\n    encoding: str,\n) -&gt; pd.DataFrame:\n    \"\"\"Creates prediction data for model inference.\n\n    Args:\n        raw_data (pd.DataFrame): The raw, preprocessed data.\n        patient_data (pd.DataFrame): Original patient data before preprocessing.\n        encoding (str): Type of encoding used ('one_hot' or 'target').\n\n    Returns:\n        predict_data: A DataFrame containing the prepared data for model prediction.\n\n    Raises:\n        ValueError: If encoding is invalid.\n        ValueError: If model type is not supported for feature extraction.\n    \"\"\"\n    base_data = raw_data.copy()\n\n    if encoding == \"one_hot\":\n        drop_columns = self.cat_vars + self.infect_vars\n        base_data = base_data.drop(columns=drop_columns, errors=\"ignore\")\n        encoded_data = pd.DataFrame(index=base_data.index)\n\n        for tooth_num in range(11, 49):\n            if tooth_num % 10 == 0 or tooth_num % 10 == 9:\n                continue\n            encoded_data[f\"tooth_{tooth_num}\"] = 0\n\n        for feature, max_val in self.cat_map.items():\n            for i in range(0, max_val + 1):\n                encoded_data[f\"{feature}_{i}\"] = 0\n\n        for idx, row in patient_data.iterrows():\n            encoded_data.loc[idx, f\"tooth_{row['tooth']}\"] = 1\n            for feature in self.cat_map:\n                encoded_data.loc[idx, f\"{feature}_{row[feature]}\"] = 1\n\n            complete_data = pd.concat(\n                [\n                    base_data.reset_index(drop=True),\n                    encoded_data.reset_index(drop=True),\n                ],\n                axis=1,\n            )\n\n        complete_data = complete_data.loc[:, ~complete_data.columns.duplicated()]\n        duplicates = complete_data.columns[\n            complete_data.columns.duplicated()\n        ].unique()\n        if len(duplicates) &gt; 0:\n            print(\"Duplicate columns found:\", duplicates)\n\n    elif encoding == \"target\":\n        complete_data = base_data.copy()\n        for column in self.target_cols:\n            if column in patient_data.columns:\n                complete_data[column] = patient_data[column].to_numpy()\n    else:\n        raise ValueError(f\"Unsupported encoding type: {encoding}\")\n\n    if hasattr(self.model, \"get_booster\"):\n        model_features = self.model.get_booster().feature_names\n    elif hasattr(self.model, \"feature_names_in_\"):\n        model_features = self.model.feature_names_in_\n    else:\n        raise ValueError(\"Model type not supported for feature extraction\")\n\n    for feature in model_features:\n        if feature not in complete_data.columns:\n            complete_data[feature] = 0\n\n    predict_data = complete_data[model_features]\n\n    return predict_data\n</code></pre>"},{"location":"reference/inference/basemodelinference/#periomod.inference.BaseModelInference.jackknife_confidence_intervals","title":"<code>jackknife_confidence_intervals(jackknife_results, alpha)</code>  <code>abstractmethod</code>","text":"<p>Compute confidence intervals from jackknife results.</p> <p>Parameters:</p> Name Type Description Default <code>jackknife_results</code> <code>DataFrame</code> <p>DataFrame with jackknife predictions.</p> required <code>alpha</code> <code>float</code> <p>Significance level for confidence intervals.</p> required Source code in <code>periomod/inference/_baseinference.py</code> <pre><code>@abstractmethod\ndef jackknife_confidence_intervals(\n    self, jackknife_results: pd.DataFrame, alpha: float\n):\n    \"\"\"Compute confidence intervals from jackknife results.\n\n    Args:\n        jackknife_results (pd.DataFrame): DataFrame with jackknife predictions.\n        alpha (float, optional): Significance level for confidence intervals.\n    \"\"\"\n</code></pre>"},{"location":"reference/inference/basemodelinference/#periomod.inference.BaseModelInference.jackknife_inference","title":"<code>jackknife_inference(model, train_df, patient_data, encoding, inference_results, alpha, sample_fraction, n_jobs, max_plots)</code>  <code>abstractmethod</code>","text":"<p>Run jackknife inference and generate confidence intervals and plots.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Trained model instance.</p> required <code>train_df</code> <code>DataFrame</code> <p>Training DataFrame.</p> required <code>patient_data</code> <code>DataFrame</code> <p>Patient data to predict on.</p> required <code>encoding</code> <code>str</code> <p>Encoding type.</p> required <code>inference_results</code> <code>DataFrame</code> <p>Original inference results.</p> required <code>alpha</code> <code>float</code> <p>Significance level for confidence intervals.</p> required <code>sample_fraction</code> <code>float</code> <p>Fraction of patient IDs for jackknife.</p> required <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs.</p> required <code>max_plots</code> <code>int</code> <p>Maximum number of plots for jackknife intervals.</p> required Source code in <code>periomod/inference/_baseinference.py</code> <pre><code>@abstractmethod\ndef jackknife_inference(\n    self,\n    model: Any,\n    train_df: pd.DataFrame,\n    patient_data: pd.DataFrame,\n    encoding: str,\n    inference_results: pd.DataFrame,\n    alpha: float,\n    sample_fraction: float,\n    n_jobs: int,\n    max_plots: int,\n):\n    \"\"\"Run jackknife inference and generate confidence intervals and plots.\n\n    Args:\n        model (Any): Trained model instance.\n        train_df (pd.DataFrame): Training DataFrame.\n        patient_data (pd.DataFrame): Patient data to predict on.\n        encoding (str): Encoding type.\n        inference_results (pd.DataFrame): Original inference results.\n        alpha (float, optional): Significance level for confidence intervals.\n        sample_fraction (float, optional): Fraction of patient IDs for jackknife.\n        n_jobs (int, optional): Number of parallel jobs.\n        max_plots (int): Maximum number of plots for jackknife intervals.\n    \"\"\"\n</code></pre>"},{"location":"reference/inference/basemodelinference/#periomod.inference.BaseModelInference.jackknife_resampling","title":"<code>jackknife_resampling(train_df, patient_data, encoding, model_params, sample_fraction, n_jobs)</code>  <code>abstractmethod</code>","text":"<p>Perform jackknife resampling with retraining for each patient.</p> <p>Parameters:</p> Name Type Description Default <code>train_df</code> <code>DataFrame</code> <p>Full training dataset.</p> required <code>patient_data</code> <code>DataFrame</code> <p>The data for the patient(s) to predict on.</p> required <code>encoding</code> <code>str</code> <p>Encoding type used ('one_hot' or 'target').</p> required <code>model_params</code> <code>dict</code> <p>Parameters for the model initialization.</p> required <code>sample_fraction</code> <code>float</code> <p>Proportion of patient IDs to use for jackknife resampling.</p> required <code>n_jobs</code> <code>int</code> <p>Number of jobs to run in parallel.</p> required Source code in <code>periomod/inference/_baseinference.py</code> <pre><code>@abstractmethod\ndef jackknife_resampling(\n    self,\n    train_df: pd.DataFrame,\n    patient_data: pd.DataFrame,\n    encoding: str,\n    model_params: dict,\n    sample_fraction: float,\n    n_jobs: int,\n):\n    \"\"\"Perform jackknife resampling with retraining for each patient.\n\n    Args:\n        train_df (pd.DataFrame): Full training dataset.\n        patient_data (pd.DataFrame): The data for the patient(s) to predict on.\n        encoding (str): Encoding type used ('one_hot' or 'target').\n        model_params (dict): Parameters for the model initialization.\n        sample_fraction (float, optional): Proportion of patient IDs to use for\n            jackknife resampling.\n        n_jobs (int, optional): Number of jobs to run in parallel.\n    \"\"\"\n</code></pre>"},{"location":"reference/inference/basemodelinference/#periomod.inference.BaseModelInference.patient_inference","title":"<code>patient_inference(predict_data, patient_data)</code>","text":"<p>Run inference on the patient's data.</p> <p>Parameters:</p> Name Type Description Default <code>predict_data</code> <code>DataFrame</code> <p>Transformed patient data for prediction.</p> required <code>patient_data</code> <code>DataFrame</code> <p>The patient's data as a DataFrame.</p> required <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[DataFrame, DataFrame, DataFrame]</code> <ul> <li>predict_data: Transformed patient data for prediction.</li> <li>output_data: DataFrame with columns \"tooth\", \"side\", transformed \"prediction\", and \"probability\".</li> <li>results: Original results from the model inference.</li> </ul> Source code in <code>periomod/inference/_baseinference.py</code> <pre><code>def patient_inference(\n    self, predict_data: pd.DataFrame, patient_data: pd.DataFrame\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Run inference on the patient's data.\n\n    Args:\n        predict_data (pd.DataFrame): Transformed patient data for prediction.\n        patient_data (pd.DataFrame): The patient's data as a DataFrame.\n\n    Returns:\n        Tuple:\n            - predict_data: Transformed patient data for prediction.\n            - output_data: DataFrame with columns \"tooth\", \"side\",\n            transformed \"prediction\", and \"probability\".\n            - results: Original results from the model inference.\n    \"\"\"\n    results = self.predict(predict_data)\n    output_data = patient_data[[\"tooth\", \"side\"]].copy()\n    output_data[\"prediction\"] = results[\"prediction\"]\n    output_data[\"probability\"] = results.drop(columns=[\"prediction\"]).max(axis=1)\n    return predict_data, output_data, results\n</code></pre>"},{"location":"reference/inference/basemodelinference/#periomod.inference.BaseModelInference.plot_jackknife_intervals","title":"<code>plot_jackknife_intervals(ci_dict, data_indices, original_preds)</code>  <code>abstractmethod</code>","text":"<p>Plot Jackknife confidence intervals.</p> <p>Parameters:</p> Name Type Description Default <code>ci_dict</code> <code>Dict[int, Dict[str, Dict[str, float]]]</code> <p>Confidence intervals for each data index and class.</p> required <code>data_indices</code> <code>List[int]</code> <p>List of data indices to plot.</p> required <code>original_preds</code> <code>DataFrame</code> <p>DataFrame containing original predictions and probabilities for each data point.</p> required Source code in <code>periomod/inference/_baseinference.py</code> <pre><code>@abstractmethod\ndef plot_jackknife_intervals(\n    self,\n    ci_dict: Dict[int, Dict[str, Dict[str, float]]],\n    data_indices: List[int],\n    original_preds: pd.DataFrame,\n):\n    \"\"\"Plot Jackknife confidence intervals.\n\n    Args:\n        ci_dict (Dict[int, Dict[str, Dict[str, float]]]): Confidence intervals for\n            each data index and class.\n        data_indices (List[int]): List of data indices to plot.\n        original_preds (pd.DataFrame): DataFrame containing original predictions and\n            probabilities for each data point.\n    \"\"\"\n</code></pre>"},{"location":"reference/inference/basemodelinference/#periomod.inference.BaseModelInference.predict","title":"<code>predict(input_data)</code>","text":"<p>Run prediction on a batch of input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>DataFrame</code> <p>DataFrame containing feature values.</p> required <p>Returns:</p> Name Type Description <code>probs_df</code> <code>DataFrame</code> <p>DataFrame with predictions and probabilities for each class.</p> Source code in <code>periomod/inference/_baseinference.py</code> <pre><code>def predict(self, input_data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Run prediction on a batch of input data.\n\n    Args:\n        input_data (pd.DataFrame): DataFrame containing feature values.\n\n    Returns:\n        probs_df: DataFrame with predictions and probabilities for each class.\n    \"\"\"\n    probs = self.model.predict_proba(input_data)\n\n    if self.classification == \"binary\":\n        if (\n            hasattr(self.model, \"best_threshold\")\n            and self.model.best_threshold is not None\n        ):\n            preds = (probs[:, 1] &gt;= self.model.best_threshold).astype(int)\n    preds = self.model.predict(input_data)\n    classes = [str(cls) for cls in self.model.classes_]\n    probs_df = pd.DataFrame(probs, columns=classes, index=input_data.index)\n    probs_df[\"prediction\"] = preds\n    return probs_df\n</code></pre>"},{"location":"reference/inference/basemodelinference/#periomod.inference.BaseModelInference.prepare_inference","title":"<code>prepare_inference(task, patient_data, encoding, X_train, y_train)</code>","text":"<p>Prepares the data for inference.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>The task name for which the model was trained.</p> required <code>patient_data</code> <code>DataFrame</code> <p>The patient's data as a DataFrame.</p> required <code>encoding</code> <code>str</code> <p>Encoding type (\"one_hot\" or \"target\").</p> required <code>X_train</code> <code>DataFrame</code> <p>Training features for target encoding.</p> required <code>y_train</code> <code>Series</code> <p>Training target for target encoding.</p> required <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[DataFrame, DataFrame]</code> <p>Transformed patient data for prediction and patient data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If patient data is empty.</p> Source code in <code>periomod/inference/_baseinference.py</code> <pre><code>def prepare_inference(\n    self,\n    task: str,\n    patient_data: pd.DataFrame,\n    encoding: str,\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Prepares the data for inference.\n\n    Args:\n        task (str): The task name for which the model was trained.\n        patient_data (pd.DataFrame): The patient's data as a DataFrame.\n        encoding (str): Encoding type (\"one_hot\" or \"target\").\n        X_train (pd.DataFrame): Training features for target encoding.\n        y_train (pd.Series): Training target for target encoding.\n\n    Returns:\n        Tuple: Transformed patient data for prediction and patient data.\n\n    Raises:\n        ValueError: If patient data is empty.\n    \"\"\"\n    if patient_data.empty:\n        raise ValueError(\n            \"Patient data empty. Please submit data before running inference.\"\n        )\n    if self.verbose:\n        print(\"Patient Data Received for Inference:\\n\", patient_data)\n\n    engine = StaticProcessEngine()\n    dataloader = ProcessedDataLoader(task, encoding)\n    patient_data[self.group_col] = \"inference_patient\"\n    raw_data = engine.create_tooth_features(\n        data=patient_data, neighbors=True, patient_id=False\n    )\n\n    if encoding == \"target\":\n        raw_data = dataloader.encode_categorical_columns(data=raw_data)\n        resampler = Resampler(self.classification, encoding)\n        _, raw_data = resampler.apply_target_encoding(\n            X=X_train, X_val=raw_data, y=y_train\n        )\n\n        for key in raw_data.columns:\n            if key not in self.cat_vars and key in patient_data.columns:\n                raw_data[key] = patient_data[key].to_numpy()\n    else:\n        raw_data = self.create_predict_data(\n            raw_data=raw_data, patient_data=patient_data, encoding=encoding\n        )\n\n    predict_data = self.create_predict_data(\n        raw_data=raw_data, patient_data=patient_data, encoding=encoding\n    )\n    predict_data = dataloader.scale_numeric_columns(data=predict_data)\n\n    return predict_data, patient_data\n</code></pre>"},{"location":"reference/inference/basemodelinference/#periomod.inference.BaseModelInference.process_patient","title":"<code>process_patient(patient_id, train_df, patient_data, encoding, model_params, resampler)</code>","text":"<p>Processes a single patient's data for jackknife resampling.</p> <p>Parameters:</p> Name Type Description Default <code>patient_id</code> <code>int</code> <p>ID of the patient to exclude from training.</p> required <code>train_df</code> <code>DataFrame</code> <p>Full training dataset.</p> required <code>patient_data</code> <code>DataFrame</code> <p>The data for the patient(s) to predict on.</p> required <code>encoding</code> <code>str</code> <p>Encoding type used ('one_hot' or 'target').</p> required <code>model_params</code> <code>dict</code> <p>Parameters for the model initialization.</p> required <code>resampler</code> <code>Resampler</code> <p>Instance of the Resampler class for encoding.</p> required <p>Returns:</p> Name Type Description <code>predictions_df</code> <code>DataFrame</code> <p>DataFrame containing patient predictions and probabilities.</p> Source code in <code>periomod/inference/_baseinference.py</code> <pre><code>def process_patient(\n    self,\n    patient_id: int,\n    train_df: pd.DataFrame,\n    patient_data: pd.DataFrame,\n    encoding: str,\n    model_params: dict,\n    resampler: Resampler,\n) -&gt; pd.DataFrame:\n    \"\"\"Processes a single patient's data for jackknife resampling.\n\n    Args:\n        patient_id (int): ID of the patient to exclude from training.\n        train_df (pd.DataFrame): Full training dataset.\n        patient_data (pd.DataFrame): The data for the patient(s) to predict on.\n        encoding (str): Encoding type used ('one_hot' or 'target').\n        model_params (dict): Parameters for the model initialization.\n        resampler (Resampler): Instance of the Resampler class for encoding.\n\n    Returns:\n        predictions_df: DataFrame containing patient predictions and probabilities.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n    train_data = train_df[train_df[self.group_col] != patient_id]\n    X_train = train_data.drop(columns=[self.y])\n    y_train = train_data[self.y]\n\n    if encoding == \"target\":\n        X_train = X_train.drop(columns=[self.group_col], errors=\"ignore\")\n        X_train_enc, _ = resampler.apply_target_encoding(\n            X=X_train, X_val=None, y=y_train, jackknife=True\n        )\n    else:\n        X_train_enc = X_train.drop(columns=[self.group_col], errors=\"ignore\")\n\n    predictor = clone(self.model)\n    predictor.set_params(**model_params)\n    predictor.fit(X_train_enc, y_train)\n\n    if self.classification == \"binary\" and hasattr(predictor, \"best_threshold\"):\n        probs = get_probs(\n            model=predictor, classification=self.classification, X=patient_data\n        )\n        if probs is not None:\n            val_pred_classes = (probs &gt;= predictor.best_threshold).astype(int)\n        else:\n            val_pred_classes = predictor.predict(patient_data)\n    else:\n        val_pred_classes = predictor.predict(patient_data)\n        probs = predictor.predict_proba(patient_data)\n\n    predictions_df = pd.DataFrame(\n        probs,\n        columns=[str(cls) for cls in predictor.classes_],\n        index=patient_data.index,\n    )\n    return predictions_df.assign(\n        prediction=val_pred_classes,\n        iteration=patient_id,\n        data_index=patient_data.index,\n    )\n</code></pre>"},{"location":"reference/inference/modelinference/","title":"ModelInference","text":"<p>               Bases: <code>BaseModelInference</code></p> <p>Performs model inference and jackknife resampling on patient data.</p> <p>This class extends <code>BaseModelInference</code> with specific implementations for jackknife resampling, confidence interval computation, and visualization of prediction intervals for binary and multiclass classification models. It incorporates methods for generating predictions, preparing data for model inference, and applying jackknife inference, thus enabling robust model evaluation with confidence bounds.</p> Inherits <ul> <li><code>BaseModelInference</code>: Base class that implements prediction and     preprocessing methods.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>classification</code> <code>str</code> <p>Specifies the classification type ('binary' or 'multiclass').</p> required <code>model</code> <code>Any</code> <p>A trained model instance compatible with scikit-learn's <code>predict_proba</code> method.</p> required <code>verbose</code> <code>bool</code> <p>Enables detailed logging if set to True.</p> <code>True</code> <p>Methods:</p> Name Description <code>jackknife_resampling</code> <p>Re-trains the model on subsets of data, excluding each patient iteratively to compute jackknife estimates.</p> <code>jackknife_confidence_intervals</code> <p>Calculates confidence intervals based on jackknife results, returning bounds for each data index and class.</p> <code>plot_jackknife_intervals</code> <p>Visualizes jackknife confidence intervals for specific data points. Displays the estimated intervals and original predictions.</p> <code>jackknife_inference</code> <p>Runs the complete jackknife inference workflow, generating confidence intervals and an optional plot to illustrate interval bounds across specified data points.</p> Inherited Methods <ul> <li><code>predict</code>: Runs predictions on a batch of input data, returning   probabilities and predicted classes.</li> <li><code>create_predict_data</code>: Encodes and prepares raw patient data for   model prediction.</li> <li><code>prepare_inference</code>: Prepares data for inference by processing and   encoding patient data.</li> <li><code>patient_inference</code>: Generates prediction probabilities for   a specified patient's data.</li> <li><code>process_patient</code>: Excludes data for each patient iteratively and   retrains the model for jackknife resampling.</li> </ul> Example <pre><code>from periomod.base import Patient, patient_to_dataframe\nfrom periomod.inference import ModelInference\n\nmodel_inference = ModelInference(\n    classification=\"binary\", model=trained_model, verbose=True\n)\n\n# Define a patient instance\npatient = Patient()\npatient_df = patient_to_df(patient=patient)\n\n# Prepare data for inference\nprepared_data, patient_data = model_inference.prepare_inference(\n    task=\"pocketclosure\",\n    patient_data=patient_df,\n    encoding=\"one_hot\",\n    X_train=X_train,\n    y_train=y_train,\n)\n\n# Run inference on patient data\ninference_results = model_inference.patient_inference(\n    predict_data=prepared_data, patient_data=patient_data\n)\n\n# Perform jackknife inference with confidence interval plotting\njackknife_results, ci_plot = model_inference.jackknife_inference(\n    model=trained_model,\n    train_df=train_df,\n    patient_data=patient_df,\n    encoding=\"target\",\n    inference_results=inference_results,\n    alpha=0.05,\n    sample_fraction=0.8,\n    n_jobs=4,\n)\n</code></pre> Source code in <code>periomod/inference/_inference.py</code> <pre><code>class ModelInference(BaseModelInference):\n    \"\"\"Performs model inference and jackknife resampling on patient data.\n\n    This class extends `BaseModelInference` with specific implementations for\n    jackknife resampling, confidence interval computation, and visualization of\n    prediction intervals for binary and multiclass classification models. It\n    incorporates methods for generating predictions, preparing data for model\n    inference, and applying jackknife inference, thus enabling robust model\n    evaluation with confidence bounds.\n\n    Inherits:\n        - `BaseModelInference`: Base class that implements prediction and\n            preprocessing methods.\n\n    Args:\n        classification (str): Specifies the classification type ('binary' or\n            'multiclass').\n        model: A trained model instance compatible with scikit-learn's\n            `predict_proba` method.\n        verbose (bool): Enables detailed logging if set to True.\n\n    Methods:\n        jackknife_resampling: Re-trains the model on subsets of data,\n            excluding each patient iteratively to compute jackknife estimates.\n        jackknife_confidence_intervals: Calculates confidence intervals based on\n            jackknife results, returning bounds for each data index and class.\n        plot_jackknife_intervals: Visualizes jackknife confidence intervals for specific\n            data points. Displays the estimated intervals and original predictions.\n        jackknife_inference: Runs the complete jackknife inference\n            workflow, generating confidence intervals and an optional plot to\n            illustrate interval bounds across specified data points.\n\n    Inherited Methods:\n        - `predict`: Runs predictions on a batch of input data, returning\n          probabilities and predicted classes.\n        - `create_predict_data`: Encodes and prepares raw patient data for\n          model prediction.\n        - `prepare_inference`: Prepares data for inference by processing and\n          encoding patient data.\n        - `patient_inference`: Generates prediction probabilities for\n          a specified patient's data.\n        - `process_patient`: Excludes data for each patient iteratively and\n          retrains the model for jackknife resampling.\n\n    Example:\n        ```\n        from periomod.base import Patient, patient_to_dataframe\n        from periomod.inference import ModelInference\n\n        model_inference = ModelInference(\n            classification=\"binary\", model=trained_model, verbose=True\n        )\n\n        # Define a patient instance\n        patient = Patient()\n        patient_df = patient_to_df(patient=patient)\n\n        # Prepare data for inference\n        prepared_data, patient_data = model_inference.prepare_inference(\n            task=\"pocketclosure\",\n            patient_data=patient_df,\n            encoding=\"one_hot\",\n            X_train=X_train,\n            y_train=y_train,\n        )\n\n        # Run inference on patient data\n        inference_results = model_inference.patient_inference(\n            predict_data=prepared_data, patient_data=patient_data\n        )\n\n        # Perform jackknife inference with confidence interval plotting\n        jackknife_results, ci_plot = model_inference.jackknife_inference(\n            model=trained_model,\n            train_df=train_df,\n            patient_data=patient_df,\n            encoding=\"target\",\n            inference_results=inference_results,\n            alpha=0.05,\n            sample_fraction=0.8,\n            n_jobs=4,\n        )\n        ```\n    \"\"\"\n\n    def __init__(self, classification: str, model: Any, verbose: bool = True):\n        \"\"\"Initialize the ModelInference class with a trained model.\"\"\"\n        super().__init__(classification=classification, model=model, verbose=verbose)\n\n    def jackknife_resampling(\n        self,\n        train_df: pd.DataFrame,\n        patient_data: pd.DataFrame,\n        encoding: str,\n        model_params: dict,\n        sample_fraction: float = 1.0,\n        n_jobs: int = -1,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Perform jackknife resampling with retraining for each patient.\n\n        Args:\n            train_df (pd.DataFrame): Full training dataset.\n            patient_data (pd.DataFrame): The data for the patient(s) to predict on.\n            encoding (str): Encoding type used ('one_hot' or 'target').\n            model_params (dict): Parameters for the model initialization.\n            sample_fraction (float, optional): Proportion of patient IDs to use for\n                jackknife resampling. Defaults to 1.0.\n            n_jobs (int, optional): Number of jobs to run in parallel. Defaults to -1.\n\n        Returns:\n            DataFrame: DataFrame containing predictions for each iteration.\n        \"\"\"\n        resampler = Resampler(classification=self.classification, encoding=encoding)\n        patient_ids = train_df[self.group_col].unique()\n\n        if sample_fraction &lt; 1.0:\n            num_patients = int(len(patient_ids) * sample_fraction)\n            rng = default_rng()\n            patient_ids = rng.choice(patient_ids, num_patients, replace=False)\n\n        results = Parallel(n_jobs=n_jobs)(\n            delayed(self.process_patient)(\n                patient_id, train_df, patient_data, encoding, model_params, resampler\n            )\n            for patient_id in patient_ids\n        )\n\n        return pd.concat(results, ignore_index=True)\n\n    def jackknife_confidence_intervals(\n        self, jackknife_results: pd.DataFrame, alpha: float = 0.05\n    ) -&gt; Dict[int, Dict[str, Dict[str, float]]]:\n        \"\"\"Compute confidence intervals from jackknife results.\n\n        Args:\n            jackknife_results (pd.DataFrame): DataFrame with jackknife predictions.\n            alpha (float, optional): Significance level for confidence intervals.\n                Defaults to 0.05.\n\n        Returns:\n            Dict: Confidence intervals for each data index and class.\n        \"\"\"\n        ci_dict: Dict[int, Dict[str, Dict[str, float]]] = {}\n        probability_columns = [\n            col\n            for col in jackknife_results.columns\n            if col not in [\"prediction\", \"iteration\", \"data_index\"]\n        ]\n        grouped = jackknife_results.groupby(\"data_index\")\n\n        for data_idx, group in grouped:\n            class_probs = group[probability_columns]\n            mean_probs = class_probs.mean()\n            se_probs = class_probs.std(ddof=1) / np.sqrt(len(class_probs))\n            z_score = stats.norm.ppf(1 - alpha / 2)\n            ci_lower = mean_probs - z_score * se_probs\n            ci_upper = mean_probs + z_score * se_probs\n\n            ci_dict[data_idx] = {}\n            for class_name in class_probs.columns:\n                ci_dict[data_idx][class_name] = {\n                    \"mean\": mean_probs[class_name],\n                    \"lower\": ci_lower[class_name],\n                    \"upper\": ci_upper[class_name],\n                }\n        return ci_dict\n\n    def plot_jackknife_intervals(\n        self,\n        ci_dict: Dict[int, Dict[str, Dict[str, float]]],\n        data_indices: List[int],\n        original_preds: pd.DataFrame,\n    ) -&gt; plt.Figure:\n        \"\"\"Plot Jackknife confidence intervals.\n\n        Args:\n            ci_dict (Dict[int, Dict[str, Dict[str, float]]]): Confidence intervals for\n                each data index and class.\n            data_indices (List[int]): List of data indices to plot.\n            original_preds (pd.DataFrame): DataFrame containing original predictions and\n                probabilities for each data point.\n\n        Returns:\n            Figure: Figure object containing the plots, with one subplot per class.\n        \"\"\"\n        classes = list(next(iter(ci_dict.values())).keys())\n        num_classes = len(classes)\n        ncols = num_classes\n        nrows = 1\n\n        fig, axes = plt.subplots(\n            nrows=nrows, ncols=ncols, figsize=(6 * ncols, 6), sharey=True, dpi=300\n        )\n        axes = np.atleast_1d(axes).flatten()\n        predicted_classes = original_preds[\"prediction\"]\n\n        for idx, class_name in enumerate(classes):\n            ax = axes[idx]\n            means = []\n            lowers = []\n            uppers = []\n            data_indices_plot = []\n\n            for data_index in data_indices:\n                if predicted_classes.loc[data_index] == int(class_name):\n                    ci = ci_dict[data_index][class_name]\n                    mean = ci[\"mean\"]\n                    lower = ci[\"lower\"]\n                    upper = ci[\"upper\"]\n                    means.append(mean)\n                    lowers.append(lower)\n                    uppers.append(upper)\n                    data_indices_plot.append(data_index)\n\n            if means:\n                errors = [\n                    np.array(means) - np.array(lowers),\n                    np.array(uppers) - np.array(means),\n                ]\n\n                ax.errorbar(\n                    means,\n                    data_indices_plot,\n                    xerr=errors,\n                    fmt=\"o\",\n                    color=\"skyblue\",\n                    ecolor=\"black\",\n                    capsize=5,\n                    label=\"Jackknife CI\",\n                )\n\n                orig_probs = original_preds.loc[data_indices_plot, class_name]\n                ax.scatter(\n                    orig_probs,\n                    data_indices_plot,\n                    color=\"red\",\n                    marker=\"x\",\n                    s=100,\n                    label=\"Original Prediction\",\n                )\n\n            ax.set_xlabel(\"Predicted Probability\")\n            if idx == 0:\n                ax.set_ylabel(\"Data Point Index\")\n            ax.set_title(f\"Class {class_name}\")\n\n            x_min = min(lowers) if lowers else 0\n            x_max = max(uppers) if uppers else 1\n            x_range = x_max - x_min\n            if x_range == 0:\n                x_range = 0.1\n            ax.set_xlim([x_min - 0.1 * x_range, x_max + 0.1 * x_range])\n\n            ax.legend()\n\n        plt.tight_layout()\n        return fig\n\n    def jackknife_inference(\n        self,\n        model: Any,\n        train_df: pd.DataFrame,\n        patient_data: pd.DataFrame,\n        encoding: str,\n        inference_results: pd.DataFrame,\n        alpha: float = 0.05,\n        sample_fraction: float = 1.0,\n        n_jobs: int = -1,\n        max_plots: int = 12,\n    ) -&gt; Tuple[pd.DataFrame, plt.Figure]:\n        \"\"\"Run jackknife inference and generate confidence intervals and plots.\n\n        Args:\n            model (Any): Trained model instance.\n            train_df (pd.DataFrame): Training DataFrame.\n            patient_data (pd.DataFrame): Patient data to predict on.\n            encoding (str): Encoding type.\n            inference_results (pd.DataFrame): Original inference results.\n            alpha (float, optional): Significance level for confidence intervals.\n                Defaults to 0.05.\n            sample_fraction (float, optional): Fraction of patient IDs for jackknife.\n                Defaults to 1.0.\n            n_jobs (int, optional): Number of parallel jobs. Defaults to -1.\n            max_plots (int): Maximum number of plots for jackknife intervals.\n\n        Returns:\n            Tuple: Jackknife results and the plot.\n        \"\"\"\n        model_params = model.get_params()\n\n        if self.classification == \"multiclass\":\n            num_classes = len(np.unique(train_df[self.y]))\n            if \"num_class\" in model.get_params().keys():\n                model_params[\"num_class\"] = num_classes\n\n        jackknife_results = self.jackknife_resampling(\n            train_df=train_df,\n            patient_data=patient_data,\n            encoding=encoding,\n            model_params=model_params,\n            sample_fraction=sample_fraction,\n            n_jobs=n_jobs,\n        )\n        ci_dict = self.jackknife_confidence_intervals(\n            jackknife_results=jackknife_results, alpha=alpha\n        )\n        data_indices = patient_data.index[:max_plots]\n        ci_plot = self.plot_jackknife_intervals(\n            ci_dict=ci_dict, data_indices=data_indices, original_preds=inference_results\n        )\n\n        return jackknife_results, ci_plot\n</code></pre>"},{"location":"reference/inference/modelinference/#periomod.inference.ModelInference.__init__","title":"<code>__init__(classification, model, verbose=True)</code>","text":"<p>Initialize the ModelInference class with a trained model.</p> Source code in <code>periomod/inference/_inference.py</code> <pre><code>def __init__(self, classification: str, model: Any, verbose: bool = True):\n    \"\"\"Initialize the ModelInference class with a trained model.\"\"\"\n    super().__init__(classification=classification, model=model, verbose=verbose)\n</code></pre>"},{"location":"reference/inference/modelinference/#periomod.inference.ModelInference.jackknife_confidence_intervals","title":"<code>jackknife_confidence_intervals(jackknife_results, alpha=0.05)</code>","text":"<p>Compute confidence intervals from jackknife results.</p> <p>Parameters:</p> Name Type Description Default <code>jackknife_results</code> <code>DataFrame</code> <p>DataFrame with jackknife predictions.</p> required <code>alpha</code> <code>float</code> <p>Significance level for confidence intervals. Defaults to 0.05.</p> <code>0.05</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict[int, Dict[str, Dict[str, float]]]</code> <p>Confidence intervals for each data index and class.</p> Source code in <code>periomod/inference/_inference.py</code> <pre><code>def jackknife_confidence_intervals(\n    self, jackknife_results: pd.DataFrame, alpha: float = 0.05\n) -&gt; Dict[int, Dict[str, Dict[str, float]]]:\n    \"\"\"Compute confidence intervals from jackknife results.\n\n    Args:\n        jackknife_results (pd.DataFrame): DataFrame with jackknife predictions.\n        alpha (float, optional): Significance level for confidence intervals.\n            Defaults to 0.05.\n\n    Returns:\n        Dict: Confidence intervals for each data index and class.\n    \"\"\"\n    ci_dict: Dict[int, Dict[str, Dict[str, float]]] = {}\n    probability_columns = [\n        col\n        for col in jackknife_results.columns\n        if col not in [\"prediction\", \"iteration\", \"data_index\"]\n    ]\n    grouped = jackknife_results.groupby(\"data_index\")\n\n    for data_idx, group in grouped:\n        class_probs = group[probability_columns]\n        mean_probs = class_probs.mean()\n        se_probs = class_probs.std(ddof=1) / np.sqrt(len(class_probs))\n        z_score = stats.norm.ppf(1 - alpha / 2)\n        ci_lower = mean_probs - z_score * se_probs\n        ci_upper = mean_probs + z_score * se_probs\n\n        ci_dict[data_idx] = {}\n        for class_name in class_probs.columns:\n            ci_dict[data_idx][class_name] = {\n                \"mean\": mean_probs[class_name],\n                \"lower\": ci_lower[class_name],\n                \"upper\": ci_upper[class_name],\n            }\n    return ci_dict\n</code></pre>"},{"location":"reference/inference/modelinference/#periomod.inference.ModelInference.jackknife_inference","title":"<code>jackknife_inference(model, train_df, patient_data, encoding, inference_results, alpha=0.05, sample_fraction=1.0, n_jobs=-1, max_plots=12)</code>","text":"<p>Run jackknife inference and generate confidence intervals and plots.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Trained model instance.</p> required <code>train_df</code> <code>DataFrame</code> <p>Training DataFrame.</p> required <code>patient_data</code> <code>DataFrame</code> <p>Patient data to predict on.</p> required <code>encoding</code> <code>str</code> <p>Encoding type.</p> required <code>inference_results</code> <code>DataFrame</code> <p>Original inference results.</p> required <code>alpha</code> <code>float</code> <p>Significance level for confidence intervals. Defaults to 0.05.</p> <code>0.05</code> <code>sample_fraction</code> <code>float</code> <p>Fraction of patient IDs for jackknife. Defaults to 1.0.</p> <code>1.0</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs. Defaults to -1.</p> <code>-1</code> <code>max_plots</code> <code>int</code> <p>Maximum number of plots for jackknife intervals.</p> <code>12</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[DataFrame, Figure]</code> <p>Jackknife results and the plot.</p> Source code in <code>periomod/inference/_inference.py</code> <pre><code>def jackknife_inference(\n    self,\n    model: Any,\n    train_df: pd.DataFrame,\n    patient_data: pd.DataFrame,\n    encoding: str,\n    inference_results: pd.DataFrame,\n    alpha: float = 0.05,\n    sample_fraction: float = 1.0,\n    n_jobs: int = -1,\n    max_plots: int = 12,\n) -&gt; Tuple[pd.DataFrame, plt.Figure]:\n    \"\"\"Run jackknife inference and generate confidence intervals and plots.\n\n    Args:\n        model (Any): Trained model instance.\n        train_df (pd.DataFrame): Training DataFrame.\n        patient_data (pd.DataFrame): Patient data to predict on.\n        encoding (str): Encoding type.\n        inference_results (pd.DataFrame): Original inference results.\n        alpha (float, optional): Significance level for confidence intervals.\n            Defaults to 0.05.\n        sample_fraction (float, optional): Fraction of patient IDs for jackknife.\n            Defaults to 1.0.\n        n_jobs (int, optional): Number of parallel jobs. Defaults to -1.\n        max_plots (int): Maximum number of plots for jackknife intervals.\n\n    Returns:\n        Tuple: Jackknife results and the plot.\n    \"\"\"\n    model_params = model.get_params()\n\n    if self.classification == \"multiclass\":\n        num_classes = len(np.unique(train_df[self.y]))\n        if \"num_class\" in model.get_params().keys():\n            model_params[\"num_class\"] = num_classes\n\n    jackknife_results = self.jackknife_resampling(\n        train_df=train_df,\n        patient_data=patient_data,\n        encoding=encoding,\n        model_params=model_params,\n        sample_fraction=sample_fraction,\n        n_jobs=n_jobs,\n    )\n    ci_dict = self.jackknife_confidence_intervals(\n        jackknife_results=jackknife_results, alpha=alpha\n    )\n    data_indices = patient_data.index[:max_plots]\n    ci_plot = self.plot_jackknife_intervals(\n        ci_dict=ci_dict, data_indices=data_indices, original_preds=inference_results\n    )\n\n    return jackknife_results, ci_plot\n</code></pre>"},{"location":"reference/inference/modelinference/#periomod.inference.ModelInference.jackknife_resampling","title":"<code>jackknife_resampling(train_df, patient_data, encoding, model_params, sample_fraction=1.0, n_jobs=-1)</code>","text":"<p>Perform jackknife resampling with retraining for each patient.</p> <p>Parameters:</p> Name Type Description Default <code>train_df</code> <code>DataFrame</code> <p>Full training dataset.</p> required <code>patient_data</code> <code>DataFrame</code> <p>The data for the patient(s) to predict on.</p> required <code>encoding</code> <code>str</code> <p>Encoding type used ('one_hot' or 'target').</p> required <code>model_params</code> <code>dict</code> <p>Parameters for the model initialization.</p> required <code>sample_fraction</code> <code>float</code> <p>Proportion of patient IDs to use for jackknife resampling. Defaults to 1.0.</p> <code>1.0</code> <code>n_jobs</code> <code>int</code> <p>Number of jobs to run in parallel. Defaults to -1.</p> <code>-1</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame containing predictions for each iteration.</p> Source code in <code>periomod/inference/_inference.py</code> <pre><code>def jackknife_resampling(\n    self,\n    train_df: pd.DataFrame,\n    patient_data: pd.DataFrame,\n    encoding: str,\n    model_params: dict,\n    sample_fraction: float = 1.0,\n    n_jobs: int = -1,\n) -&gt; pd.DataFrame:\n    \"\"\"Perform jackknife resampling with retraining for each patient.\n\n    Args:\n        train_df (pd.DataFrame): Full training dataset.\n        patient_data (pd.DataFrame): The data for the patient(s) to predict on.\n        encoding (str): Encoding type used ('one_hot' or 'target').\n        model_params (dict): Parameters for the model initialization.\n        sample_fraction (float, optional): Proportion of patient IDs to use for\n            jackknife resampling. Defaults to 1.0.\n        n_jobs (int, optional): Number of jobs to run in parallel. Defaults to -1.\n\n    Returns:\n        DataFrame: DataFrame containing predictions for each iteration.\n    \"\"\"\n    resampler = Resampler(classification=self.classification, encoding=encoding)\n    patient_ids = train_df[self.group_col].unique()\n\n    if sample_fraction &lt; 1.0:\n        num_patients = int(len(patient_ids) * sample_fraction)\n        rng = default_rng()\n        patient_ids = rng.choice(patient_ids, num_patients, replace=False)\n\n    results = Parallel(n_jobs=n_jobs)(\n        delayed(self.process_patient)(\n            patient_id, train_df, patient_data, encoding, model_params, resampler\n        )\n        for patient_id in patient_ids\n    )\n\n    return pd.concat(results, ignore_index=True)\n</code></pre>"},{"location":"reference/inference/modelinference/#periomod.inference.ModelInference.plot_jackknife_intervals","title":"<code>plot_jackknife_intervals(ci_dict, data_indices, original_preds)</code>","text":"<p>Plot Jackknife confidence intervals.</p> <p>Parameters:</p> Name Type Description Default <code>ci_dict</code> <code>Dict[int, Dict[str, Dict[str, float]]]</code> <p>Confidence intervals for each data index and class.</p> required <code>data_indices</code> <code>List[int]</code> <p>List of data indices to plot.</p> required <code>original_preds</code> <code>DataFrame</code> <p>DataFrame containing original predictions and probabilities for each data point.</p> required <p>Returns:</p> Name Type Description <code>Figure</code> <code>Figure</code> <p>Figure object containing the plots, with one subplot per class.</p> Source code in <code>periomod/inference/_inference.py</code> <pre><code>def plot_jackknife_intervals(\n    self,\n    ci_dict: Dict[int, Dict[str, Dict[str, float]]],\n    data_indices: List[int],\n    original_preds: pd.DataFrame,\n) -&gt; plt.Figure:\n    \"\"\"Plot Jackknife confidence intervals.\n\n    Args:\n        ci_dict (Dict[int, Dict[str, Dict[str, float]]]): Confidence intervals for\n            each data index and class.\n        data_indices (List[int]): List of data indices to plot.\n        original_preds (pd.DataFrame): DataFrame containing original predictions and\n            probabilities for each data point.\n\n    Returns:\n        Figure: Figure object containing the plots, with one subplot per class.\n    \"\"\"\n    classes = list(next(iter(ci_dict.values())).keys())\n    num_classes = len(classes)\n    ncols = num_classes\n    nrows = 1\n\n    fig, axes = plt.subplots(\n        nrows=nrows, ncols=ncols, figsize=(6 * ncols, 6), sharey=True, dpi=300\n    )\n    axes = np.atleast_1d(axes).flatten()\n    predicted_classes = original_preds[\"prediction\"]\n\n    for idx, class_name in enumerate(classes):\n        ax = axes[idx]\n        means = []\n        lowers = []\n        uppers = []\n        data_indices_plot = []\n\n        for data_index in data_indices:\n            if predicted_classes.loc[data_index] == int(class_name):\n                ci = ci_dict[data_index][class_name]\n                mean = ci[\"mean\"]\n                lower = ci[\"lower\"]\n                upper = ci[\"upper\"]\n                means.append(mean)\n                lowers.append(lower)\n                uppers.append(upper)\n                data_indices_plot.append(data_index)\n\n        if means:\n            errors = [\n                np.array(means) - np.array(lowers),\n                np.array(uppers) - np.array(means),\n            ]\n\n            ax.errorbar(\n                means,\n                data_indices_plot,\n                xerr=errors,\n                fmt=\"o\",\n                color=\"skyblue\",\n                ecolor=\"black\",\n                capsize=5,\n                label=\"Jackknife CI\",\n            )\n\n            orig_probs = original_preds.loc[data_indices_plot, class_name]\n            ax.scatter(\n                orig_probs,\n                data_indices_plot,\n                color=\"red\",\n                marker=\"x\",\n                s=100,\n                label=\"Original Prediction\",\n            )\n\n        ax.set_xlabel(\"Predicted Probability\")\n        if idx == 0:\n            ax.set_ylabel(\"Data Point Index\")\n        ax.set_title(f\"Class {class_name}\")\n\n        x_min = min(lowers) if lowers else 0\n        x_max = max(uppers) if uppers else 1\n        x_range = x_max - x_min\n        if x_range == 0:\n            x_range = 0.1\n        ax.set_xlim([x_min - 0.1 * x_range, x_max + 0.1 * x_range])\n\n        ax.legend()\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"reference/learner/","title":"periomod.learner Overview","text":"<p>The <code>periomod.learner</code> module provides machine learning model definitions and configurations for training and evaluation.</p>"},{"location":"reference/learner/#available-components","title":"Available Components","text":"Component Description Model Class for initializing machine learning models with tuning options. s"},{"location":"reference/learner/model/","title":"Model","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configurable machine learning model class with HPO options.</p> <p>This class provides an interface for initializing machine learning models based on the specified learner type (e.g., random forest, logistic regression) and classification type (binary or multiclass). It supports optional hyperparameter optimization (HPO) configurations.</p> Inherits <ul> <li><code>BaseConfig</code>: Provides base configuration settings.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>learner</code> <code>str</code> <p>The machine learning algorithm to use. Options include: 'rf' (random forest), 'mlp' (multi-layer perceptron), 'xgb' (XGBoost), or 'lr' (logistic regression).</p> required <code>classification</code> <code>str</code> <p>Specifies the classification type. Can be either 'binary' or 'multiclass'.</p> required <code>hpo</code> <code>Optional[str]</code> <p>The hyperparameter optimization (HPO) method to use. Options are 'hebo' or 'rs'. Defaults to None, which requires specifying HPO in relevant methods.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>learner</code> <code>str</code> <p>The specified machine learning algorithm for the model. Options include 'rf', 'mlp', 'xgb', and 'lr'.</p> <code>classification</code> <code>str</code> <p>Defines the type of classification task. Options are 'binary' or 'multiclass'.</p> <code>hpo</code> <code>Optional[str]</code> <p>Hyperparameter optimization method for tuning, if specified. Options are 'hebo' or 'rs'.</p> <p>Methods:</p> Name Description <code>get</code> <p>Class method returning a model and hyperparameter search space or parameter grid.</p> <code>get_model</code> <p>Class method that returns only the instantiated model without HPO options.</p> Example <pre><code>model_instance = Model.get(learner=\"rf\", classification=\"binary\", hpo=\"hebo\")\ntrained_model = Model.get_model(learner=\"mlp\", classification=\"multiclass\")\n</code></pre> Source code in <code>periomod/learner/_learners.py</code> <pre><code>class Model(BaseConfig):\n    \"\"\"Configurable machine learning model class with HPO options.\n\n    This class provides an interface for initializing machine learning models\n    based on the specified learner type (e.g., random forest, logistic regression)\n    and classification type (binary or multiclass). It supports optional\n    hyperparameter optimization (HPO) configurations.\n\n    Inherits:\n        - `BaseConfig`: Provides base configuration settings.\n\n    Args:\n        learner (str): The machine learning algorithm to use. Options include:\n            'rf' (random forest), 'mlp' (multi-layer perceptron), 'xgb' (XGBoost),\n            or 'lr' (logistic regression).\n        classification (str): Specifies the classification type. Can be either\n            'binary' or 'multiclass'.\n        hpo (Optional[str]): The hyperparameter optimization (HPO) method to\n            use. Options are 'hebo' or 'rs'. Defaults to None, which requires\n            specifying HPO in relevant methods.\n\n    Attributes:\n        learner (str): The specified machine learning algorithm for the model.\n            Options include 'rf', 'mlp', 'xgb', and 'lr'.\n        classification (str): Defines the type of classification task.\n            Options are 'binary' or 'multiclass'.\n        hpo (Optional[str]): Hyperparameter optimization method for tuning, if\n            specified. Options are 'hebo' or 'rs'.\n\n    Methods:\n        get: Class method returning a model and hyperparameter search space\n            or parameter grid.\n        get_model: Class method that returns only the instantiated model\n            without HPO options.\n\n    Example:\n        ```\n        model_instance = Model.get(learner=\"rf\", classification=\"binary\", hpo=\"hebo\")\n        trained_model = Model.get_model(learner=\"mlp\", classification=\"multiclass\")\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        learner: str,\n        classification: str,\n        hpo: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the Model with the learner type and classification.\n\n        Args:\n            learner (str): The machine learning algorithm to use\n                (e.g., 'rf', 'mlp', 'xgb', 'lr').\n            classification (str): The type of classification ('binary' or 'multiclass').\n            hpo (str, optional): The hyperparameter optimization method to use.\n                Defaults to None.\n        \"\"\"\n        super().__init__()\n        self.classification = classification\n        self.hpo = hpo\n        self.learner = learner\n\n    def _get_model_instance(self):\n        \"\"\"Return the machine learning model based on the learner and classification.\n\n        Returns:\n            model instance.\n\n        Raises:\n            ValueError: If an invalid learner or classification is provided.\n        \"\"\"\n        if self.learner == \"rf\":\n            return RandomForestClassifier(random_state=self.learner_state)\n        elif self.learner == \"mlp\":\n            return MLPClassifier(random_state=self.learner_state)\n        elif self.learner == \"xgb\":\n            if self.classification == \"binary\":\n                return xgb.XGBClassifier(\n                    objective=self.xgb_obj_binary,\n                    eval_metric=self.xgb_loss_binary,\n                    random_state=self.learner_state,\n                )\n            elif self.classification == \"multiclass\":\n                return xgb.XGBClassifier(\n                    objective=self.xgb_obj_multi,\n                    eval_metric=self.xgb_loss_multi,\n                    random_state=self.learner_state,\n                )\n        elif self.learner == \"lr\":\n            if self.classification == \"binary\":\n                return LogisticRegression(\n                    solver=self.lr_solver_binary,\n                    random_state=self.learner_state,\n                )\n            elif self.classification == \"multiclass\":\n                return LogisticRegression(\n                    multi_class=self.lr_multi_loss,\n                    solver=self.lr_solver_multi,\n                    random_state=self.learner_state,\n                )\n        else:\n            raise ValueError(f\"Unsupported learner type: {self.learner}\")\n\n    @classmethod\n    def get(\n        cls, learner: str, classification: str, hpo: Optional[str] = None\n    ) -&gt; Union[Tuple, Tuple]:\n        \"\"\"Return the machine learning model and parameter grid or HEBO search space.\n\n        Args:\n            learner (str): The machine learning algorithm to use.\n            classification (str): The type of classification ('binary' or 'multiclass').\n            hpo (str): The hyperparameter optimization method ('hebo' or 'rs').\n\n        Returns:\n            Union: If hpo is 'rs', returns a tuple of (model, parameter grid).\n                If hpo is 'hebo', returns a tuple of (model, HEBO search space,\n                transformation function).\n\n        Raises:\n            ValueError: If hpo or \u00f6earner are unsupported types.\n        \"\"\"\n        instance = cls(learner, classification)\n        model = instance._get_model_instance()\n\n        if hpo is None:\n            raise ValueError(\"hpo must be provided as 'hebo' or 'rs'\")\n\n        if hpo == \"hebo\":\n            if learner == \"rf\":\n                return model, rf_search_space_hebo, get_rf_params_hebo\n            elif learner == \"mlp\":\n                return model, mlp_search_space_hebo, get_mlp_params_hebo\n            elif learner == \"xgb\":\n                return model, xgb_search_space_hebo, get_xgb_params_hebo\n            elif learner == \"lr\":\n                return model, lr_search_space_hebo_oh, get_lr_params_hebo_oh\n        elif hpo == \"rs\":\n            if learner == \"rf\":\n                return model, rf_param_grid\n            elif learner == \"mlp\":\n                return model, mlp_param_grid\n            elif learner == \"xgb\":\n                return model, xgb_param_grid\n            elif learner == \"lr\":\n                return model, lr_param_grid_oh\n\n        raise ValueError(f\"Unsupported hpo type '{hpo}' or learner type '{learner}'\")\n\n    @classmethod\n    def get_model(\n        cls, learner: str, classification: str\n    ) -&gt; Union[\n        RandomForestClassifier,\n        LogisticRegression,\n        MLPClassifier,\n        xgb.XGBClassifier,\n    ]:\n        \"\"\"Return only the machine learning model based on learner and classification.\n\n        Args:\n            learner (str): The machine learning algorithm to use.\n            classification (str): Type of classification ('binary' or 'multiclass').\n\n        Returns:\n            model: model instance (Union[sklearn estiamtor]).\n        \"\"\"\n        instance = cls(learner, classification)\n        return instance._get_model_instance()\n</code></pre>"},{"location":"reference/learner/model/#periomod.learner.Model.__init__","title":"<code>__init__(learner, classification, hpo=None)</code>","text":"<p>Initializes the Model with the learner type and classification.</p> <p>Parameters:</p> Name Type Description Default <code>learner</code> <code>str</code> <p>The machine learning algorithm to use (e.g., 'rf', 'mlp', 'xgb', 'lr').</p> required <code>classification</code> <code>str</code> <p>The type of classification ('binary' or 'multiclass').</p> required <code>hpo</code> <code>str</code> <p>The hyperparameter optimization method to use. Defaults to None.</p> <code>None</code> Source code in <code>periomod/learner/_learners.py</code> <pre><code>def __init__(\n    self,\n    learner: str,\n    classification: str,\n    hpo: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Initializes the Model with the learner type and classification.\n\n    Args:\n        learner (str): The machine learning algorithm to use\n            (e.g., 'rf', 'mlp', 'xgb', 'lr').\n        classification (str): The type of classification ('binary' or 'multiclass').\n        hpo (str, optional): The hyperparameter optimization method to use.\n            Defaults to None.\n    \"\"\"\n    super().__init__()\n    self.classification = classification\n    self.hpo = hpo\n    self.learner = learner\n</code></pre>"},{"location":"reference/learner/model/#periomod.learner.Model.get","title":"<code>get(learner, classification, hpo=None)</code>  <code>classmethod</code>","text":"<p>Return the machine learning model and parameter grid or HEBO search space.</p> <p>Parameters:</p> Name Type Description Default <code>learner</code> <code>str</code> <p>The machine learning algorithm to use.</p> required <code>classification</code> <code>str</code> <p>The type of classification ('binary' or 'multiclass').</p> required <code>hpo</code> <code>str</code> <p>The hyperparameter optimization method ('hebo' or 'rs').</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Union</code> <code>Union[Tuple, Tuple]</code> <p>If hpo is 'rs', returns a tuple of (model, parameter grid). If hpo is 'hebo', returns a tuple of (model, HEBO search space, transformation function).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If hpo or \u00f6earner are unsupported types.</p> Source code in <code>periomod/learner/_learners.py</code> <pre><code>@classmethod\ndef get(\n    cls, learner: str, classification: str, hpo: Optional[str] = None\n) -&gt; Union[Tuple, Tuple]:\n    \"\"\"Return the machine learning model and parameter grid or HEBO search space.\n\n    Args:\n        learner (str): The machine learning algorithm to use.\n        classification (str): The type of classification ('binary' or 'multiclass').\n        hpo (str): The hyperparameter optimization method ('hebo' or 'rs').\n\n    Returns:\n        Union: If hpo is 'rs', returns a tuple of (model, parameter grid).\n            If hpo is 'hebo', returns a tuple of (model, HEBO search space,\n            transformation function).\n\n    Raises:\n        ValueError: If hpo or \u00f6earner are unsupported types.\n    \"\"\"\n    instance = cls(learner, classification)\n    model = instance._get_model_instance()\n\n    if hpo is None:\n        raise ValueError(\"hpo must be provided as 'hebo' or 'rs'\")\n\n    if hpo == \"hebo\":\n        if learner == \"rf\":\n            return model, rf_search_space_hebo, get_rf_params_hebo\n        elif learner == \"mlp\":\n            return model, mlp_search_space_hebo, get_mlp_params_hebo\n        elif learner == \"xgb\":\n            return model, xgb_search_space_hebo, get_xgb_params_hebo\n        elif learner == \"lr\":\n            return model, lr_search_space_hebo_oh, get_lr_params_hebo_oh\n    elif hpo == \"rs\":\n        if learner == \"rf\":\n            return model, rf_param_grid\n        elif learner == \"mlp\":\n            return model, mlp_param_grid\n        elif learner == \"xgb\":\n            return model, xgb_param_grid\n        elif learner == \"lr\":\n            return model, lr_param_grid_oh\n\n    raise ValueError(f\"Unsupported hpo type '{hpo}' or learner type '{learner}'\")\n</code></pre>"},{"location":"reference/learner/model/#periomod.learner.Model.get_model","title":"<code>get_model(learner, classification)</code>  <code>classmethod</code>","text":"<p>Return only the machine learning model based on learner and classification.</p> <p>Parameters:</p> Name Type Description Default <code>learner</code> <code>str</code> <p>The machine learning algorithm to use.</p> required <code>classification</code> <code>str</code> <p>Type of classification ('binary' or 'multiclass').</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>Union[RandomForestClassifier, LogisticRegression, MLPClassifier, XGBClassifier]</code> <p>model instance (Union[sklearn estiamtor]).</p> Source code in <code>periomod/learner/_learners.py</code> <pre><code>@classmethod\ndef get_model(\n    cls, learner: str, classification: str\n) -&gt; Union[\n    RandomForestClassifier,\n    LogisticRegression,\n    MLPClassifier,\n    xgb.XGBClassifier,\n]:\n    \"\"\"Return only the machine learning model based on learner and classification.\n\n    Args:\n        learner (str): The machine learning algorithm to use.\n        classification (str): Type of classification ('binary' or 'multiclass').\n\n    Returns:\n        model: model instance (Union[sklearn estiamtor]).\n    \"\"\"\n    instance = cls(learner, classification)\n    return instance._get_model_instance()\n</code></pre>"},{"location":"reference/resampling/","title":"periomod.resampling Overview","text":"<p>The <code>periomod.resampling</code> module includes classes for handling data resampling and addressing class imbalance.</p>"},{"location":"reference/resampling/#available-components","title":"Available Components","text":"Component Description BaseResampler Base class defining structure for data resampling techniques. Resampler Class for implementing specific resampling methods."},{"location":"reference/resampling/baseresampler/","title":"BaseResampler","text":"<p>               Bases: <code>BaseConfig</code>, <code>ABC</code></p> <p>Abstract base class for implementing various resampling strategies.</p> <p>This class provides a foundational framework for resampling data and validating input parameters in classification tasks. It includes methods for applying upsampling, downsampling, and SMOTE, as well as handling target encoding, data validation, and configuring cross-validation folds.</p> Inherits <ul> <li><code>BaseConfig</code>: Provides configuration settings for data processing.</li> <li><code>ABC</code>: Specifies abstract methods for subclasses to implement.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>classification</code> <code>str</code> <p>Specifies the classification type ('binary' or 'multiclass').</p> required <code>encoding</code> <code>str</code> <p>Specifies the encoding type ('one_hot' or 'target').</p> required <p>Attributes:</p> Name Type Description <code>classification</code> <code>str</code> <p>The type of classification task ('binary' or 'multiclass').</p> <code>encoding</code> <code>str</code> <p>Encoding method for categorical features ('one_hot' or 'target').</p> <code>all_cat_vars</code> <code>list</code> <p>List of all categorical variables in the dataset that can be used in target encoding.</p> <p>Methods:</p> Name Description <code>apply_sampling</code> <p>Applies resampling techniques like SMOTE, upsampling, or downsampling to balance the dataset.</p> <code>apply_target_encoding</code> <p>Encodes categorical features based on the target variable for improved model performance.</p> <code>validate_dataframe</code> <p>Ensures the input DataFrame contains required columns and correct data types.</p> <code>validate_n_folds</code> <p>Verifies that the cross-validation fold count is a positive integer.</p> <code>validate_sampling_strategy</code> <p>Checks if the specified sampling strategy is valid.</p> Abstract Methods <ul> <li><code>split_train_test_df</code>: Splits the dataset into training and testing sets   based on group-based identifiers.</li> <li><code>split_x_y</code>: Divides the train and test DataFrames into feature and   target sets, with optional resampling.</li> <li><code>cv_folds</code>: Performs cross-validation with group-based constraints and   optional resampling for each fold.</li> </ul> Source code in <code>periomod/resampling/_baseresampler.py</code> <pre><code>class BaseResampler(BaseConfig, ABC):\n    \"\"\"Abstract base class for implementing various resampling strategies.\n\n    This class provides a foundational framework for resampling data and validating\n    input parameters in classification tasks. It includes methods for applying\n    upsampling, downsampling, and SMOTE, as well as handling target encoding,\n    data validation, and configuring cross-validation folds.\n\n    Inherits:\n        - `BaseConfig`: Provides configuration settings for data processing.\n        - `ABC`: Specifies abstract methods for subclasses to implement.\n\n    Args:\n        classification (str): Specifies the classification type ('binary' or\n            'multiclass').\n        encoding (str): Specifies the encoding type ('one_hot' or 'target').\n\n    Attributes:\n        classification (str): The type of classification task\n            ('binary' or 'multiclass').\n        encoding (str): Encoding method for categorical features\n            ('one_hot' or 'target').\n        all_cat_vars (list): List of all categorical variables in the dataset that\n            can be used in target encoding.\n\n    Methods:\n        apply_sampling: Applies resampling techniques like SMOTE, upsampling,\n            or downsampling to balance the dataset.\n        apply_target_encoding: Encodes categorical features based on the\n            target variable for improved model performance.\n        validate_dataframe: Ensures the input DataFrame contains required\n            columns and correct data types.\n        validate_n_folds: Verifies that the cross-validation fold count is a\n            positive integer.\n        validate_sampling_strategy: Checks if the specified sampling strategy\n            is valid.\n\n    Abstract Methods:\n        - `split_train_test_df`: Splits the dataset into training and testing sets\n          based on group-based identifiers.\n        - `split_x_y`: Divides the train and test DataFrames into feature and\n          target sets, with optional resampling.\n        - `cv_folds`: Performs cross-validation with group-based constraints and\n          optional resampling for each fold.\n    \"\"\"\n\n    def __init__(self, classification: str, encoding: str) -&gt; None:\n        \"\"\"Base class to provide validation and error handling for other classes.\"\"\"\n        super().__init__()\n        self.classification = classification\n        self.encoding = encoding\n\n    def apply_sampling(\n        self,\n        X: pd.DataFrame,\n        y: pd.Series,\n        sampling: str,\n        sampling_factor: Optional[float] = None,\n        random_state: Optional[int] = 0,\n    ) -&gt; Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"Applies resampling strategies to the dataset.\n\n        Methods such as SMOTE, upsampling, or downsampling are applied.\n\n        Args:\n            X (pd.DataFrame): The feature set of the dataset.\n            y (pd.Series): The target variable containing class labels.\n            sampling (str): The type of sampling to apply. Options are 'smote',\n                'upsampling', 'downsampling', or None.\n            sampling_factor (Optional[float]): The factor by which to upsample or\n                downsample.\n            random_state (Optional[int]): Random state for sampling. Defaults to 0.\n\n        Returns:\n            Tuple: Resampled feature set (X_resampled) and target labels (y_resampled).\n        \"\"\"\n        self.validate_sampling_strategy(sampling=sampling)\n        if sampling == \"smote\":\n            if self.classification == \"multiclass\":\n                smote_strategy = {\n                    1: int(sum(y == 1) * sampling_factor),\n                    2: int(sum(y == 2) * sampling_factor),\n                }\n            elif self.classification == \"binary\":\n                smote_strategy = {1: int(sum(y == 1) * sampling_factor)}\n            smote_sampler = SMOTE(\n                sampling_strategy=smote_strategy,\n                random_state=random_state,\n            )\n            return smote_sampler.fit_resample(X=X, y=y)\n\n        elif sampling == \"upsampling\":\n            if self.classification == \"multiclass\":\n                up_strategy = {\n                    1: int(sum(y == 1) * sampling_factor),\n                    2: int(sum(y == 2) * sampling_factor),\n                }\n            elif self.classification == \"binary\":\n                up_strategy = {0: int(sum(y == 0) * sampling_factor)}\n            up_sampler = RandomOverSampler(\n                sampling_strategy=up_strategy, random_state=random_state\n            )\n            return up_sampler.fit_resample(X=X, y=y)\n\n        elif sampling == \"downsampling\":\n            if self.classification in [\"binary\", \"multiclass\"]:\n                down_strategy = {1: int(sum(y == 1) // sampling_factor)}\n            down_sampler = RandomUnderSampler(\n                sampling_strategy=down_strategy, random_state=random_state\n            )\n            return down_sampler.fit_resample(X=X, y=y)\n\n        else:\n            return X, y\n\n    def apply_target_encoding(\n        self,\n        X: pd.DataFrame,\n        X_val: pd.DataFrame,\n        y: pd.Series,\n        jackknife: bool = False,\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"Applies target encoding to categorical variables.\n\n        Args:\n            X (pd.DataFrame): Training dataset.\n            X_val (pd.DataFrame): Validation dataset.\n            y (pd.Series): The target variable.\n            jackknife (bool, optional): If True, do not transform X_val.\n                Defaults to False.\n\n        Returns:\n            Tuple: X and X_val dataset with target encoded features.\n        \"\"\"\n        cat_vars = [col for col in self.all_cat_vars if col in X.columns]\n\n        if cat_vars:\n            encoder = TargetEncoder(\n                target_type=self.classification, random_state=self.target_state\n            )\n            X_encoded = encoder.fit_transform(X[cat_vars], y)\n\n            if not jackknife and X_val is not None:\n                X_val_encoded = encoder.transform(X_val[cat_vars])\n            else:\n                X_val_encoded = None\n\n            if self.classification == \"multiclass\":\n                n_classes = len(set(y))\n                encoded_cols = [\n                    f\"{col}_class_{i}\" for col in cat_vars for i in range(n_classes)\n                ]\n            else:\n                encoded_cols = cat_vars\n\n            X_encoded = pd.DataFrame(X_encoded, columns=encoded_cols, index=X.index)\n\n            if X_val_encoded is not None:\n                X_val_encoded = pd.DataFrame(\n                    X_val_encoded, columns=encoded_cols, index=X_val.index\n                )\n\n                missing_cols = set(X_encoded.columns) - set(X_val_encoded.columns)\n                for col in missing_cols:\n                    X_val_encoded[col] = 0\n\n                X_val_encoded = X_val_encoded[X_encoded.columns]\n\n            X = X.drop(columns=cat_vars)\n            if X_val is not None:\n                X_val = X_val.drop(columns=cat_vars)\n\n            X = pd.concat([X, X_encoded], axis=1)\n            if X_val_encoded is not None:\n                X_val = pd.concat([X_val, X_val_encoded], axis=1)\n                X_val = X_val[X.columns]\n\n        return X, X_val\n\n    @staticmethod\n    def validate_dataframe(df: pd.DataFrame, required_columns: list) -&gt; None:\n        \"\"\"Validate input is a pandas DataFrame and contains required columns.\n\n        Args:\n            df (pd.DataFrame): The DataFrame to validate.\n            required_columns (list): A list of column names that are required in\n                the DataFrame.\n\n        Raises:\n            TypeError: If the input is not a pandas DataFrame.\n            ValueError: If required columns are missing from the DataFrame.\n        \"\"\"\n        if not isinstance(df, pd.DataFrame):\n            raise TypeError(\n                f\"Expected input to be a pandas DataFrame, but got {type(df)}.\"\n            )\n\n        missing_columns = [col for col in required_columns if col not in df.columns]\n        if missing_columns:\n            raise ValueError(\n                f\"The following required columns are missing: \"\n                f\"{', '.join(missing_columns)}.\"\n            )\n\n    @staticmethod\n    def validate_n_folds(n_folds: Optional[int]) -&gt; None:\n        \"\"\"Validates the number of folds used in cross-validation.\n\n        Args:\n            n_folds (Optional[int]): The number of folds for cross-validation.\n\n        Raises:\n            ValueError: If the number of folds is not a positive integer.\n        \"\"\"\n        if not (isinstance(n_folds, int) and n_folds &gt; 0):\n            raise ValueError(\"'n_folds' must be a positive integer.\")\n\n    @staticmethod\n    def validate_sampling_strategy(sampling: str) -&gt; None:\n        \"\"\"Validates the sampling strategy.\n\n        Args:\n            sampling (str): The sampling strategy to validate.\n\n        Raises:\n            ValueError: If the sampling strategy is invalid.\n        \"\"\"\n        valid_strategies = [\"smote\", \"upsampling\", \"downsampling\", None]\n        if sampling not in valid_strategies:\n            raise ValueError(\n                f\"Invalid sampling strategy: {sampling}. Valid options are \"\n                f\"{valid_strategies}.\"\n            )\n\n    @abstractmethod\n    def split_train_test_df(\n        self,\n        df: pd.DataFrame,\n        seed: int,\n        test_size: float,\n    ):\n        \"\"\"Splits the dataset into train_df and test_df based on group identifiers.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame.\n            seed (int): Random seed for splitting.\n            test_size (float): Size of grouped train test split.\n        \"\"\"\n\n    @abstractmethod\n    def split_x_y(\n        self,\n        train_df: Optional[pd.DataFrame],\n        test_df: Optional[pd.DataFrame],\n        sampling: Union[str, None],\n        factor: Union[float, None],\n    ):\n        \"\"\"Splits the train and test DataFrames into feature and label sets.\n\n        Splits into (X_train, y_train, X_test, y_test).\n\n        Args:\n            train_df (Optional[pd.DataFrame]): The training DataFrame.\n            test_df (Optional[pd.DataFrame]): The testing DataFrame.\n            sampling (str, optional): Resampling method to apply (e.g.,\n                'upsampling', 'downsampling', 'smote').\n            factor (float, optional): Factor for sampling.\n        \"\"\"\n\n    @abstractmethod\n    def cv_folds(\n        self,\n        df: pd.DataFrame,\n        seed: int,\n        n_folds: int,\n        sampling: Union[str, None],\n        factor: Union[float, None],\n    ):\n        \"\"\"Performs cross-validation with group constraints.\n\n        Applies optional resampling strategies.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame.\n            seed (Optional[int], optional): Random seed for reproducibility.\n            n_folds (Optional[int], optional): Number of folds for cross-validation.\n            sampling (str, optional): Sampling method to apply (e.g.,\n                'upsampling', 'downsampling', 'smote').\n            factor (float, optional): Factor for resampling, applied to upsample,\n                downsample, or SMOTE.\n        \"\"\"\n</code></pre>"},{"location":"reference/resampling/baseresampler/#periomod.resampling.BaseResampler.__init__","title":"<code>__init__(classification, encoding)</code>","text":"<p>Base class to provide validation and error handling for other classes.</p> Source code in <code>periomod/resampling/_baseresampler.py</code> <pre><code>def __init__(self, classification: str, encoding: str) -&gt; None:\n    \"\"\"Base class to provide validation and error handling for other classes.\"\"\"\n    super().__init__()\n    self.classification = classification\n    self.encoding = encoding\n</code></pre>"},{"location":"reference/resampling/baseresampler/#periomod.resampling.BaseResampler.apply_sampling","title":"<code>apply_sampling(X, y, sampling, sampling_factor=None, random_state=0)</code>","text":"<p>Applies resampling strategies to the dataset.</p> <p>Methods such as SMOTE, upsampling, or downsampling are applied.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The feature set of the dataset.</p> required <code>y</code> <code>Series</code> <p>The target variable containing class labels.</p> required <code>sampling</code> <code>str</code> <p>The type of sampling to apply. Options are 'smote', 'upsampling', 'downsampling', or None.</p> required <code>sampling_factor</code> <code>Optional[float]</code> <p>The factor by which to upsample or downsample.</p> <code>None</code> <code>random_state</code> <code>Optional[int]</code> <p>Random state for sampling. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[DataFrame, Series]</code> <p>Resampled feature set (X_resampled) and target labels (y_resampled).</p> Source code in <code>periomod/resampling/_baseresampler.py</code> <pre><code>def apply_sampling(\n    self,\n    X: pd.DataFrame,\n    y: pd.Series,\n    sampling: str,\n    sampling_factor: Optional[float] = None,\n    random_state: Optional[int] = 0,\n) -&gt; Tuple[pd.DataFrame, pd.Series]:\n    \"\"\"Applies resampling strategies to the dataset.\n\n    Methods such as SMOTE, upsampling, or downsampling are applied.\n\n    Args:\n        X (pd.DataFrame): The feature set of the dataset.\n        y (pd.Series): The target variable containing class labels.\n        sampling (str): The type of sampling to apply. Options are 'smote',\n            'upsampling', 'downsampling', or None.\n        sampling_factor (Optional[float]): The factor by which to upsample or\n            downsample.\n        random_state (Optional[int]): Random state for sampling. Defaults to 0.\n\n    Returns:\n        Tuple: Resampled feature set (X_resampled) and target labels (y_resampled).\n    \"\"\"\n    self.validate_sampling_strategy(sampling=sampling)\n    if sampling == \"smote\":\n        if self.classification == \"multiclass\":\n            smote_strategy = {\n                1: int(sum(y == 1) * sampling_factor),\n                2: int(sum(y == 2) * sampling_factor),\n            }\n        elif self.classification == \"binary\":\n            smote_strategy = {1: int(sum(y == 1) * sampling_factor)}\n        smote_sampler = SMOTE(\n            sampling_strategy=smote_strategy,\n            random_state=random_state,\n        )\n        return smote_sampler.fit_resample(X=X, y=y)\n\n    elif sampling == \"upsampling\":\n        if self.classification == \"multiclass\":\n            up_strategy = {\n                1: int(sum(y == 1) * sampling_factor),\n                2: int(sum(y == 2) * sampling_factor),\n            }\n        elif self.classification == \"binary\":\n            up_strategy = {0: int(sum(y == 0) * sampling_factor)}\n        up_sampler = RandomOverSampler(\n            sampling_strategy=up_strategy, random_state=random_state\n        )\n        return up_sampler.fit_resample(X=X, y=y)\n\n    elif sampling == \"downsampling\":\n        if self.classification in [\"binary\", \"multiclass\"]:\n            down_strategy = {1: int(sum(y == 1) // sampling_factor)}\n        down_sampler = RandomUnderSampler(\n            sampling_strategy=down_strategy, random_state=random_state\n        )\n        return down_sampler.fit_resample(X=X, y=y)\n\n    else:\n        return X, y\n</code></pre>"},{"location":"reference/resampling/baseresampler/#periomod.resampling.BaseResampler.apply_target_encoding","title":"<code>apply_target_encoding(X, X_val, y, jackknife=False)</code>","text":"<p>Applies target encoding to categorical variables.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Training dataset.</p> required <code>X_val</code> <code>DataFrame</code> <p>Validation dataset.</p> required <code>y</code> <code>Series</code> <p>The target variable.</p> required <code>jackknife</code> <code>bool</code> <p>If True, do not transform X_val. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[DataFrame, DataFrame]</code> <p>X and X_val dataset with target encoded features.</p> Source code in <code>periomod/resampling/_baseresampler.py</code> <pre><code>def apply_target_encoding(\n    self,\n    X: pd.DataFrame,\n    X_val: pd.DataFrame,\n    y: pd.Series,\n    jackknife: bool = False,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Applies target encoding to categorical variables.\n\n    Args:\n        X (pd.DataFrame): Training dataset.\n        X_val (pd.DataFrame): Validation dataset.\n        y (pd.Series): The target variable.\n        jackknife (bool, optional): If True, do not transform X_val.\n            Defaults to False.\n\n    Returns:\n        Tuple: X and X_val dataset with target encoded features.\n    \"\"\"\n    cat_vars = [col for col in self.all_cat_vars if col in X.columns]\n\n    if cat_vars:\n        encoder = TargetEncoder(\n            target_type=self.classification, random_state=self.target_state\n        )\n        X_encoded = encoder.fit_transform(X[cat_vars], y)\n\n        if not jackknife and X_val is not None:\n            X_val_encoded = encoder.transform(X_val[cat_vars])\n        else:\n            X_val_encoded = None\n\n        if self.classification == \"multiclass\":\n            n_classes = len(set(y))\n            encoded_cols = [\n                f\"{col}_class_{i}\" for col in cat_vars for i in range(n_classes)\n            ]\n        else:\n            encoded_cols = cat_vars\n\n        X_encoded = pd.DataFrame(X_encoded, columns=encoded_cols, index=X.index)\n\n        if X_val_encoded is not None:\n            X_val_encoded = pd.DataFrame(\n                X_val_encoded, columns=encoded_cols, index=X_val.index\n            )\n\n            missing_cols = set(X_encoded.columns) - set(X_val_encoded.columns)\n            for col in missing_cols:\n                X_val_encoded[col] = 0\n\n            X_val_encoded = X_val_encoded[X_encoded.columns]\n\n        X = X.drop(columns=cat_vars)\n        if X_val is not None:\n            X_val = X_val.drop(columns=cat_vars)\n\n        X = pd.concat([X, X_encoded], axis=1)\n        if X_val_encoded is not None:\n            X_val = pd.concat([X_val, X_val_encoded], axis=1)\n            X_val = X_val[X.columns]\n\n    return X, X_val\n</code></pre>"},{"location":"reference/resampling/baseresampler/#periomod.resampling.BaseResampler.cv_folds","title":"<code>cv_folds(df, seed, n_folds, sampling, factor)</code>  <code>abstractmethod</code>","text":"<p>Performs cross-validation with group constraints.</p> <p>Applies optional resampling strategies.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility.</p> required <code>n_folds</code> <code>Optional[int]</code> <p>Number of folds for cross-validation.</p> required <code>sampling</code> <code>str</code> <p>Sampling method to apply (e.g., 'upsampling', 'downsampling', 'smote').</p> required <code>factor</code> <code>float</code> <p>Factor for resampling, applied to upsample, downsample, or SMOTE.</p> required Source code in <code>periomod/resampling/_baseresampler.py</code> <pre><code>@abstractmethod\ndef cv_folds(\n    self,\n    df: pd.DataFrame,\n    seed: int,\n    n_folds: int,\n    sampling: Union[str, None],\n    factor: Union[float, None],\n):\n    \"\"\"Performs cross-validation with group constraints.\n\n    Applies optional resampling strategies.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        seed (Optional[int], optional): Random seed for reproducibility.\n        n_folds (Optional[int], optional): Number of folds for cross-validation.\n        sampling (str, optional): Sampling method to apply (e.g.,\n            'upsampling', 'downsampling', 'smote').\n        factor (float, optional): Factor for resampling, applied to upsample,\n            downsample, or SMOTE.\n    \"\"\"\n</code></pre>"},{"location":"reference/resampling/baseresampler/#periomod.resampling.BaseResampler.split_train_test_df","title":"<code>split_train_test_df(df, seed, test_size)</code>  <code>abstractmethod</code>","text":"<p>Splits the dataset into train_df and test_df based on group identifiers.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>seed</code> <code>int</code> <p>Random seed for splitting.</p> required <code>test_size</code> <code>float</code> <p>Size of grouped train test split.</p> required Source code in <code>periomod/resampling/_baseresampler.py</code> <pre><code>@abstractmethod\ndef split_train_test_df(\n    self,\n    df: pd.DataFrame,\n    seed: int,\n    test_size: float,\n):\n    \"\"\"Splits the dataset into train_df and test_df based on group identifiers.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        seed (int): Random seed for splitting.\n        test_size (float): Size of grouped train test split.\n    \"\"\"\n</code></pre>"},{"location":"reference/resampling/baseresampler/#periomod.resampling.BaseResampler.split_x_y","title":"<code>split_x_y(train_df, test_df, sampling, factor)</code>  <code>abstractmethod</code>","text":"<p>Splits the train and test DataFrames into feature and label sets.</p> <p>Splits into (X_train, y_train, X_test, y_test).</p> <p>Parameters:</p> Name Type Description Default <code>train_df</code> <code>Optional[DataFrame]</code> <p>The training DataFrame.</p> required <code>test_df</code> <code>Optional[DataFrame]</code> <p>The testing DataFrame.</p> required <code>sampling</code> <code>str</code> <p>Resampling method to apply (e.g., 'upsampling', 'downsampling', 'smote').</p> required <code>factor</code> <code>float</code> <p>Factor for sampling.</p> required Source code in <code>periomod/resampling/_baseresampler.py</code> <pre><code>@abstractmethod\ndef split_x_y(\n    self,\n    train_df: Optional[pd.DataFrame],\n    test_df: Optional[pd.DataFrame],\n    sampling: Union[str, None],\n    factor: Union[float, None],\n):\n    \"\"\"Splits the train and test DataFrames into feature and label sets.\n\n    Splits into (X_train, y_train, X_test, y_test).\n\n    Args:\n        train_df (Optional[pd.DataFrame]): The training DataFrame.\n        test_df (Optional[pd.DataFrame]): The testing DataFrame.\n        sampling (str, optional): Resampling method to apply (e.g.,\n            'upsampling', 'downsampling', 'smote').\n        factor (float, optional): Factor for sampling.\n    \"\"\"\n</code></pre>"},{"location":"reference/resampling/baseresampler/#periomod.resampling.BaseResampler.validate_dataframe","title":"<code>validate_dataframe(df, required_columns)</code>  <code>staticmethod</code>","text":"<p>Validate input is a pandas DataFrame and contains required columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to validate.</p> required <code>required_columns</code> <code>list</code> <p>A list of column names that are required in the DataFrame.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a pandas DataFrame.</p> <code>ValueError</code> <p>If required columns are missing from the DataFrame.</p> Source code in <code>periomod/resampling/_baseresampler.py</code> <pre><code>@staticmethod\ndef validate_dataframe(df: pd.DataFrame, required_columns: list) -&gt; None:\n    \"\"\"Validate input is a pandas DataFrame and contains required columns.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to validate.\n        required_columns (list): A list of column names that are required in\n            the DataFrame.\n\n    Raises:\n        TypeError: If the input is not a pandas DataFrame.\n        ValueError: If required columns are missing from the DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\n            f\"Expected input to be a pandas DataFrame, but got {type(df)}.\"\n        )\n\n    missing_columns = [col for col in required_columns if col not in df.columns]\n    if missing_columns:\n        raise ValueError(\n            f\"The following required columns are missing: \"\n            f\"{', '.join(missing_columns)}.\"\n        )\n</code></pre>"},{"location":"reference/resampling/baseresampler/#periomod.resampling.BaseResampler.validate_n_folds","title":"<code>validate_n_folds(n_folds)</code>  <code>staticmethod</code>","text":"<p>Validates the number of folds used in cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>n_folds</code> <code>Optional[int]</code> <p>The number of folds for cross-validation.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of folds is not a positive integer.</p> Source code in <code>periomod/resampling/_baseresampler.py</code> <pre><code>@staticmethod\ndef validate_n_folds(n_folds: Optional[int]) -&gt; None:\n    \"\"\"Validates the number of folds used in cross-validation.\n\n    Args:\n        n_folds (Optional[int]): The number of folds for cross-validation.\n\n    Raises:\n        ValueError: If the number of folds is not a positive integer.\n    \"\"\"\n    if not (isinstance(n_folds, int) and n_folds &gt; 0):\n        raise ValueError(\"'n_folds' must be a positive integer.\")\n</code></pre>"},{"location":"reference/resampling/baseresampler/#periomod.resampling.BaseResampler.validate_sampling_strategy","title":"<code>validate_sampling_strategy(sampling)</code>  <code>staticmethod</code>","text":"<p>Validates the sampling strategy.</p> <p>Parameters:</p> Name Type Description Default <code>sampling</code> <code>str</code> <p>The sampling strategy to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the sampling strategy is invalid.</p> Source code in <code>periomod/resampling/_baseresampler.py</code> <pre><code>@staticmethod\ndef validate_sampling_strategy(sampling: str) -&gt; None:\n    \"\"\"Validates the sampling strategy.\n\n    Args:\n        sampling (str): The sampling strategy to validate.\n\n    Raises:\n        ValueError: If the sampling strategy is invalid.\n    \"\"\"\n    valid_strategies = [\"smote\", \"upsampling\", \"downsampling\", None]\n    if sampling not in valid_strategies:\n        raise ValueError(\n            f\"Invalid sampling strategy: {sampling}. Valid options are \"\n            f\"{valid_strategies}.\"\n        )\n</code></pre>"},{"location":"reference/resampling/resampler/","title":"Resampler","text":"<p>               Bases: <code>BaseResampler</code></p> <p>Resampler class for handling data resampling and train-test splitting.</p> <p>This class extends <code>BaseResampler</code> to provide additional functionality for resampling datasets using various strategies (e.g., SMOTE, upsampling, downsampling) and for handling train-test splitting and cross-validation with group constraints.</p> Inherits <ul> <li><code>BaseResampler</code>: Base class for resampling and validation methods.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>classification</code> <code>str</code> <p>Specifies the type of classification ('binary' or 'multiclass').</p> required <code>encoding</code> <code>str</code> <p>Specifies the encoding type ('one_hot' or 'target').</p> required <p>Attributes:</p> Name Type Description <code>classification</code> <code>str</code> <p>Type of classification task ('binary' or 'multiclass').</p> <code>encoding</code> <code>str</code> <p>Encoding strategy for categorical features ('one_hot' or 'target').</p> <code>all_cat_vars</code> <code>list</code> <p>List of categorical variables in the dataset, used in target encoding when applicable.</p> <p>Methods:</p> Name Description <code>split_train_test_df</code> <p>Splits the dataset into train and test sets based on group constraints, ensuring reproducibility.</p> <code>split_x_y</code> <p>Separates features and target labels in both train and test sets, applying optional sampling and encoding.</p> <code>cv_folds</code> <p>Performs group-based cross-validation, applying resampling strategies to balance training data where specified.</p> Inherited Methods <ul> <li><code>apply_sampling</code>: Applies specified sampling strategy to balance   the dataset, supporting SMOTE, upsampling, and downsampling.</li> <li><code>apply_target_encoding</code>: Applies target encoding to categorical   variables in the dataset.</li> <li><code>validate_dataframe</code>: Validates that input data meets requirements,   such as having specified columns.</li> <li><code>validate_n_folds</code>: Ensures the number of cross-validation folds   is a positive integer.</li> <li><code>validate_sampling_strategy</code>: Verifies the sampling strategy is   one of the allowed options.</li> </ul> Example <pre><code>from periomod.data import ProcessedDataLoader\nfrom periomod.resampling import Resampler\n\ndf = dataloader.load_data(path=\"data/processed/training_data.csv\")\n\nresampler = Resampler(classification=\"binary\", encoding=\"one_hot\")\ntrain_df, test_df = resampler.split_train_test_df(df=df, seed=42, test_size=0.3)\n\n# upsample minority class by a factor of 2.\nX_train, y_train, X_test, y_test = resampler.split_x_y(\n    train_df, test_df, sampling=\"upsampling\", factor=2\n)\n# performs grouped cross-validation with \"smote\" sampling on the training folds\nouter_splits, cv_folds_indices = resampler.cv_folds(\n    df, sampling=\"smote\", factor=2.0, seed=42, n_folds=5\n)\n</code></pre> Source code in <code>periomod/resampling/_resampler.py</code> <pre><code>class Resampler(BaseResampler):\n    \"\"\"Resampler class for handling data resampling and train-test splitting.\n\n    This class extends `BaseResampler` to provide additional functionality\n    for resampling datasets using various strategies (e.g., SMOTE, upsampling,\n    downsampling) and for handling train-test splitting and cross-validation\n    with group constraints.\n\n    Inherits:\n        - `BaseResampler`: Base class for resampling and validation methods.\n\n    Args:\n        classification (str): Specifies the type of classification ('binary'\n            or 'multiclass').\n        encoding (str): Specifies the encoding type ('one_hot' or 'target').\n\n    Attributes:\n        classification (str): Type of classification task ('binary' or 'multiclass').\n        encoding (str): Encoding strategy for categorical features\n            ('one_hot' or 'target').\n        all_cat_vars (list): List of categorical variables in the dataset, used in\n            target encoding when applicable.\n\n    Methods:\n        split_train_test_df: Splits the dataset into train and test sets based\n            on group constraints, ensuring reproducibility.\n        split_x_y: Separates features and target labels in both train and test sets,\n            applying optional sampling and encoding.\n        cv_folds: Performs group-based cross-validation, applying resampling\n            strategies to balance training data where specified.\n\n    Inherited Methods:\n        - `apply_sampling`: Applies specified sampling strategy to balance\n          the dataset, supporting SMOTE, upsampling, and downsampling.\n        - `apply_target_encoding`: Applies target encoding to categorical\n          variables in the dataset.\n        - `validate_dataframe`: Validates that input data meets requirements,\n          such as having specified columns.\n        - `validate_n_folds`: Ensures the number of cross-validation folds\n          is a positive integer.\n        - `validate_sampling_strategy`: Verifies the sampling strategy is\n          one of the allowed options.\n\n    Example:\n        ```\n        from periomod.data import ProcessedDataLoader\n        from periomod.resampling import Resampler\n\n        df = dataloader.load_data(path=\"data/processed/training_data.csv\")\n\n        resampler = Resampler(classification=\"binary\", encoding=\"one_hot\")\n        train_df, test_df = resampler.split_train_test_df(df=df, seed=42, test_size=0.3)\n\n        # upsample minority class by a factor of 2.\n        X_train, y_train, X_test, y_test = resampler.split_x_y(\n            train_df, test_df, sampling=\"upsampling\", factor=2\n        )\n        # performs grouped cross-validation with \"smote\" sampling on the training folds\n        outer_splits, cv_folds_indices = resampler.cv_folds(\n            df, sampling=\"smote\", factor=2.0, seed=42, n_folds=5\n        )\n        ```\n    \"\"\"\n\n    def __init__(self, classification: str, encoding: str) -&gt; None:\n        \"\"\"Initializes the Resampler class.\"\"\"\n        super().__init__(classification=classification, encoding=encoding)\n\n    def split_train_test_df(\n        self,\n        df: pd.DataFrame,\n        seed: int = 0,\n        test_size: Optional[float] = 0.2,\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"Splits the dataset into train_df and test_df based on group identifiers.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame.\n            seed (int): Random seed for splitting. Defaults to 0.\n            test_size (Optional[float]): Size of grouped train test split.\n                Defaults to 0.2.\n\n        Returns:\n            Tuple: Tuple containing the training and test DataFrames\n                (train_df, test_df).\n\n        Raises:\n            ValueError: If required columns are missing from the input DataFrame.\n        \"\"\"\n        self.validate_dataframe(df=df, required_columns=[self.y, self.group_col])\n\n        gss = GroupShuffleSplit(\n            n_splits=1,\n            test_size=test_size,\n            random_state=seed,\n        )\n        train_idx, test_idx = next(gss.split(df, groups=df[self.group_col]))\n\n        train_df = df.iloc[train_idx].reset_index(drop=True)\n        test_df = df.iloc[test_idx].reset_index(drop=True)\n\n        train_patient_ids = set(train_df[self.group_col])\n        test_patient_ids = set(test_df[self.group_col])\n        if not train_patient_ids.isdisjoint(test_patient_ids):\n            raise ValueError(\n                \"Overlapping group values between the train and test sets.\"\n            )\n\n        return train_df, test_df\n\n    def split_x_y(\n        self,\n        train_df: Optional[pd.DataFrame],\n        test_df: Optional[pd.DataFrame],\n        sampling: Union[str, None] = None,\n        factor: Union[float, None] = None,\n    ) -&gt; Tuple[\n        Optional[pd.DataFrame],\n        Optional[pd.Series],\n        Optional[pd.DataFrame],\n        Optional[pd.Series],\n    ]:\n        \"\"\"Splits the train and test DataFrames into feature and label sets.\n\n        Splits into (X_train, y_train, X_test, y_test).\n\n        Args:\n            train_df (Optional[pd.DataFrame]): The training DataFrame.\n            test_df (Optional[pd.DataFrame]): The testing DataFrame.\n            sampling (str, optional): Resampling method to apply (e.g.,\n                'upsampling', 'downsampling', 'smote'), defaults to None.\n            factor (float, optional): Factor for sampling, defaults to None.\n\n        Returns:\n            Tuple[Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.DataFrame],\n            Optional[pd.Series]]: Tuple containing feature and label sets (X_train,\n            y_train, X_test, y_test).\n        \"\"\"\n        X_train, y_train, X_test, y_test = None, None, None, None\n\n        if train_df is not None:\n            X_train = train_df.drop(columns=[self.y])\n            y_train = train_df[self.y]\n\n        if test_df is not None:\n            X_test = test_df.drop(columns=[self.y])\n            y_test = test_df[self.y]\n\n        if (\n            self.encoding == \"target\"\n            and X_train is not None\n            and X_test is not None\n            and y_train is not None\n        ):\n            X_train, X_test = self.apply_target_encoding(\n                X=X_train, X_val=X_test, y=y_train\n            )\n\n        if sampling is not None and X_train is not None and y_train is not None:\n            X_train, y_train = self.apply_sampling(\n                X=X_train, y=y_train, sampling=sampling, sampling_factor=factor\n            )\n\n        if X_train is not None and self.group_col in X_train:\n            X_train = X_train.drop(columns=[self.group_col])\n\n        if X_test is not None and self.group_col in X_test:\n            X_test = X_test.drop(columns=[self.group_col])\n\n        return X_train, y_train, X_test, y_test\n\n    def cv_folds(\n        self,\n        df: pd.DataFrame,\n        seed: Optional[int] = 0,\n        n_folds: Optional[int] = 10,\n        sampling: Union[str, None] = None,\n        factor: Union[float, None] = None,\n    ) -&gt; Tuple[list, list]:\n        \"\"\"Performs cross-validation with group constraints.\n\n        Applies optional resampling strategies.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame.\n            seed (Optional[int]): Random seed for reproducibility. Defaults to 0.\n            n_folds (Optional[[int]): Number of folds for cross-validation.\n                Defaults to 10.\n            sampling (str, optional): Sampling method to apply (e.g.,\n                'upsampling', 'downsampling', 'smote').\n            factor (float, optional): Factor for resampling, applied to upsample,\n                downsample, or SMOTE.\n\n\n        Returns:\n            Tuple: Tuple containing outer splits and cross-validation fold indices.\n\n        Raises:\n            ValueError: If required columns are missing or folds are inconsistent.\n        \"\"\"\n        np.random.default_rng(seed=seed)\n\n        self.validate_dataframe(df=df, required_columns=[self.y, self.group_col])\n        self.validate_n_folds(n_folds=n_folds)\n        train_df, _ = self.split_train_test_df(df=df)\n        gkf = GroupKFold(n_splits=n_folds)\n\n        cv_folds_indices = []\n        outer_splits = []\n        original_validation_data = []\n\n        for train_idx, test_idx in gkf.split(train_df, groups=train_df[self.group_col]):\n            X_train_fold = train_df.iloc[train_idx].drop([self.y], axis=1)\n            y_train_fold = train_df.iloc[train_idx][self.y]\n            X_test_fold = train_df.iloc[test_idx].drop([self.y], axis=1)\n            y_test_fold = train_df.iloc[test_idx][self.y]\n\n            original_validation_data.append(\n                train_df.iloc[test_idx].drop([self.y], axis=1).reset_index(drop=True)\n            )\n\n            if sampling is not None:\n                X_train_fold, y_train_fold = self.apply_sampling(\n                    X=X_train_fold,\n                    y=y_train_fold,\n                    sampling=sampling,\n                    sampling_factor=factor,\n                    random_state=seed,\n                )\n\n            cv_folds_indices.append((train_idx, test_idx))\n            outer_splits.append((\n                (X_train_fold, y_train_fold),\n                (X_test_fold, y_test_fold),\n            ))\n\n        for original_test_data, (_, (X_test_fold, _)) in zip(\n            original_validation_data, outer_splits, strict=False\n        ):\n            if not original_test_data.equals(X_test_fold.reset_index(drop=True)):\n                raise ValueError(\n                    \"Validation folds' data not consistent after applying sampling \"\n                    \"strategies.\"\n                )\n        if self.encoding == \"target\":\n            outer_splits_t = []\n\n            for (X_t, y_t), (X_val, y_val) in outer_splits:\n                X_t, X_val = self.apply_target_encoding(X=X_t, X_val=X_val, y=y_t)\n                if sampling == \"smote\":\n                    X_t, y_t = self.apply_sampling(\n                        X=X_t,\n                        y=y_t,\n                        sampling=sampling,\n                        sampling_factor=factor,\n                        random_state=seed,\n                    )\n\n                outer_splits_t.append(((X_t, y_t), (X_val, y_val)))\n            outer_splits = outer_splits_t\n\n        return outer_splits, cv_folds_indices\n</code></pre>"},{"location":"reference/resampling/resampler/#periomod.resampling.Resampler.__init__","title":"<code>__init__(classification, encoding)</code>","text":"<p>Initializes the Resampler class.</p> Source code in <code>periomod/resampling/_resampler.py</code> <pre><code>def __init__(self, classification: str, encoding: str) -&gt; None:\n    \"\"\"Initializes the Resampler class.\"\"\"\n    super().__init__(classification=classification, encoding=encoding)\n</code></pre>"},{"location":"reference/resampling/resampler/#periomod.resampling.Resampler.cv_folds","title":"<code>cv_folds(df, seed=0, n_folds=10, sampling=None, factor=None)</code>","text":"<p>Performs cross-validation with group constraints.</p> <p>Applies optional resampling strategies.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility. Defaults to 0.</p> <code>0</code> <code>n_folds</code> <code>Optional[[int]</code> <p>Number of folds for cross-validation. Defaults to 10.</p> <code>10</code> <code>sampling</code> <code>str</code> <p>Sampling method to apply (e.g., 'upsampling', 'downsampling', 'smote').</p> <code>None</code> <code>factor</code> <code>float</code> <p>Factor for resampling, applied to upsample, downsample, or SMOTE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[list, list]</code> <p>Tuple containing outer splits and cross-validation fold indices.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are missing or folds are inconsistent.</p> Source code in <code>periomod/resampling/_resampler.py</code> <pre><code>def cv_folds(\n    self,\n    df: pd.DataFrame,\n    seed: Optional[int] = 0,\n    n_folds: Optional[int] = 10,\n    sampling: Union[str, None] = None,\n    factor: Union[float, None] = None,\n) -&gt; Tuple[list, list]:\n    \"\"\"Performs cross-validation with group constraints.\n\n    Applies optional resampling strategies.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        seed (Optional[int]): Random seed for reproducibility. Defaults to 0.\n        n_folds (Optional[[int]): Number of folds for cross-validation.\n            Defaults to 10.\n        sampling (str, optional): Sampling method to apply (e.g.,\n            'upsampling', 'downsampling', 'smote').\n        factor (float, optional): Factor for resampling, applied to upsample,\n            downsample, or SMOTE.\n\n\n    Returns:\n        Tuple: Tuple containing outer splits and cross-validation fold indices.\n\n    Raises:\n        ValueError: If required columns are missing or folds are inconsistent.\n    \"\"\"\n    np.random.default_rng(seed=seed)\n\n    self.validate_dataframe(df=df, required_columns=[self.y, self.group_col])\n    self.validate_n_folds(n_folds=n_folds)\n    train_df, _ = self.split_train_test_df(df=df)\n    gkf = GroupKFold(n_splits=n_folds)\n\n    cv_folds_indices = []\n    outer_splits = []\n    original_validation_data = []\n\n    for train_idx, test_idx in gkf.split(train_df, groups=train_df[self.group_col]):\n        X_train_fold = train_df.iloc[train_idx].drop([self.y], axis=1)\n        y_train_fold = train_df.iloc[train_idx][self.y]\n        X_test_fold = train_df.iloc[test_idx].drop([self.y], axis=1)\n        y_test_fold = train_df.iloc[test_idx][self.y]\n\n        original_validation_data.append(\n            train_df.iloc[test_idx].drop([self.y], axis=1).reset_index(drop=True)\n        )\n\n        if sampling is not None:\n            X_train_fold, y_train_fold = self.apply_sampling(\n                X=X_train_fold,\n                y=y_train_fold,\n                sampling=sampling,\n                sampling_factor=factor,\n                random_state=seed,\n            )\n\n        cv_folds_indices.append((train_idx, test_idx))\n        outer_splits.append((\n            (X_train_fold, y_train_fold),\n            (X_test_fold, y_test_fold),\n        ))\n\n    for original_test_data, (_, (X_test_fold, _)) in zip(\n        original_validation_data, outer_splits, strict=False\n    ):\n        if not original_test_data.equals(X_test_fold.reset_index(drop=True)):\n            raise ValueError(\n                \"Validation folds' data not consistent after applying sampling \"\n                \"strategies.\"\n            )\n    if self.encoding == \"target\":\n        outer_splits_t = []\n\n        for (X_t, y_t), (X_val, y_val) in outer_splits:\n            X_t, X_val = self.apply_target_encoding(X=X_t, X_val=X_val, y=y_t)\n            if sampling == \"smote\":\n                X_t, y_t = self.apply_sampling(\n                    X=X_t,\n                    y=y_t,\n                    sampling=sampling,\n                    sampling_factor=factor,\n                    random_state=seed,\n                )\n\n            outer_splits_t.append(((X_t, y_t), (X_val, y_val)))\n        outer_splits = outer_splits_t\n\n    return outer_splits, cv_folds_indices\n</code></pre>"},{"location":"reference/resampling/resampler/#periomod.resampling.Resampler.split_train_test_df","title":"<code>split_train_test_df(df, seed=0, test_size=0.2)</code>","text":"<p>Splits the dataset into train_df and test_df based on group identifiers.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>seed</code> <code>int</code> <p>Random seed for splitting. Defaults to 0.</p> <code>0</code> <code>test_size</code> <code>Optional[float]</code> <p>Size of grouped train test split. Defaults to 0.2.</p> <code>0.2</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[DataFrame, DataFrame]</code> <p>Tuple containing the training and test DataFrames (train_df, test_df).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are missing from the input DataFrame.</p> Source code in <code>periomod/resampling/_resampler.py</code> <pre><code>def split_train_test_df(\n    self,\n    df: pd.DataFrame,\n    seed: int = 0,\n    test_size: Optional[float] = 0.2,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Splits the dataset into train_df and test_df based on group identifiers.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        seed (int): Random seed for splitting. Defaults to 0.\n        test_size (Optional[float]): Size of grouped train test split.\n            Defaults to 0.2.\n\n    Returns:\n        Tuple: Tuple containing the training and test DataFrames\n            (train_df, test_df).\n\n    Raises:\n        ValueError: If required columns are missing from the input DataFrame.\n    \"\"\"\n    self.validate_dataframe(df=df, required_columns=[self.y, self.group_col])\n\n    gss = GroupShuffleSplit(\n        n_splits=1,\n        test_size=test_size,\n        random_state=seed,\n    )\n    train_idx, test_idx = next(gss.split(df, groups=df[self.group_col]))\n\n    train_df = df.iloc[train_idx].reset_index(drop=True)\n    test_df = df.iloc[test_idx].reset_index(drop=True)\n\n    train_patient_ids = set(train_df[self.group_col])\n    test_patient_ids = set(test_df[self.group_col])\n    if not train_patient_ids.isdisjoint(test_patient_ids):\n        raise ValueError(\n            \"Overlapping group values between the train and test sets.\"\n        )\n\n    return train_df, test_df\n</code></pre>"},{"location":"reference/resampling/resampler/#periomod.resampling.Resampler.split_x_y","title":"<code>split_x_y(train_df, test_df, sampling=None, factor=None)</code>","text":"<p>Splits the train and test DataFrames into feature and label sets.</p> <p>Splits into (X_train, y_train, X_test, y_test).</p> <p>Parameters:</p> Name Type Description Default <code>train_df</code> <code>Optional[DataFrame]</code> <p>The training DataFrame.</p> required <code>test_df</code> <code>Optional[DataFrame]</code> <p>The testing DataFrame.</p> required <code>sampling</code> <code>str</code> <p>Resampling method to apply (e.g., 'upsampling', 'downsampling', 'smote'), defaults to None.</p> <code>None</code> <code>factor</code> <code>float</code> <p>Factor for sampling, defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>Tuple[Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.DataFrame],</p> <code>Optional[Series]</code> <p>Optional[pd.Series]]: Tuple containing feature and label sets (X_train,</p> <code>Optional[DataFrame]</code> <p>y_train, X_test, y_test).</p> Source code in <code>periomod/resampling/_resampler.py</code> <pre><code>def split_x_y(\n    self,\n    train_df: Optional[pd.DataFrame],\n    test_df: Optional[pd.DataFrame],\n    sampling: Union[str, None] = None,\n    factor: Union[float, None] = None,\n) -&gt; Tuple[\n    Optional[pd.DataFrame],\n    Optional[pd.Series],\n    Optional[pd.DataFrame],\n    Optional[pd.Series],\n]:\n    \"\"\"Splits the train and test DataFrames into feature and label sets.\n\n    Splits into (X_train, y_train, X_test, y_test).\n\n    Args:\n        train_df (Optional[pd.DataFrame]): The training DataFrame.\n        test_df (Optional[pd.DataFrame]): The testing DataFrame.\n        sampling (str, optional): Resampling method to apply (e.g.,\n            'upsampling', 'downsampling', 'smote'), defaults to None.\n        factor (float, optional): Factor for sampling, defaults to None.\n\n    Returns:\n        Tuple[Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.DataFrame],\n        Optional[pd.Series]]: Tuple containing feature and label sets (X_train,\n        y_train, X_test, y_test).\n    \"\"\"\n    X_train, y_train, X_test, y_test = None, None, None, None\n\n    if train_df is not None:\n        X_train = train_df.drop(columns=[self.y])\n        y_train = train_df[self.y]\n\n    if test_df is not None:\n        X_test = test_df.drop(columns=[self.y])\n        y_test = test_df[self.y]\n\n    if (\n        self.encoding == \"target\"\n        and X_train is not None\n        and X_test is not None\n        and y_train is not None\n    ):\n        X_train, X_test = self.apply_target_encoding(\n            X=X_train, X_val=X_test, y=y_train\n        )\n\n    if sampling is not None and X_train is not None and y_train is not None:\n        X_train, y_train = self.apply_sampling(\n            X=X_train, y=y_train, sampling=sampling, sampling_factor=factor\n        )\n\n    if X_train is not None and self.group_col in X_train:\n        X_train = X_train.drop(columns=[self.group_col])\n\n    if X_test is not None and self.group_col in X_test:\n        X_test = X_test.drop(columns=[self.group_col])\n\n    return X_train, y_train, X_test, y_test\n</code></pre>"},{"location":"reference/training/","title":"periomod.training Overview","text":"<p>The <code>periomod.training</code> module covers model training functions and includes methods for computing metrics and probability calculations.</p>"},{"location":"reference/training/#available-components","title":"Available Components","text":"Component Description brier_loss_multi Function for calculating Brier loss in multiclass setups. get_probs Function for probability prediction and output. final_metrics Function for computing final evaluation metrics. BaseTrainer Base class for training models with standard configurations. Trainer Training class with full pipeline for model fitting and tuning."},{"location":"reference/training/basetrainer/","title":"BaseTrainer","text":"<p>               Bases: <code>BaseValidator</code>, <code>ABC</code></p> <p>Abstract base class for training machine learning models.</p> <p>This class provides foundational methods for training and evaluating machine learning models, including MLP models with early stopping, and optimizing decision thresholds. It supports binary and multiclass classification and allows for various evaluation metrics, threshold tuning, and cross-validation procedures.</p> Inherits <ul> <li><code>BaseValidator</code>: Validates instance level variables.</li> <li><code>ABC</code>: Specifies abstract methods for subclasses to implement.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>classification</code> <code>str</code> <p>Specifies the type of classification ('binary' or 'multiclass').</p> required <code>criterion</code> <code>str</code> <p>Defines the performance criterion to optimize (e.g., 'f1' or 'brier_score').</p> required <code>tuning</code> <code>Optional[str]</code> <p>Specifies the tuning method ('holdout' or 'cv') or None.</p> required <code>hpo</code> <code>Optional[str]</code> <p>Specifies the hyperparameter optimization method.</p> required <code>mlp_training</code> <code>Optional[bool]</code> <p>Flag to indicate if a separate MLP training procedure with early stopping is to be used.</p> required <code>threshold_tuning</code> <code>Optional[bool]</code> <p>Determines if threshold tuning is performed for binary classification when the criterion is \"f1\".</p> required <p>Attributes:</p> Name Type Description <code>classification</code> <code>str</code> <p>Type of classification ('binary' or 'multiclass').</p> <code>criterion</code> <code>str</code> <p>Performance criterion to optimize ('f1', 'brier_score' or 'macro_f1').</p> <code>tuning</code> <code>Optional[str]</code> <p>Tuning method ('holdout' or 'cv') or None.</p> <code>hpo</code> <code>Optional[str]</code> <p>Hyperparameter optimization method if specified.</p> <code>mlp_training</code> <code>Optional[bool]</code> <p>Indicates if MLP training with early stopping is applied.</p> <code>threshold_tuning</code> <code>Optional[bool]</code> <p>Specifies if threshold tuning is performed for binary classification when the criterion is 'f1'.</p> <p>Methods:</p> Name Description <code>evaluate</code> <p>Determines model performance based on the specified classification criterion.</p> <code>optimize_threshold</code> <p>Utilizes cross-validation to optimize the decision threshold by aggregating probability predictions.</p> <code>evaluate_cv</code> <p>Evaluates a model on a training-validation fold based on the specified criterion, supporting cross-validation.</p> Abstract Methods <ul> <li><code>train</code>: Trains the model with standard or custom logic depending   on the specified learner type.</li> <li><code>train_mlp</code>: Trains an MLP model with early stopping and additional   evaluation logic if required.</li> <li><code>train_final_model</code>: Trains the final model on the entire dataset,   applying resampling, parallel processing, and specified sampling   methods.</li> </ul> Source code in <code>periomod/training/_basetrainer.py</code> <pre><code>class BaseTrainer(BaseValidator, ABC):\n    \"\"\"Abstract base class for training machine learning models.\n\n    This class provides foundational methods for training and evaluating\n    machine learning models, including MLP models with early stopping,\n    and optimizing decision thresholds. It supports binary and multiclass\n    classification and allows for various evaluation metrics, threshold\n    tuning, and cross-validation procedures.\n\n    Inherits:\n        - `BaseValidator`: Validates instance level variables.\n        - `ABC`: Specifies abstract methods for subclasses to implement.\n\n    Args:\n        classification (str): Specifies the type of classification ('binary'\n            or 'multiclass').\n        criterion (str): Defines the performance criterion to optimize (e.g.,\n            'f1' or 'brier_score').\n        tuning (Optional[str]): Specifies the tuning method ('holdout' or\n            'cv') or None.\n        hpo (Optional[str]): Specifies the hyperparameter optimization method.\n        mlp_training (Optional[bool]): Flag to indicate if a separate MLP training\n            procedure with early stopping is to be used.\n        threshold_tuning (Optional[bool]): Determines if threshold tuning is performed\n            for binary classification when the criterion is \"f1\".\n\n    Attributes:\n        classification (str): Type of classification ('binary' or 'multiclass').\n        criterion (str): Performance criterion to optimize\n            ('f1', 'brier_score' or 'macro_f1').\n        tuning (Optional[str]): Tuning method ('holdout' or 'cv') or None.\n        hpo (Optional[str]): Hyperparameter optimization method if specified.\n        mlp_training (Optional[bool]): Indicates if MLP training with early stopping is\n            applied.\n        threshold_tuning (Optional[bool]): Specifies if threshold tuning is performed\n            for binary classification when the criterion is 'f1'.\n\n    Methods:\n        evaluate: Determines model performance based on the specified\n            classification criterion.\n        optimize_threshold: Utilizes cross-validation to optimize the\n            decision threshold by aggregating probability predictions.\n        evaluate_cv: Evaluates a model on a training-validation fold\n            based on the specified criterion, supporting cross-validation.\n\n    Abstract Methods:\n        - `train`: Trains the model with standard or custom logic depending\n          on the specified learner type.\n        - `train_mlp`: Trains an MLP model with early stopping and additional\n          evaluation logic if required.\n        - `train_final_model`: Trains the final model on the entire dataset,\n          applying resampling, parallel processing, and specified sampling\n          methods.\n    \"\"\"\n\n    def __init__(\n        self,\n        classification: str,\n        criterion: str,\n        tuning: Optional[str],\n        hpo: Optional[str],\n        mlp_training: Optional[bool],\n        threshold_tuning: Optional[bool],\n    ) -&gt; None:\n        \"\"\"Initializes the Trainer with classification type and criterion.\"\"\"\n        super().__init__(\n            classification=classification, criterion=criterion, tuning=tuning, hpo=hpo\n        )\n        self.mlp_training = mlp_training\n        self.threshold_tuning = threshold_tuning\n\n    def evaluate(\n        self,\n        y: np.ndarray,\n        probs: np.ndarray,\n        threshold: Optional[bool] = None,\n    ) -&gt; Tuple[float, Optional[float]]:\n        \"\"\"Evaluates model performance based on the classification criterion.\n\n        For binary or multiclass classification.\n\n        Args:\n            y (np.ndarray): True labels for the validation data.\n            probs (np.ndarray): Probability predictions for each class.\n                For binary classification, the probability for the positive class.\n                For multiclass, a 2D array with probabilities.\n            threshold (bool): Flag for threshold tuning when tuning with F1.\n                Defaults to None.\n\n        Returns:\n            Tuple: Score and optimal threshold (if for binary).\n                For multiclass, only the score is returned.\n        \"\"\"\n        if self.classification == \"binary\":\n            return self._evaluate_binary(y=y, probs=probs, threshold=threshold)\n        else:\n            return self._evaluate_multiclass(y=y, probs=probs)\n\n    def _evaluate_binary(\n        self,\n        y: np.ndarray,\n        probs: np.ndarray,\n        threshold: Optional[bool] = None,\n    ) -&gt; Tuple[float, Optional[float]]:\n        \"\"\"Evaluates binary classification metrics based on probabilities.\n\n        Args:\n            y (np.ndarray): True labels for the validation data.\n            probs (np.ndarray): Probability predictions for the positive class.\n            threshold (bool): Flag for threshold tuning when tuning with F1.\n                Defaults to None.\n\n        Returns:\n            Tuple: Score and optimal threshold (if applicable).\n        \"\"\"\n        if self.criterion == \"f1\":\n            if threshold:\n                scores, thresholds = [], np.linspace(0, 1, 101)\n                for threshold in thresholds:\n                    preds = (probs &gt;= threshold).astype(int)\n                    scores.append(f1_score(y, preds, pos_label=0))\n                best_idx = np.argmax(scores)\n                return scores[best_idx], thresholds[best_idx]\n            else:\n                preds = (probs &gt;= 0.5).astype(int)\n                return f1_score(y_true=y, y_pred=preds, pos_label=0), 0.5\n        else:\n            return brier_score_loss(y_true=y, y_proba=probs), None\n\n    def _evaluate_multiclass(\n        self, y: np.ndarray, probs: np.ndarray\n    ) -&gt; Tuple[float, Optional[float]]:\n        \"\"\"Evaluates multiclass classification metrics based on probabilities.\n\n        Args:\n            y (np.ndarray): True labels for the validation data.\n            probs (np.ndarray): Probability predictions for each class (2D array).\n\n        Returns:\n            Tuple: The calculated score and None.\n        \"\"\"\n        preds = np.argmax(probs, axis=1)\n\n        if self.criterion == \"macro_f1\":\n            return f1_score(y_true=y, y_pred=preds, average=\"macro\"), None\n        else:\n            return brier_loss_multi(y=y, probs=probs), None\n\n    def evaluate_cv(\n        self, model: Any, fold: Tuple, return_probs: bool = False\n    ) -&gt; Union[float, Tuple[float, np.ndarray, np.ndarray]]:\n        \"\"\"Evaluates a model on a specific training-validation fold.\n\n        Based on a chosen performance criterion.\n\n        Args:\n            model (Any): The machine learning model used for\n                evaluation.\n            fold (tuple): A tuple containing two tuples:\n                - The first tuple contains the training data (features and labels).\n                - The second tuple contains the validation data (features and labels).\n                Specifically, it is structured as ((X_train, y_train), (X_val, y_val)),\n                where X_train and X_val are the feature matrices, and y_train and y_val\n                are the target vectors.\n            return_probs (bool): Return predicted probabilities with score if True.\n\n        Returns:\n            Union: The calculated score of the model on the validation data, and\n                optionally the true labels and predicted probabilities.\n\n        Raises:\n            AttributeError: If model does not support predict_proba method.\n        \"\"\"\n        (X_train, y_train), (X_val, y_val) = fold\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=UserWarning)\n            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n            score, _, _ = self.train(\n                model=model, X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val\n            )\n\n            if return_probs:\n                if hasattr(model, \"predict_proba\"):\n                    probs = model.predict_proba(X_val)[:, 1]\n                    return score, y_val, probs\n                else:\n                    raise AttributeError(\n                        f\"The model {type(model)} does not support predict_proba.\"\n                    )\n\n        return score\n\n    def _find_optimal_threshold(\n        self, true_labels: np.ndarray, probs: np.ndarray\n    ) -&gt; Union[float, None]:\n        \"\"\"Find the optimal threshold based on the criterion.\n\n        Converts probabilities into binary decisions.\n\n        Args:\n            true_labels (np.ndarray): The true labels for validation or test data.\n            probs (np.ndarray): Predicted probabilities for the positive class.\n\n        Returns:\n            Union: The optimal threshold for 'f1', or None if the criterion is\n                'brier_score'.\n\n        Raises:\n            ValueError: If self.criterion is not valid.\n        \"\"\"\n        if self.criterion == \"brier_score\":\n            return None\n\n        elif self.criterion == \"f1\":\n            thresholds = np.linspace(0, 1, 101)\n            scores = [\n                f1_score(y_true=true_labels, y_pred=probs &gt;= th, pos_label=0)\n                for th in thresholds\n            ]\n            best_threshold = thresholds[np.argmax(scores)]\n            print(f\"Best threshold: {best_threshold}, Best F1 score: {np.max(scores)}\")\n            return best_threshold\n        raise ValueError(f\"Invalid criterion: {self.criterion}\")\n\n    def optimize_threshold(\n        self,\n        model: Any,\n        outer_splits: Optional[List[Tuple[pd.DataFrame, pd.DataFrame]]],\n        n_jobs: int,\n    ) -&gt; Union[float, None]:\n        \"\"\"Optimize the decision threshold using cross-validation.\n\n        Aggregates probability predictions across cross-validation folds.\n\n        Args:\n            model (Any): The trained machine learning model.\n            outer_splits (List[Tuple]): List of ((X_train, y_train), (X_val, y_val)).\n            n_jobs (int): Number of parallel jobs to use for cross-validation.\n\n        Returns:\n            Union: The optimal threshold for 'f1', or None if the criterion is\n                'brier_score'.\n        \"\"\"\n        if outer_splits is None:\n            return None\n\n        results = Parallel(n_jobs=n_jobs)(\n            delayed(self.evaluate_cv)(model, fold, return_probs=True)\n            for fold in outer_splits\n        )\n\n        all_true_labels = np.concatenate([y for _, y, _ in results])\n        all_probs = np.concatenate([probs for _, _, probs in results])\n\n        return self._find_optimal_threshold(\n            true_labels=all_true_labels, probs=all_probs\n        )\n\n    @abstractmethod\n    def train(\n        self,\n        model: Any,\n        X_train: pd.DataFrame,\n        y_train: pd.Series,\n        X_val: pd.DataFrame,\n        y_val: pd.Series,\n    ):\n        \"\"\"Trains either an MLP model with custom logic or a standard model.\n\n        Args:\n            model (Any): The machine learning model to be trained.\n            X_train (pd.DataFrame): Training features.\n            y_train (pd.Series): Training labels.\n            X_val (pd.DataFrame): Validation features.\n            y_val (pd.Series): Validation labels.\n        \"\"\"\n\n    @abstractmethod\n    def train_mlp(\n        self,\n        mlp_model: MLPClassifier,\n        X_train: pd.DataFrame,\n        y_train: pd.Series,\n        X_val: pd.DataFrame,\n        y_val: pd.Series,\n        final: bool = False,\n    ):\n        \"\"\"Trains MLPClassifier with early stopping and evaluates performance.\n\n        Applies evaluation for both binary and multiclass classification.\n\n        Args:\n            mlp_model (MLPClassifier): The MLPClassifier to be trained.\n            X_train (pd.DataFrame): Training features.\n            y_train (pd.Series): Training labels.\n            X_val (pd.DataFrame): Validation features.\n            y_val (pd.Series): Validation labels.\n            final (bool): Flag for final model training.\n        \"\"\"\n\n    @abstractmethod\n    def train_final_model(\n        self,\n        df: pd.DataFrame,\n        resampler: Resampler,\n        model: Tuple,\n        sampling: Optional[str],\n        factor: Optional[float],\n        n_jobs: int,\n        seed: int,\n        test_size: float,\n        verbose: bool,\n    ):\n        \"\"\"Trains the final model.\n\n        Args:\n            df (pandas.DataFrame): The dataset used for model evaluation.\n            resampler: Resampling class.\n            model (sklearn estimator): The machine learning model used for evaluation.\n            sampling (str): The type of sampling to apply.\n            factor (float): The factor by which to upsample or downsample.\n            n_jobs (int): The number of parallel jobs to run for evaluation.\n            seed (int): Seed for splitting.\n            test_size (float): Size of train test split.\n            verbose (bool): verbose during model evaluation process if set to True.\n        \"\"\"\n</code></pre>"},{"location":"reference/training/basetrainer/#periomod.training.BaseTrainer.__init__","title":"<code>__init__(classification, criterion, tuning, hpo, mlp_training, threshold_tuning)</code>","text":"<p>Initializes the Trainer with classification type and criterion.</p> Source code in <code>periomod/training/_basetrainer.py</code> <pre><code>def __init__(\n    self,\n    classification: str,\n    criterion: str,\n    tuning: Optional[str],\n    hpo: Optional[str],\n    mlp_training: Optional[bool],\n    threshold_tuning: Optional[bool],\n) -&gt; None:\n    \"\"\"Initializes the Trainer with classification type and criterion.\"\"\"\n    super().__init__(\n        classification=classification, criterion=criterion, tuning=tuning, hpo=hpo\n    )\n    self.mlp_training = mlp_training\n    self.threshold_tuning = threshold_tuning\n</code></pre>"},{"location":"reference/training/basetrainer/#periomod.training.BaseTrainer.evaluate","title":"<code>evaluate(y, probs, threshold=None)</code>","text":"<p>Evaluates model performance based on the classification criterion.</p> <p>For binary or multiclass classification.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray</code> <p>True labels for the validation data.</p> required <code>probs</code> <code>ndarray</code> <p>Probability predictions for each class. For binary classification, the probability for the positive class. For multiclass, a 2D array with probabilities.</p> required <code>threshold</code> <code>bool</code> <p>Flag for threshold tuning when tuning with F1. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[float, Optional[float]]</code> <p>Score and optimal threshold (if for binary). For multiclass, only the score is returned.</p> Source code in <code>periomod/training/_basetrainer.py</code> <pre><code>def evaluate(\n    self,\n    y: np.ndarray,\n    probs: np.ndarray,\n    threshold: Optional[bool] = None,\n) -&gt; Tuple[float, Optional[float]]:\n    \"\"\"Evaluates model performance based on the classification criterion.\n\n    For binary or multiclass classification.\n\n    Args:\n        y (np.ndarray): True labels for the validation data.\n        probs (np.ndarray): Probability predictions for each class.\n            For binary classification, the probability for the positive class.\n            For multiclass, a 2D array with probabilities.\n        threshold (bool): Flag for threshold tuning when tuning with F1.\n            Defaults to None.\n\n    Returns:\n        Tuple: Score and optimal threshold (if for binary).\n            For multiclass, only the score is returned.\n    \"\"\"\n    if self.classification == \"binary\":\n        return self._evaluate_binary(y=y, probs=probs, threshold=threshold)\n    else:\n        return self._evaluate_multiclass(y=y, probs=probs)\n</code></pre>"},{"location":"reference/training/basetrainer/#periomod.training.BaseTrainer.evaluate_cv","title":"<code>evaluate_cv(model, fold, return_probs=False)</code>","text":"<p>Evaluates a model on a specific training-validation fold.</p> <p>Based on a chosen performance criterion.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The machine learning model used for evaluation.</p> required <code>fold</code> <code>tuple</code> <p>A tuple containing two tuples: - The first tuple contains the training data (features and labels). - The second tuple contains the validation data (features and labels). Specifically, it is structured as ((X_train, y_train), (X_val, y_val)), where X_train and X_val are the feature matrices, and y_train and y_val are the target vectors.</p> required <code>return_probs</code> <code>bool</code> <p>Return predicted probabilities with score if True.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Union</code> <code>Union[float, Tuple[float, ndarray, ndarray]]</code> <p>The calculated score of the model on the validation data, and optionally the true labels and predicted probabilities.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If model does not support predict_proba method.</p> Source code in <code>periomod/training/_basetrainer.py</code> <pre><code>def evaluate_cv(\n    self, model: Any, fold: Tuple, return_probs: bool = False\n) -&gt; Union[float, Tuple[float, np.ndarray, np.ndarray]]:\n    \"\"\"Evaluates a model on a specific training-validation fold.\n\n    Based on a chosen performance criterion.\n\n    Args:\n        model (Any): The machine learning model used for\n            evaluation.\n        fold (tuple): A tuple containing two tuples:\n            - The first tuple contains the training data (features and labels).\n            - The second tuple contains the validation data (features and labels).\n            Specifically, it is structured as ((X_train, y_train), (X_val, y_val)),\n            where X_train and X_val are the feature matrices, and y_train and y_val\n            are the target vectors.\n        return_probs (bool): Return predicted probabilities with score if True.\n\n    Returns:\n        Union: The calculated score of the model on the validation data, and\n            optionally the true labels and predicted probabilities.\n\n    Raises:\n        AttributeError: If model does not support predict_proba method.\n    \"\"\"\n    (X_train, y_train), (X_val, y_val) = fold\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n        score, _, _ = self.train(\n            model=model, X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val\n        )\n\n        if return_probs:\n            if hasattr(model, \"predict_proba\"):\n                probs = model.predict_proba(X_val)[:, 1]\n                return score, y_val, probs\n            else:\n                raise AttributeError(\n                    f\"The model {type(model)} does not support predict_proba.\"\n                )\n\n    return score\n</code></pre>"},{"location":"reference/training/basetrainer/#periomod.training.BaseTrainer.optimize_threshold","title":"<code>optimize_threshold(model, outer_splits, n_jobs)</code>","text":"<p>Optimize the decision threshold using cross-validation.</p> <p>Aggregates probability predictions across cross-validation folds.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The trained machine learning model.</p> required <code>outer_splits</code> <code>List[Tuple]</code> <p>List of ((X_train, y_train), (X_val, y_val)).</p> required <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs to use for cross-validation.</p> required <p>Returns:</p> Name Type Description <code>Union</code> <code>Union[float, None]</code> <p>The optimal threshold for 'f1', or None if the criterion is 'brier_score'.</p> Source code in <code>periomod/training/_basetrainer.py</code> <pre><code>def optimize_threshold(\n    self,\n    model: Any,\n    outer_splits: Optional[List[Tuple[pd.DataFrame, pd.DataFrame]]],\n    n_jobs: int,\n) -&gt; Union[float, None]:\n    \"\"\"Optimize the decision threshold using cross-validation.\n\n    Aggregates probability predictions across cross-validation folds.\n\n    Args:\n        model (Any): The trained machine learning model.\n        outer_splits (List[Tuple]): List of ((X_train, y_train), (X_val, y_val)).\n        n_jobs (int): Number of parallel jobs to use for cross-validation.\n\n    Returns:\n        Union: The optimal threshold for 'f1', or None if the criterion is\n            'brier_score'.\n    \"\"\"\n    if outer_splits is None:\n        return None\n\n    results = Parallel(n_jobs=n_jobs)(\n        delayed(self.evaluate_cv)(model, fold, return_probs=True)\n        for fold in outer_splits\n    )\n\n    all_true_labels = np.concatenate([y for _, y, _ in results])\n    all_probs = np.concatenate([probs for _, _, probs in results])\n\n    return self._find_optimal_threshold(\n        true_labels=all_true_labels, probs=all_probs\n    )\n</code></pre>"},{"location":"reference/training/basetrainer/#periomod.training.BaseTrainer.train","title":"<code>train(model, X_train, y_train, X_val, y_val)</code>  <code>abstractmethod</code>","text":"<p>Trains either an MLP model with custom logic or a standard model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The machine learning model to be trained.</p> required <code>X_train</code> <code>DataFrame</code> <p>Training features.</p> required <code>y_train</code> <code>Series</code> <p>Training labels.</p> required <code>X_val</code> <code>DataFrame</code> <p>Validation features.</p> required <code>y_val</code> <code>Series</code> <p>Validation labels.</p> required Source code in <code>periomod/training/_basetrainer.py</code> <pre><code>@abstractmethod\ndef train(\n    self,\n    model: Any,\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_val: pd.DataFrame,\n    y_val: pd.Series,\n):\n    \"\"\"Trains either an MLP model with custom logic or a standard model.\n\n    Args:\n        model (Any): The machine learning model to be trained.\n        X_train (pd.DataFrame): Training features.\n        y_train (pd.Series): Training labels.\n        X_val (pd.DataFrame): Validation features.\n        y_val (pd.Series): Validation labels.\n    \"\"\"\n</code></pre>"},{"location":"reference/training/basetrainer/#periomod.training.BaseTrainer.train_final_model","title":"<code>train_final_model(df, resampler, model, sampling, factor, n_jobs, seed, test_size, verbose)</code>  <code>abstractmethod</code>","text":"<p>Trains the final model.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataset used for model evaluation.</p> required <code>resampler</code> <code>Resampler</code> <p>Resampling class.</p> required <code>model</code> <code>sklearn estimator</code> <p>The machine learning model used for evaluation.</p> required <code>sampling</code> <code>str</code> <p>The type of sampling to apply.</p> required <code>factor</code> <code>float</code> <p>The factor by which to upsample or downsample.</p> required <code>n_jobs</code> <code>int</code> <p>The number of parallel jobs to run for evaluation.</p> required <code>seed</code> <code>int</code> <p>Seed for splitting.</p> required <code>test_size</code> <code>float</code> <p>Size of train test split.</p> required <code>verbose</code> <code>bool</code> <p>verbose during model evaluation process if set to True.</p> required Source code in <code>periomod/training/_basetrainer.py</code> <pre><code>@abstractmethod\ndef train_final_model(\n    self,\n    df: pd.DataFrame,\n    resampler: Resampler,\n    model: Tuple,\n    sampling: Optional[str],\n    factor: Optional[float],\n    n_jobs: int,\n    seed: int,\n    test_size: float,\n    verbose: bool,\n):\n    \"\"\"Trains the final model.\n\n    Args:\n        df (pandas.DataFrame): The dataset used for model evaluation.\n        resampler: Resampling class.\n        model (sklearn estimator): The machine learning model used for evaluation.\n        sampling (str): The type of sampling to apply.\n        factor (float): The factor by which to upsample or downsample.\n        n_jobs (int): The number of parallel jobs to run for evaluation.\n        seed (int): Seed for splitting.\n        test_size (float): Size of train test split.\n        verbose (bool): verbose during model evaluation process if set to True.\n    \"\"\"\n</code></pre>"},{"location":"reference/training/basetrainer/#periomod.training.BaseTrainer.train_mlp","title":"<code>train_mlp(mlp_model, X_train, y_train, X_val, y_val, final=False)</code>  <code>abstractmethod</code>","text":"<p>Trains MLPClassifier with early stopping and evaluates performance.</p> <p>Applies evaluation for both binary and multiclass classification.</p> <p>Parameters:</p> Name Type Description Default <code>mlp_model</code> <code>MLPClassifier</code> <p>The MLPClassifier to be trained.</p> required <code>X_train</code> <code>DataFrame</code> <p>Training features.</p> required <code>y_train</code> <code>Series</code> <p>Training labels.</p> required <code>X_val</code> <code>DataFrame</code> <p>Validation features.</p> required <code>y_val</code> <code>Series</code> <p>Validation labels.</p> required <code>final</code> <code>bool</code> <p>Flag for final model training.</p> <code>False</code> Source code in <code>periomod/training/_basetrainer.py</code> <pre><code>@abstractmethod\ndef train_mlp(\n    self,\n    mlp_model: MLPClassifier,\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_val: pd.DataFrame,\n    y_val: pd.Series,\n    final: bool = False,\n):\n    \"\"\"Trains MLPClassifier with early stopping and evaluates performance.\n\n    Applies evaluation for both binary and multiclass classification.\n\n    Args:\n        mlp_model (MLPClassifier): The MLPClassifier to be trained.\n        X_train (pd.DataFrame): Training features.\n        y_train (pd.Series): Training labels.\n        X_val (pd.DataFrame): Validation features.\n        y_val (pd.Series): Validation labels.\n        final (bool): Flag for final model training.\n    \"\"\"\n</code></pre>"},{"location":"reference/training/brier_multi/","title":"brier_loss_multi","text":"<p>Calculates the multiclass Brier score.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray</code> <p>True labels for the validation data.</p> required <code>probs</code> <code>ndarray</code> <p>Probability predictions for each class. For binary classification, this is the probability for the positive class. For multiclass, it is a 2D array with probabilities.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The calculated multiclass Brier score.</p> Source code in <code>periomod/training/_metrics.py</code> <pre><code>def brier_loss_multi(y: np.ndarray, probs: np.ndarray) -&gt; float:\n    \"\"\"Calculates the multiclass Brier score.\n\n    Args:\n        y (np.ndarray): True labels for the validation data.\n        probs (np.ndarray): Probability predictions for each class.\n            For binary classification, this is the probability for the positive class.\n            For multiclass, it is a 2D array with probabilities.\n\n    Returns:\n        float: The calculated multiclass Brier score.\n    \"\"\"\n    y_bin = label_binarize(y, classes=np.unique(y))\n    g = y_bin.shape[1]\n    return np.mean([\n        brier_score_loss(y_true=y_bin[:, i], y_proba=probs[:, i]) for i in range(g)\n    ]) * (g / 2)\n</code></pre>"},{"location":"reference/training/final_metrics/","title":"final_metrics","text":"<p>Calculate final metrics for binary or multiclass classification.</p> <p>Parameters:</p> Name Type Description Default <code>classification</code> <code>str</code> <p>The type of classification.</p> required <code>y</code> <code>ndarray</code> <p>Ground truth (actual) labels.</p> required <code>preds</code> <code>ndarray</code> <p>Predicted labels from the model.</p> required <code>probs</code> <code>Union[ndarray, None]</code> <p>Predicted probabilities from model. Only used for binary classification and if available.</p> required <code>threshold</code> <code>Union[float, None]</code> <p>Best threshold used for binary classification. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary of evaluation metrics.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the classification type is not supported.</p> Source code in <code>periomod/training/_metrics.py</code> <pre><code>def final_metrics(\n    classification: str,\n    y: np.ndarray,\n    preds: np.ndarray,\n    probs: Union[np.ndarray, None],\n    threshold: Union[float, None] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Calculate final metrics for binary or multiclass classification.\n\n    Args:\n        classification (str): The type of classification.\n        y (np.ndarray): Ground truth (actual) labels.\n        preds (np.ndarray): Predicted labels from the model.\n        probs (Union[np.ndarray, None]): Predicted probabilities from model.\n            Only used for binary classification and if available.\n        threshold (Union[float, None]): Best threshold used for binary classification.\n            Defaults to None.\n\n    Returns:\n        Dict[str, Any]: Dictionary of evaluation metrics.\n\n    Raises:\n        ValueError: If the classification type is not supported.\n    \"\"\"\n    if classification == \"binary\":\n        f1: float = f1_score(y_true=y, y_pred=preds, pos_label=0)\n        precision: float = precision_score(y_true=y, y_pred=preds, pos_label=0)\n        recall: float = recall_score(y_true=y, y_pred=preds, pos_label=0)\n        accuracy: float = accuracy_score(y_true=y, y_pred=preds)\n        brier_score_value: Union[float, None] = (\n            brier_score_loss(y_true=y, y_proba=probs) if probs is not None else None\n        )\n        roc_auc_value: Union[float, None] = (\n            roc_auc_score(y, probs) if probs is not None else None\n        )\n        conf_matrix: np.ndarray = confusion_matrix(y, preds)\n\n        return {\n            \"F1 Score\": f1,\n            \"Precision\": precision,\n            \"Recall\": recall,\n            \"Accuracy\": accuracy,\n            \"Brier Score\": brier_score_value,\n            \"ROC AUC Score\": roc_auc_value,\n            \"Confusion Matrix\": conf_matrix,\n            \"Best Threshold\": threshold,\n        }\n\n    elif classification == \"multiclass\":\n        brier_score: float = brier_loss_multi(y=y, probs=probs)\n\n        return {\n            \"Macro F1\": f1_score(y_true=y, y_pred=preds, average=\"macro\"),\n            \"Accuracy\": accuracy_score(y_true=y, y_pred=preds),\n            \"Class F1 Scores\": f1_score(y_true=y, y_pred=preds, average=None),\n            \"Multiclass Brier Score\": brier_score,\n        }\n\n    raise ValueError(f\"Unsupported classification type: {classification}\")\n</code></pre>"},{"location":"reference/training/probs/","title":"get_probs","text":"<p>Gets the predicted probabilities from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The trained model.</p> required <code>classification</code> <code>str</code> <p>The type of classification.</p> required <code>X</code> <code>DataFrame</code> <p>Predict features.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>array-like: Predicted probabilities.</p> Source code in <code>periomod/training/_metrics.py</code> <pre><code>def get_probs(model: Any, classification: str, X: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Gets the predicted probabilities from the model.\n\n    Args:\n        model (Any): The trained model.\n        classification (str): The type of classification.\n        X (pd.DataFrame): Predict features.\n\n    Returns:\n        array-like: Predicted probabilities.\n    \"\"\"\n    if classification == \"binary\":\n        return model.predict_proba(X)[:, 1]\n    else:\n        return model.predict_proba(X)\n</code></pre>"},{"location":"reference/training/trainer/","title":"Trainer","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Trainer class for supervised machine learning model training.</p> <p>Extends functionality to support MLP training with early stopping, threshold optimization, and performance evaluation based on specified criteria. The Trainer class is compatible with both binary and multiclass classification, with options for cross-validation and hyperparameter tuning.</p> Inherits <ul> <li><code>BaseTrainer</code>: Base class that implements evaluation methods.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>classification</code> <code>str</code> <p>Specifies the type of classification ('binary' or 'multiclass').</p> required <code>criterion</code> <code>str</code> <p>Defines the performance criterion to optimize (e.g., 'f1' or 'brier_score').</p> required <code>tuning</code> <code>Optional[str]</code> <p>Specifies the tuning method ('holdout' or 'cv') or None.</p> required <code>hpo</code> <code>Optional[str]</code> <p>Specifies the hyperparameter optimization method.</p> required <code>mlp_training</code> <code>Optional[bool]</code> <p>Flag to indicate if a separate MLP training procedure with early stopping is to be used. Defaults to True</p> <code>None</code> <code>threshold_tuning</code> <code>Optional[bool]</code> <p>Determines if threshold tuning is performed for binary classification when the criterion is \"f1\".</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>classification</code> <code>str</code> <p>Type of classification ('binary' or 'multiclass').</p> <code>criterion</code> <code>str</code> <p>Performance criterion to optimize ('f1', 'brier_score' or 'macro_f1').</p> <code>tuning</code> <code>Optional[str]</code> <p>Tuning method ('holdout' or 'cv') or None.</p> <code>hpo</code> <code>Optional[str]</code> <p>Hyperparameter optimization method if specified.</p> <code>mlp_training</code> <code>Optional[bool]</code> <p>Indicates if MLP training with early stopping is applied. Defaults to None.</p> <code>threshold_tuning</code> <code>Optional[bool]</code> <p>Specifies if threshold tuning is performed for binary classification when the criterion is 'f1'. Defaults to Noen.</p> <p>Methods:</p> Name Description <code>train</code> <p>Trains a machine learning model, handling custom logic for MLP and standard models.</p> <code>train_mlp</code> <p>Trains an MLPClassifier with early stopping, adapting based on classification type and criterion.</p> <code>train_final_model</code> <p>Trains the final model on resampled data, returning model and metrics.</p> Inherited Methods <ul> <li><code>evaluate</code>: Determines model performance based on the criterion.</li> <li><code>optimize_threshold</code>: Aggregates predictions across CV folds to   optimize the decision threshold.</li> <li><code>evaluate_cv</code>: Evaluates a model's performance on a CV fold.</li> </ul> Example <pre><code>from periomod.training import Trainer\nfrom sklearn.ensemble import RandomForestClassifier\n\ntrainer = Trainer(\n    classification=\"binary\", criterion=\"f1\", tuning=\"cv\", hpo=\"hebo\"\n    )\n\n# Use Resampler to obtain splits\nscore, trained_model, threshold = trainer.train(\n    model=RandomForestClassifier,\n    X_train=X_train,\n    y_train=y_train,\n    X_val=X_val,\n    y_val=y_val,\n)\nprint(f\"Score: {score}, Optimal Threshold: {threshold}\")\n\nfrom sklearn.neural_network import MLPClassifier\n\nscore, trained_mlp, threshold = trainer.train_mlp(\n    mlp_model=MLPClassifier,\n    X_train=X_train,\n    y_train=y_train,\n    X_val=X_val,\n    y_val=y_val,\n    final=True,\n)\nprint(f\"MLP Validation Score: {score}, Optimal Threshold: {threshold}\")\n</code></pre> Source code in <code>periomod/training/_trainer.py</code> <pre><code>class Trainer(BaseTrainer):\n    \"\"\"Trainer class for supervised machine learning model training.\n\n    Extends functionality to support MLP training with early stopping,\n    threshold optimization, and performance evaluation based on specified\n    criteria. The Trainer class is compatible with both binary and multiclass\n    classification, with options for cross-validation and hyperparameter\n    tuning.\n\n    Inherits:\n        - `BaseTrainer`: Base class that implements evaluation methods.\n\n    Args:\n        classification (str): Specifies the type of classification ('binary'\n            or 'multiclass').\n        criterion (str): Defines the performance criterion to optimize (e.g.,\n            'f1' or 'brier_score').\n        tuning (Optional[str]): Specifies the tuning method ('holdout' or\n            'cv') or None.\n        hpo (Optional[str]): Specifies the hyperparameter optimization method.\n        mlp_training (Optional[bool]): Flag to indicate if a separate MLP training\n            procedure with early stopping is to be used. Defaults to True\n        threshold_tuning (Optional[bool]): Determines if threshold tuning is performed\n            for binary classification when the criterion is \"f1\".\n\n    Attributes:\n        classification (str): Type of classification ('binary' or 'multiclass').\n        criterion (str): Performance criterion to optimize\n            ('f1', 'brier_score' or 'macro_f1').\n        tuning (Optional[str]): Tuning method ('holdout' or 'cv') or None.\n        hpo (Optional[str]): Hyperparameter optimization method if specified.\n        mlp_training (Optional[bool]): Indicates if MLP training with early stopping is\n            applied. Defaults to None.\n        threshold_tuning (Optional[bool]): Specifies if threshold tuning is performed\n            for binary classification when the criterion is 'f1'. Defaults to Noen.\n\n    Methods:\n        train: Trains a machine learning model, handling custom logic for\n            MLP and standard models.\n        train_mlp: Trains an MLPClassifier with early stopping, adapting\n            based on classification type and criterion.\n        train_final_model: Trains the final model on resampled data,\n            returning model and metrics.\n\n    Inherited Methods:\n        - `evaluate`: Determines model performance based on the criterion.\n        - `optimize_threshold`: Aggregates predictions across CV folds to\n          optimize the decision threshold.\n        - `evaluate_cv`: Evaluates a model's performance on a CV fold.\n\n    Example:\n        ```\n        from periomod.training import Trainer\n        from sklearn.ensemble import RandomForestClassifier\n\n        trainer = Trainer(\n            classification=\"binary\", criterion=\"f1\", tuning=\"cv\", hpo=\"hebo\"\n            )\n\n        # Use Resampler to obtain splits\n        score, trained_model, threshold = trainer.train(\n            model=RandomForestClassifier,\n            X_train=X_train,\n            y_train=y_train,\n            X_val=X_val,\n            y_val=y_val,\n        )\n        print(f\"Score: {score}, Optimal Threshold: {threshold}\")\n\n        from sklearn.neural_network import MLPClassifier\n\n        score, trained_mlp, threshold = trainer.train_mlp(\n            mlp_model=MLPClassifier,\n            X_train=X_train,\n            y_train=y_train,\n            X_val=X_val,\n            y_val=y_val,\n            final=True,\n        )\n        print(f\"MLP Validation Score: {score}, Optimal Threshold: {threshold}\")\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        classification: str,\n        criterion: str,\n        tuning: Optional[str],\n        hpo: Optional[str],\n        mlp_training: Optional[bool] = None,\n        threshold_tuning: Optional[bool] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the Trainer with classification type and criterion.\"\"\"\n        super().__init__(\n            classification=classification,\n            criterion=criterion,\n            tuning=tuning,\n            hpo=hpo,\n            mlp_training=mlp_training,\n            threshold_tuning=threshold_tuning,\n        )\n\n    def train(\n        self,\n        model: Any,\n        X_train: pd.DataFrame,\n        y_train: pd.Series,\n        X_val: pd.DataFrame,\n        y_val: pd.Series,\n    ) -&gt; Tuple[float, object, Union[float, None]]:\n        \"\"\"Trains either an MLP model with custom logic or a standard model.\n\n        Args:\n            model (Any): The machine learning model to be trained.\n            X_train (pd.DataFrame): Training features.\n            y_train (pd.Series): Training labels.\n            X_val (pd.DataFrame): Validation features.\n            y_val (pd.Series): Validation labels.\n\n        Returns:\n            Tuple: The evaluation score, trained model, and the best threshold.\n        \"\"\"\n        if isinstance(model, MLPClassifier) and self.mlp_training:\n            score, model, best_threshold = self.train_mlp(\n                mlp_model=model,\n                X_train=X_train,\n                y_train=y_train,\n                X_val=X_val,\n                y_val=y_val,\n                final=self.mlp_training,\n            )\n        else:\n            model.fit(X_train, y_train)\n            probs = get_probs(model=model, classification=self.classification, X=X_val)\n            best_threshold = None\n\n            if self.classification == \"binary\" and (\n                self.tuning == \"cv\" or self.hpo == \"hebo\"\n            ):\n                score, _ = self.evaluate(y=y_val, probs=probs, threshold=False)\n            else:\n                score, best_threshold = self.evaluate(\n                    y=y_val, probs=probs, threshold=self.threshold_tuning\n                )\n\n        return score, model, best_threshold\n\n    def train_mlp(\n        self,\n        mlp_model: MLPClassifier,\n        X_train: pd.DataFrame,\n        y_train: pd.Series,\n        X_val: pd.DataFrame,\n        y_val: pd.Series,\n        final: bool = False,\n        tol: float = 0.0001,\n        n_iter_no_change: int = 5,\n    ) -&gt; Tuple[float, MLPClassifier, Union[float, None]]:\n        \"\"\"Trains MLPClassifier with early stopping and evaluates performance.\n\n        Applies evaluation for both binary and multiclass classification.\n\n        Args:\n            mlp_model (MLPClassifier): The MLPClassifier to be trained.\n            X_train (pd.DataFrame): Training features.\n            y_train (pd.Series): Training labels.\n            X_val (pd.DataFrame): Validation features.\n            y_val (pd.Series): Validation labels.\n            final (bool): Flag for final model training.\n            tol (float): Tolerance for improvement. Defaults to 0.0001.\n            n_iter_no_change (int): Iterations without improvement in criterion for\n                early stopping. Defaults to 5.\n\n        Returns:\n            Tuple: Best validation score, trained MLPClassifier, and optimal threshold\n                (None for multiclass or if criterion is \"brier_score\").\n        \"\"\"\n        best_val_score = (\n            -float(\"inf\") if self.criterion in [\"f1\", \"macro_f1\"] else float(\"inf\")\n        )\n        best_threshold = None\n        no_improvement_count = 0\n\n        for _ in range(mlp_model.max_iter):\n            mlp_model.partial_fit(X_train, y_train, classes=np.unique(y_train))\n\n            probs = get_probs(\n                model=mlp_model, classification=self.classification, X=X_val\n            )\n            if self.classification == \"binary\":\n                if final or (self.tuning == \"cv\" or self.hpo == \"hebo\"):\n                    score, _ = self.evaluate(y=y_val, probs=probs, threshold=False)\n            else:\n                score, best_threshold = self.evaluate(\n                    y=y_val, probs=probs, threshold=self.threshold_tuning\n                )\n\n            if self.criterion in [\"f1\", \"macro_f1\"]:\n                improvement = score &gt; best_val_score + tol\n            else:\n                improvement = score &lt; best_val_score - tol\n\n            if improvement:\n                best_val_score = score\n                no_improvement_count = 0\n            else:\n                no_improvement_count += 1\n\n            if no_improvement_count &gt;= n_iter_no_change:\n                break\n\n        return best_val_score, mlp_model, best_threshold\n\n    def train_final_model(\n        self,\n        df: pd.DataFrame,\n        resampler: Resampler,\n        model: Tuple,\n        sampling: Optional[str],\n        factor: Optional[float],\n        n_jobs: int,\n        seed: int,\n        test_size: float,\n        verbose: bool = True,\n    ) -&gt; dict:\n        \"\"\"Trains the final model.\n\n        Args:\n            df (pandas.DataFrame): The dataset used for model evaluation.\n            resampler: Resampling class.\n            model (sklearn estimator): The machine learning model used for evaluation.\n            sampling (str): The type of sampling to apply.\n            factor (float): The factor by which to upsample or downsample.\n            n_jobs (int): The number of parallel jobs to run for evaluation.\n            seed (int): Seed for splitting.\n            test_size (float): Size of train test split.\n            verbose (bool): verbose during model evaluation process if set to True.\n\n        Returns:\n            dict: A dictionary containing the trained model and metrics.\n        \"\"\"\n        learner, best_params, best_threshold = model\n        model = Model.get_model(learner, self.classification)\n        final_model = clone(model)\n        final_model.set_params(**best_params)\n        final_model.best_threshold = best_threshold\n\n        if \"n_jobs\" in final_model.get_params():\n            final_model.set_params(n_jobs=n_jobs)\n\n        train_df, test_df = resampler.split_train_test_df(\n            df=df, seed=seed, test_size=test_size\n        )\n\n        X_train, y_train, X_test, y_test = resampler.split_x_y(\n            train_df=train_df, test_df=test_df, sampling=sampling, factor=factor\n        )\n        if learner == \"mlp\" and self.mlp_training:\n            train_df_h, test_df_h = resampler.split_train_test_df(\n                df=train_df, seed=seed, test_size=test_size\n            )\n\n            X_train_h, y_train_h, X_val, y_val = resampler.split_x_y(\n                train_df=train_df_h, test_df=test_df_h, sampling=sampling, factor=factor\n            )\n            _, final_model, _ = self.train_mlp(\n                mlp_model=final_model,\n                X_train=X_train_h,\n                y_train=y_train_h,\n                X_val=X_val,\n                y_val=y_val,\n                final=self.mlp_training,\n            )\n        else:\n            final_model.fit(X_train, y_train)\n        final_probs = get_probs(\n            model=final_model, classification=self.classification, X=X_test\n        )\n\n        if (\n            self.criterion == \"f1\"\n            and final_probs is not None\n            and np.any(final_probs)\n            and best_threshold is not None\n        ):\n            final_predictions = (final_probs &gt;= best_threshold).astype(int)\n        else:\n            final_predictions = final_model.predict(X_test)\n\n        metrics = final_metrics(\n            classification=self.classification,\n            y=y_test,\n            preds=final_predictions,\n            probs=final_probs,\n            threshold=best_threshold,\n        )\n        if verbose:\n            unpacked_metrics = {\n                k: round(v, 4) if isinstance(v, float) else v\n                for k, v in metrics.items()\n            }\n            results = {\n                \"Learner\": learner,\n                \"Tuning\": \"final\",\n                \"HPO\": self.hpo,\n                \"Criterion\": self.criterion,\n                **unpacked_metrics,\n            }\n\n            df_results = pd.DataFrame([results])\n            pd.set_option(\"display.max_columns\", None, \"display.width\", 1000)\n            print(\"\\nFinal Model Metrics Summary:\\n\", df_results)\n\n        return {\"model\": final_model, \"metrics\": metrics}\n</code></pre>"},{"location":"reference/training/trainer/#periomod.training.Trainer.__init__","title":"<code>__init__(classification, criterion, tuning, hpo, mlp_training=None, threshold_tuning=None)</code>","text":"<p>Initializes the Trainer with classification type and criterion.</p> Source code in <code>periomod/training/_trainer.py</code> <pre><code>def __init__(\n    self,\n    classification: str,\n    criterion: str,\n    tuning: Optional[str],\n    hpo: Optional[str],\n    mlp_training: Optional[bool] = None,\n    threshold_tuning: Optional[bool] = None,\n) -&gt; None:\n    \"\"\"Initializes the Trainer with classification type and criterion.\"\"\"\n    super().__init__(\n        classification=classification,\n        criterion=criterion,\n        tuning=tuning,\n        hpo=hpo,\n        mlp_training=mlp_training,\n        threshold_tuning=threshold_tuning,\n    )\n</code></pre>"},{"location":"reference/training/trainer/#periomod.training.Trainer.train","title":"<code>train(model, X_train, y_train, X_val, y_val)</code>","text":"<p>Trains either an MLP model with custom logic or a standard model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The machine learning model to be trained.</p> required <code>X_train</code> <code>DataFrame</code> <p>Training features.</p> required <code>y_train</code> <code>Series</code> <p>Training labels.</p> required <code>X_val</code> <code>DataFrame</code> <p>Validation features.</p> required <code>y_val</code> <code>Series</code> <p>Validation labels.</p> required <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[float, object, Union[float, None]]</code> <p>The evaluation score, trained model, and the best threshold.</p> Source code in <code>periomod/training/_trainer.py</code> <pre><code>def train(\n    self,\n    model: Any,\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_val: pd.DataFrame,\n    y_val: pd.Series,\n) -&gt; Tuple[float, object, Union[float, None]]:\n    \"\"\"Trains either an MLP model with custom logic or a standard model.\n\n    Args:\n        model (Any): The machine learning model to be trained.\n        X_train (pd.DataFrame): Training features.\n        y_train (pd.Series): Training labels.\n        X_val (pd.DataFrame): Validation features.\n        y_val (pd.Series): Validation labels.\n\n    Returns:\n        Tuple: The evaluation score, trained model, and the best threshold.\n    \"\"\"\n    if isinstance(model, MLPClassifier) and self.mlp_training:\n        score, model, best_threshold = self.train_mlp(\n            mlp_model=model,\n            X_train=X_train,\n            y_train=y_train,\n            X_val=X_val,\n            y_val=y_val,\n            final=self.mlp_training,\n        )\n    else:\n        model.fit(X_train, y_train)\n        probs = get_probs(model=model, classification=self.classification, X=X_val)\n        best_threshold = None\n\n        if self.classification == \"binary\" and (\n            self.tuning == \"cv\" or self.hpo == \"hebo\"\n        ):\n            score, _ = self.evaluate(y=y_val, probs=probs, threshold=False)\n        else:\n            score, best_threshold = self.evaluate(\n                y=y_val, probs=probs, threshold=self.threshold_tuning\n            )\n\n    return score, model, best_threshold\n</code></pre>"},{"location":"reference/training/trainer/#periomod.training.Trainer.train_final_model","title":"<code>train_final_model(df, resampler, model, sampling, factor, n_jobs, seed, test_size, verbose=True)</code>","text":"<p>Trains the final model.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataset used for model evaluation.</p> required <code>resampler</code> <code>Resampler</code> <p>Resampling class.</p> required <code>model</code> <code>sklearn estimator</code> <p>The machine learning model used for evaluation.</p> required <code>sampling</code> <code>str</code> <p>The type of sampling to apply.</p> required <code>factor</code> <code>float</code> <p>The factor by which to upsample or downsample.</p> required <code>n_jobs</code> <code>int</code> <p>The number of parallel jobs to run for evaluation.</p> required <code>seed</code> <code>int</code> <p>Seed for splitting.</p> required <code>test_size</code> <code>float</code> <p>Size of train test split.</p> required <code>verbose</code> <code>bool</code> <p>verbose during model evaluation process if set to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the trained model and metrics.</p> Source code in <code>periomod/training/_trainer.py</code> <pre><code>def train_final_model(\n    self,\n    df: pd.DataFrame,\n    resampler: Resampler,\n    model: Tuple,\n    sampling: Optional[str],\n    factor: Optional[float],\n    n_jobs: int,\n    seed: int,\n    test_size: float,\n    verbose: bool = True,\n) -&gt; dict:\n    \"\"\"Trains the final model.\n\n    Args:\n        df (pandas.DataFrame): The dataset used for model evaluation.\n        resampler: Resampling class.\n        model (sklearn estimator): The machine learning model used for evaluation.\n        sampling (str): The type of sampling to apply.\n        factor (float): The factor by which to upsample or downsample.\n        n_jobs (int): The number of parallel jobs to run for evaluation.\n        seed (int): Seed for splitting.\n        test_size (float): Size of train test split.\n        verbose (bool): verbose during model evaluation process if set to True.\n\n    Returns:\n        dict: A dictionary containing the trained model and metrics.\n    \"\"\"\n    learner, best_params, best_threshold = model\n    model = Model.get_model(learner, self.classification)\n    final_model = clone(model)\n    final_model.set_params(**best_params)\n    final_model.best_threshold = best_threshold\n\n    if \"n_jobs\" in final_model.get_params():\n        final_model.set_params(n_jobs=n_jobs)\n\n    train_df, test_df = resampler.split_train_test_df(\n        df=df, seed=seed, test_size=test_size\n    )\n\n    X_train, y_train, X_test, y_test = resampler.split_x_y(\n        train_df=train_df, test_df=test_df, sampling=sampling, factor=factor\n    )\n    if learner == \"mlp\" and self.mlp_training:\n        train_df_h, test_df_h = resampler.split_train_test_df(\n            df=train_df, seed=seed, test_size=test_size\n        )\n\n        X_train_h, y_train_h, X_val, y_val = resampler.split_x_y(\n            train_df=train_df_h, test_df=test_df_h, sampling=sampling, factor=factor\n        )\n        _, final_model, _ = self.train_mlp(\n            mlp_model=final_model,\n            X_train=X_train_h,\n            y_train=y_train_h,\n            X_val=X_val,\n            y_val=y_val,\n            final=self.mlp_training,\n        )\n    else:\n        final_model.fit(X_train, y_train)\n    final_probs = get_probs(\n        model=final_model, classification=self.classification, X=X_test\n    )\n\n    if (\n        self.criterion == \"f1\"\n        and final_probs is not None\n        and np.any(final_probs)\n        and best_threshold is not None\n    ):\n        final_predictions = (final_probs &gt;= best_threshold).astype(int)\n    else:\n        final_predictions = final_model.predict(X_test)\n\n    metrics = final_metrics(\n        classification=self.classification,\n        y=y_test,\n        preds=final_predictions,\n        probs=final_probs,\n        threshold=best_threshold,\n    )\n    if verbose:\n        unpacked_metrics = {\n            k: round(v, 4) if isinstance(v, float) else v\n            for k, v in metrics.items()\n        }\n        results = {\n            \"Learner\": learner,\n            \"Tuning\": \"final\",\n            \"HPO\": self.hpo,\n            \"Criterion\": self.criterion,\n            **unpacked_metrics,\n        }\n\n        df_results = pd.DataFrame([results])\n        pd.set_option(\"display.max_columns\", None, \"display.width\", 1000)\n        print(\"\\nFinal Model Metrics Summary:\\n\", df_results)\n\n    return {\"model\": final_model, \"metrics\": metrics}\n</code></pre>"},{"location":"reference/training/trainer/#periomod.training.Trainer.train_mlp","title":"<code>train_mlp(mlp_model, X_train, y_train, X_val, y_val, final=False, tol=0.0001, n_iter_no_change=5)</code>","text":"<p>Trains MLPClassifier with early stopping and evaluates performance.</p> <p>Applies evaluation for both binary and multiclass classification.</p> <p>Parameters:</p> Name Type Description Default <code>mlp_model</code> <code>MLPClassifier</code> <p>The MLPClassifier to be trained.</p> required <code>X_train</code> <code>DataFrame</code> <p>Training features.</p> required <code>y_train</code> <code>Series</code> <p>Training labels.</p> required <code>X_val</code> <code>DataFrame</code> <p>Validation features.</p> required <code>y_val</code> <code>Series</code> <p>Validation labels.</p> required <code>final</code> <code>bool</code> <p>Flag for final model training.</p> <code>False</code> <code>tol</code> <code>float</code> <p>Tolerance for improvement. Defaults to 0.0001.</p> <code>0.0001</code> <code>n_iter_no_change</code> <code>int</code> <p>Iterations without improvement in criterion for early stopping. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[float, MLPClassifier, Union[float, None]]</code> <p>Best validation score, trained MLPClassifier, and optimal threshold (None for multiclass or if criterion is \"brier_score\").</p> Source code in <code>periomod/training/_trainer.py</code> <pre><code>def train_mlp(\n    self,\n    mlp_model: MLPClassifier,\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_val: pd.DataFrame,\n    y_val: pd.Series,\n    final: bool = False,\n    tol: float = 0.0001,\n    n_iter_no_change: int = 5,\n) -&gt; Tuple[float, MLPClassifier, Union[float, None]]:\n    \"\"\"Trains MLPClassifier with early stopping and evaluates performance.\n\n    Applies evaluation for both binary and multiclass classification.\n\n    Args:\n        mlp_model (MLPClassifier): The MLPClassifier to be trained.\n        X_train (pd.DataFrame): Training features.\n        y_train (pd.Series): Training labels.\n        X_val (pd.DataFrame): Validation features.\n        y_val (pd.Series): Validation labels.\n        final (bool): Flag for final model training.\n        tol (float): Tolerance for improvement. Defaults to 0.0001.\n        n_iter_no_change (int): Iterations without improvement in criterion for\n            early stopping. Defaults to 5.\n\n    Returns:\n        Tuple: Best validation score, trained MLPClassifier, and optimal threshold\n            (None for multiclass or if criterion is \"brier_score\").\n    \"\"\"\n    best_val_score = (\n        -float(\"inf\") if self.criterion in [\"f1\", \"macro_f1\"] else float(\"inf\")\n    )\n    best_threshold = None\n    no_improvement_count = 0\n\n    for _ in range(mlp_model.max_iter):\n        mlp_model.partial_fit(X_train, y_train, classes=np.unique(y_train))\n\n        probs = get_probs(\n            model=mlp_model, classification=self.classification, X=X_val\n        )\n        if self.classification == \"binary\":\n            if final or (self.tuning == \"cv\" or self.hpo == \"hebo\"):\n                score, _ = self.evaluate(y=y_val, probs=probs, threshold=False)\n        else:\n            score, best_threshold = self.evaluate(\n                y=y_val, probs=probs, threshold=self.threshold_tuning\n            )\n\n        if self.criterion in [\"f1\", \"macro_f1\"]:\n            improvement = score &gt; best_val_score + tol\n        else:\n            improvement = score &lt; best_val_score - tol\n\n        if improvement:\n            best_val_score = score\n            no_improvement_count = 0\n        else:\n            no_improvement_count += 1\n\n        if no_improvement_count &gt;= n_iter_no_change:\n            break\n\n    return best_val_score, mlp_model, best_threshold\n</code></pre>"},{"location":"reference/tuning/","title":"periomod.tuning Overview","text":"<p>The <code>periomod.tuning</code> module provides classes for model tuning and hyperparameter optimization.</p>"},{"location":"reference/tuning/#available-components","title":"Available Components","text":"Component Description BaseTuner Base class for tuning models with various optimization strategies. HeboTuner Tuner using the HEBO (Heuristic BO) method for hyperparameter tuning. RandomSearchTuner Tuner using Random Search for hyperparameter tuning."},{"location":"reference/tuning/basetuner/","title":"BaseTuner","text":"<p>               Bases: <code>BaseValidator</code>, <code>ABC</code></p> <p>Base class for implementing hyperparameter tuning strategies.</p> <p>This class provides a framework for various hyperparameter optimization (HPO) strategies, supporting cross-validation (CV) and holdout tuning with options for binary and multiclass classification. Subclasses are expected to implement specific tuning methods, including holdout and CV procedures, while inheriting shared parameters, evaluation, and iteration logging functions.</p> Inherits <ul> <li><code>BaseValidator</code>: Validates instance-level variables.</li> <li><code>ABC</code>: Specifies abstract methods for subclasses to implement.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>classification</code> <code>str</code> <p>The type of classification ('binary' or 'multiclass').</p> required <code>criterion</code> <code>str</code> <p>The evaluation criterion (e.g., 'f1', 'brier_score').</p> required <code>tuning</code> <code>str</code> <p>The tuning type ('holdout' or 'cv').</p> required <code>hpo</code> <code>str</code> <p>The hyperparameter optimization method (e.g., 'random_search').</p> required <code>n_configs</code> <code>int</code> <p>Number of configurations to evaluate during HPO.</p> required <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for model training.</p> required <code>verbose</code> <code>bool</code> <p>Enables detailed logs during tuning if True.</p> required <code>trainer</code> <code>Optional[Trainer]</code> <p>Trainer instance for evaluation.</p> required <code>mlp_training</code> <code>bool</code> <p>Enables MLP training with early stopping.</p> required <code>threshold_tuning</code> <code>bool</code> <p>Performs threshold tuning for binary classification if criterion is 'f1'.</p> required <p>Attributes:</p> Name Type Description <code>classification</code> <code>str</code> <p>Type of classification ('binary' or 'multiclass').</p> <code>criterion</code> <code>str</code> <p>The performance criterion for optimization (e.g., 'f1', 'brier_score').</p> <code>tuning</code> <code>str</code> <p>Indicates the tuning approach ('holdout' or 'cv').</p> <code>hpo</code> <code>str</code> <p>Hyperparameter optimization method (e.g., 'random_search').</p> <code>n_configs</code> <code>int</code> <p>Number of configurations for HPO.</p> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for evaluation.</p> <code>verbose</code> <code>bool</code> <p>Enables logs during tuning if True.</p> <code>mlp_training</code> <code>bool</code> <p>Flag to enable MLP training with early stopping.</p> <code>threshold_tuning</code> <code>bool</code> <p>Enables threshold tuning for binary classification.</p> <code>trainer</code> <code>Trainer</code> <p>Trainer instance to handle model training and evaluation.</p> Abstract Methods <ul> <li><code>cv</code>: Defines cross-validation strategy with or without tuning.</li> <li><code>holdout</code>: Implements holdout tuning on a validation set for selected   hyperparameter configurations.</li> </ul> Source code in <code>periomod/tuning/_basetuner.py</code> <pre><code>class BaseTuner(BaseValidator, ABC):\n    \"\"\"Base class for implementing hyperparameter tuning strategies.\n\n    This class provides a framework for various hyperparameter optimization\n    (HPO) strategies, supporting cross-validation (CV) and holdout tuning\n    with options for binary and multiclass classification. Subclasses are\n    expected to implement specific tuning methods, including holdout and CV\n    procedures, while inheriting shared parameters, evaluation, and iteration\n    logging functions.\n\n    Inherits:\n        - `BaseValidator`: Validates instance-level variables.\n        - `ABC`: Specifies abstract methods for subclasses to implement.\n\n    Args:\n        classification (str): The type of classification ('binary' or 'multiclass').\n        criterion (str): The evaluation criterion (e.g., 'f1', 'brier_score').\n        tuning (str): The tuning type ('holdout' or 'cv').\n        hpo (str): The hyperparameter optimization method (e.g., 'random_search').\n        n_configs (int): Number of configurations to evaluate during HPO.\n        n_jobs (int): Number of parallel jobs for model training.\n        verbose (bool): Enables detailed logs during tuning if True.\n        trainer (Optional[Trainer]): Trainer instance for evaluation.\n        mlp_training (bool): Enables MLP training with early stopping.\n        threshold_tuning (bool): Performs threshold tuning for binary\n            classification if criterion is 'f1'.\n\n    Attributes:\n        classification (str): Type of classification ('binary' or 'multiclass').\n        criterion (str): The performance criterion for optimization\n            (e.g., 'f1', 'brier_score').\n        tuning (str): Indicates the tuning approach ('holdout' or 'cv').\n        hpo (str): Hyperparameter optimization method (e.g., 'random_search').\n        n_configs (int): Number of configurations for HPO.\n        n_jobs (int): Number of parallel jobs for evaluation.\n        verbose (bool): Enables logs during tuning if True.\n        mlp_training (bool): Flag to enable MLP training with early stopping.\n        threshold_tuning (bool): Enables threshold tuning for binary classification.\n        trainer (Trainer): Trainer instance to handle model training and evaluation.\n\n    Abstract Methods:\n        - `cv`: Defines cross-validation strategy with or without tuning.\n        - `holdout`: Implements holdout tuning on a validation set for selected\n          hyperparameter configurations.\n    \"\"\"\n\n    def __init__(\n        self,\n        classification: str,\n        criterion: str,\n        tuning: str,\n        hpo: str,\n        n_configs: int,\n        n_jobs: int,\n        verbose: bool,\n        trainer: Optional[Trainer],\n        mlp_training: bool,\n        threshold_tuning: bool,\n    ) -&gt; None:\n        \"\"\"Initializes the base tuner class with common parameters.\"\"\"\n        super().__init__(\n            classification=classification, criterion=criterion, tuning=tuning, hpo=hpo\n        )\n        self.n_configs = n_configs\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n        self.mlp_training = mlp_training\n        self.threshold_tuning = threshold_tuning\n        self.trainer = (\n            trainer\n            if trainer\n            else Trainer(\n                classification=self.classification,\n                criterion=self.criterion,\n                tuning=self.tuning,\n                hpo=self.hpo,\n                mlp_training=self.mlp_training,\n                threshold_tuning=self.threshold_tuning,\n            )\n        )\n\n    def _print_iteration_info(\n        self,\n        iteration: int,\n        model,\n        params_dict: Dict[str, Union[float, int]],\n        score: float,\n        threshold: Optional[float] = None,\n    ) -&gt; None:\n        \"\"\"Common method for printing iteration info during tuning.\n\n        Args:\n            iteration (int): The current iteration index.\n            model: The machine learning model being evaluated.\n            params_dict (Dict[str, Union[float, int]]): The suggested hyperparameters\n                as a dictionary.\n            score (float): The score achieved in the current iteration.\n            threshold (Optional[float]): The threshold if applicable\n                (for binary classification).\n        \"\"\"\n        model_name = model.__class__.__name__\n        params_str = \", \".join([\n            (\n                f\"{key}={value:.4f}\"\n                if isinstance(value, (int, float))\n                else f\"{key}={value}\"\n            )\n            for key, value in params_dict.items()\n        ])\n        score_value = (\n            f\"{score:.4f}\"\n            if np.isscalar(score) and isinstance(score, (int, float))\n            else None\n        )\n\n        if self.tuning == \"holdout\":\n            print(\n                f\"{self.hpo} holdout iteration {iteration + 1} {model_name}: \"\n                f\"'{params_str}', {self.criterion}={score_value}, \"\n                f\"threshold={threshold}\"\n            )\n        elif self.tuning == \"cv\":\n            print(\n                f\"{self.hpo} CV iteration {iteration + 1} {model_name}: \"\n                f\"'{params_str}', {self.criterion}={score_value}\"\n            )\n\n    @abstractmethod\n    def cv(\n        self,\n        learner: str,\n        outer_splits: List[Tuple[pd.DataFrame, pd.DataFrame]],\n        racing_folds: Optional[int],\n    ):\n        \"\"\"Perform cross-validation with optional tuning.\n\n        Args:\n            learner (str): The model to evaluate.\n            outer_splits (List[Tuple[pd.DataFrame, pd.DataFrame]]): Train/validation\n                splits.\n            racing_folds (Optional[int]): Number of racing folds; if None regular\n                cross-validation is performed.\n        \"\"\"\n\n    @abstractmethod\n    def holdout(\n        self,\n        learner: str,\n        X_train: pd.DataFrame,\n        y_train: pd.Series,\n        X_val: pd.DataFrame,\n        y_val: pd.Series,\n    ):\n        \"\"\"Perform random search on the holdout set for binary and multiclass .\n\n        Args:\n            learner (str): The machine learning model used for evaluation.\n            X_train (pd.DataFrame): Training features for the holdout set.\n            y_train (pd.Series): Training labels for the holdout set.\n            X_val (pd.DataFrame): Validation features for the holdout set.\n            y_val (pd.Series): Validation labels for the holdout set.\n        \"\"\"\n</code></pre>"},{"location":"reference/tuning/basetuner/#periomod.tuning.BaseTuner.__init__","title":"<code>__init__(classification, criterion, tuning, hpo, n_configs, n_jobs, verbose, trainer, mlp_training, threshold_tuning)</code>","text":"<p>Initializes the base tuner class with common parameters.</p> Source code in <code>periomod/tuning/_basetuner.py</code> <pre><code>def __init__(\n    self,\n    classification: str,\n    criterion: str,\n    tuning: str,\n    hpo: str,\n    n_configs: int,\n    n_jobs: int,\n    verbose: bool,\n    trainer: Optional[Trainer],\n    mlp_training: bool,\n    threshold_tuning: bool,\n) -&gt; None:\n    \"\"\"Initializes the base tuner class with common parameters.\"\"\"\n    super().__init__(\n        classification=classification, criterion=criterion, tuning=tuning, hpo=hpo\n    )\n    self.n_configs = n_configs\n    self.n_jobs = n_jobs\n    self.verbose = verbose\n    self.mlp_training = mlp_training\n    self.threshold_tuning = threshold_tuning\n    self.trainer = (\n        trainer\n        if trainer\n        else Trainer(\n            classification=self.classification,\n            criterion=self.criterion,\n            tuning=self.tuning,\n            hpo=self.hpo,\n            mlp_training=self.mlp_training,\n            threshold_tuning=self.threshold_tuning,\n        )\n    )\n</code></pre>"},{"location":"reference/tuning/basetuner/#periomod.tuning.BaseTuner.cv","title":"<code>cv(learner, outer_splits, racing_folds)</code>  <code>abstractmethod</code>","text":"<p>Perform cross-validation with optional tuning.</p> <p>Parameters:</p> Name Type Description Default <code>learner</code> <code>str</code> <p>The model to evaluate.</p> required <code>outer_splits</code> <code>List[Tuple[DataFrame, DataFrame]]</code> <p>Train/validation splits.</p> required <code>racing_folds</code> <code>Optional[int]</code> <p>Number of racing folds; if None regular cross-validation is performed.</p> required Source code in <code>periomod/tuning/_basetuner.py</code> <pre><code>@abstractmethod\ndef cv(\n    self,\n    learner: str,\n    outer_splits: List[Tuple[pd.DataFrame, pd.DataFrame]],\n    racing_folds: Optional[int],\n):\n    \"\"\"Perform cross-validation with optional tuning.\n\n    Args:\n        learner (str): The model to evaluate.\n        outer_splits (List[Tuple[pd.DataFrame, pd.DataFrame]]): Train/validation\n            splits.\n        racing_folds (Optional[int]): Number of racing folds; if None regular\n            cross-validation is performed.\n    \"\"\"\n</code></pre>"},{"location":"reference/tuning/basetuner/#periomod.tuning.BaseTuner.holdout","title":"<code>holdout(learner, X_train, y_train, X_val, y_val)</code>  <code>abstractmethod</code>","text":"<p>Perform random search on the holdout set for binary and multiclass .</p> <p>Parameters:</p> Name Type Description Default <code>learner</code> <code>str</code> <p>The machine learning model used for evaluation.</p> required <code>X_train</code> <code>DataFrame</code> <p>Training features for the holdout set.</p> required <code>y_train</code> <code>Series</code> <p>Training labels for the holdout set.</p> required <code>X_val</code> <code>DataFrame</code> <p>Validation features for the holdout set.</p> required <code>y_val</code> <code>Series</code> <p>Validation labels for the holdout set.</p> required Source code in <code>periomod/tuning/_basetuner.py</code> <pre><code>@abstractmethod\ndef holdout(\n    self,\n    learner: str,\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_val: pd.DataFrame,\n    y_val: pd.Series,\n):\n    \"\"\"Perform random search on the holdout set for binary and multiclass .\n\n    Args:\n        learner (str): The machine learning model used for evaluation.\n        X_train (pd.DataFrame): Training features for the holdout set.\n        y_train (pd.Series): Training labels for the holdout set.\n        X_val (pd.DataFrame): Validation features for the holdout set.\n        y_val (pd.Series): Validation labels for the holdout set.\n    \"\"\"\n</code></pre>"},{"location":"reference/tuning/hebotuner/","title":"HeboTuner","text":"<p>               Bases: <code>BaseTuner</code></p> <p>HEBO (Bayesian Optimization) hyperparameter tuning class.</p> <p>This class performs hyperparameter tuning for machine learning models using Bayesian Optimization with the HEBO library, supporting both holdout and cross-validation (CV) tuning methods.</p> Inherits <ul> <li><code>BaseTuner</code>: Provides a framework for implementing HPO strategies,   including shared evaluation and logging functions.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>classification</code> <code>str</code> <p>The type of classification ('binary' or 'multiclass').</p> required <code>criterion</code> <code>str</code> <p>The evaluation criterion (e.g., 'f1', 'brier_score').</p> required <code>tuning</code> <code>str</code> <p>The type of tuning ('holdout' or 'cv').</p> required <code>hpo</code> <code>str</code> <p>The hyperparameter optimization method (default is 'HEBO').</p> <code>'hebo'</code> <code>n_configs</code> <code>int</code> <p>Number of configurations to evaluate. Defaults to 10.</p> <code>10</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for model training. Defaults to 1.</p> <code>1</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed logs during HEBO optimization. Defaults to True.</p> <code>True</code> <code>trainer</code> <code>Optional[Trainer]</code> <p>Trainer instance for model training.</p> <code>None</code> <code>mlp_training</code> <code>bool</code> <p>Enables MLP-specific training with early stopping.</p> <code>True</code> <code>threshold_tuning</code> <code>bool</code> <p>Enables threshold tuning for binary classification when the criterion is \"f1\".</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>classification</code> <code>str</code> <p>Specifies the classification type ('binary' or 'multiclass').</p> <code>criterion</code> <code>str</code> <p>The tuning criterion to optimize ('f1', 'brier_score' or 'macro_f1').</p> <code>tuning</code> <code>str</code> <p>Indicates the tuning approach ('holdout' or 'cv').</p> <code>hpo</code> <code>str</code> <p>Hyperparameter optimization method, default is 'HEBO'.</p> <code>n_configs</code> <code>int</code> <p>Number of configurations for HPO.</p> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for model evaluation.</p> <code>verbose</code> <code>bool</code> <p>Enables logging during tuning if set to True.</p> <code>mlp_training</code> <code>bool</code> <p>Flag to enable MLP training with early stopping.</p> <code>threshold_tuning</code> <code>bool</code> <p>Enables threshold tuning for binary classification.</p> <code>trainer</code> <code>Trainer</code> <p>Trainer instance for managing model training and evaluation.</p> <p>Methods:</p> Name Description <code>holdout</code> <p>Optimizes hyperparameters using HEBO for holdout validation.</p> <code>cv</code> <p>Optimizes hyperparameters using HEBO with cross-validation.</p> Example <pre><code>trainer = Trainer(\n    classification=\"binary\",\n    criterion=\"f1\",\n    tuning=\"holdout\",\n    hpo=\"hebo\",\n    mlp_training=True,\n    threshold_tuning=True,\n)\n\ntuner = HEBOTuner(\n    classification=\"binary\",\n    criterion=\"f1\",\n    tuning=\"holdout\",\n    hpo=\"hebo\",\n    n_configs=10,\n    n_jobs=-1,\n    verbose=True,\n    trainer=trainer,\n    mlp_training=True,\n    threshold_tuning=True,\n)\n\nbest_params, best_threshold = tuner.holdout(\n    learner=\"rf\",\n    X_train=X_train,\n    y_train=y_train,\n    X_val=X_val,\n    y_val=y_val\n)\n\n# Using cross-validation\nbest_params, best_threshold = tuner.cv(\n    learner=\"rf\",\n    outer_splits=cross_val_splits\n)\n</code></pre> Source code in <code>periomod/tuning/_hebo.py</code> <pre><code>class HEBOTuner(BaseTuner):\n    \"\"\"HEBO (Bayesian Optimization) hyperparameter tuning class.\n\n    This class performs hyperparameter tuning for machine learning models\n    using Bayesian Optimization with the HEBO library, supporting both holdout\n    and cross-validation (CV) tuning methods.\n\n    Inherits:\n        - `BaseTuner`: Provides a framework for implementing HPO strategies,\n          including shared evaluation and logging functions.\n\n    Args:\n        classification (str): The type of classification ('binary' or 'multiclass').\n        criterion (str): The evaluation criterion (e.g., 'f1', 'brier_score').\n        tuning (str): The type of tuning ('holdout' or 'cv').\n        hpo (str): The hyperparameter optimization method (default is 'HEBO').\n        n_configs (int): Number of configurations to evaluate. Defaults to 10.\n        n_jobs (int): Number of parallel jobs for model training.\n            Defaults to 1.\n        verbose (bool): Whether to print detailed logs during HEBO optimization.\n            Defaults to True.\n        trainer (Optional[Trainer]): Trainer instance for model training.\n        mlp_training (bool): Enables MLP-specific training with early stopping.\n        threshold_tuning (bool): Enables threshold tuning for binary classification\n            when the criterion is \"f1\".\n\n    Attributes:\n        classification (str): Specifies the classification type\n            ('binary' or 'multiclass').\n        criterion (str): The tuning criterion to optimize\n            ('f1', 'brier_score' or 'macro_f1').\n        tuning (str): Indicates the tuning approach ('holdout' or 'cv').\n        hpo (str): Hyperparameter optimization method, default is 'HEBO'.\n        n_configs (int): Number of configurations for HPO.\n        n_jobs (int): Number of parallel jobs for model evaluation.\n        verbose (bool): Enables logging during tuning if set to True.\n        mlp_training (bool): Flag to enable MLP training with early stopping.\n        threshold_tuning (bool): Enables threshold tuning for binary classification.\n        trainer (Trainer): Trainer instance for managing model training and evaluation.\n\n    Methods:\n        holdout: Optimizes hyperparameters using HEBO for holdout validation.\n        cv: Optimizes hyperparameters using HEBO with cross-validation.\n\n    Example:\n        ```\n        trainer = Trainer(\n            classification=\"binary\",\n            criterion=\"f1\",\n            tuning=\"holdout\",\n            hpo=\"hebo\",\n            mlp_training=True,\n            threshold_tuning=True,\n        )\n\n        tuner = HEBOTuner(\n            classification=\"binary\",\n            criterion=\"f1\",\n            tuning=\"holdout\",\n            hpo=\"hebo\",\n            n_configs=10,\n            n_jobs=-1,\n            verbose=True,\n            trainer=trainer,\n            mlp_training=True,\n            threshold_tuning=True,\n        )\n\n        best_params, best_threshold = tuner.holdout(\n            learner=\"rf\",\n            X_train=X_train,\n            y_train=y_train,\n            X_val=X_val,\n            y_val=y_val\n        )\n\n        # Using cross-validation\n        best_params, best_threshold = tuner.cv(\n            learner=\"rf\",\n            outer_splits=cross_val_splits\n        )\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        classification: str,\n        criterion: str,\n        tuning: str,\n        hpo: str = \"hebo\",\n        n_configs: int = 10,\n        n_jobs: int = 1,\n        verbose: bool = True,\n        trainer: Optional[Trainer] = None,\n        mlp_training: bool = True,\n        threshold_tuning: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize HEBOTuner.\"\"\"\n        super().__init__(\n            classification=classification,\n            criterion=criterion,\n            tuning=tuning,\n            hpo=hpo,\n            n_configs=n_configs,\n            n_jobs=n_jobs,\n            verbose=verbose,\n            trainer=trainer,\n            mlp_training=mlp_training,\n            threshold_tuning=threshold_tuning,\n        )\n\n    def holdout(\n        self,\n        learner: str,\n        X_train: pd.DataFrame,\n        y_train: pd.Series,\n        X_val: pd.DataFrame,\n        y_val: pd.Series,\n    ) -&gt; Tuple[Dict[str, Union[float, int]], Optional[float]]:\n        \"\"\"Perform Bayesian Optimization using hebo for holdout validation.\n\n        Args:\n            learner (str): The machine learning model to evaluate.\n            X_train (pd.DataFrame): The training features for the holdout set.\n            y_train (pd.Series): The training labels for the holdout set.\n            X_val (pd.DataFrame): The validation features for the holdout set.\n            y_val (pd.Series): The validation labels for the holdout set.\n\n        Returns:\n            Tuple: The best hyperparameters and the best threshold.\n        \"\"\"\n        return self._run_optimization(\n            learner=learner,\n            X_train=X_train,\n            y_train=y_train,\n            X_val=X_val,\n            y_val=y_val,\n            outer_splits=None,\n        )\n\n    def cv(\n        self,\n        learner: str,\n        outer_splits: List[Tuple[pd.DataFrame, pd.DataFrame]],\n        racing_folds: Optional[int] = None,\n    ) -&gt; Tuple[Dict[str, Union[float, int]], Optional[float]]:\n        \"\"\"Perform Bayesian Optimization using HEBO with cross-validation.\n\n        Args:\n            learner (str): The machine learning model to evaluate.\n            outer_splits (List[Tuple[pd.DataFrame, pd.DataFrame]]):\n                List of cross-validation folds.\n            racing_folds (Optional[int]): Number of racing folds; if None, regular\n                cross-validation is performed.\n\n        Returns:\n            Tuple: The best hyperparameters and the best threshold.\n        \"\"\"\n        return self._run_optimization(\n            learner=learner,\n            X_train=None,\n            y_train=None,\n            X_val=None,\n            y_val=None,\n            outer_splits=outer_splits,\n        )\n\n    def _run_optimization(\n        self,\n        learner: str,\n        X_train: Optional[pd.DataFrame],\n        y_train: Optional[pd.Series],\n        X_val: Optional[pd.DataFrame],\n        y_val: Optional[pd.Series],\n        outer_splits: Optional[List[Tuple[pd.DataFrame, pd.DataFrame]]],\n    ) -&gt; Tuple[Dict[str, Union[float, int]], Optional[float]]:\n        \"\"\"Perform Bayesian Optimization using HEBO for holdout and cross-validation.\n\n        Args:\n            learner (str): The machine learning model to evaluate.\n            X_train (Optional[pd.DataFrame]): Training features for the holdout set\n                (None if using CV).\n            y_train (Optional[pd.Series]): Training labels for the holdout set\n                (None if using CV).\n            X_val (Optional[pd.DataFrame]): Validation features for the holdout set\n                (None if using CV).\n            y_val (Optional[pd.Series]): Validation labels for the holdout set\n                (None if using CV).\n            outer_splits (Optional[List[Tuple[pd.DataFrame, pd.DataFrame]]]):\n                Cross-validation folds (None if using holdout).\n\n        Returns:\n            Tuple: The best hyperparameters and the best threshold.\n        \"\"\"\n        model, search_space, params_func = Model.get(\n            learner=learner, classification=self.classification, hpo=self.hpo\n        )\n        space = DesignSpace().parse(search_space)\n        optimizer = HEBO(space)\n\n        for i in range(self.n_configs):\n            params_suggestion = optimizer.suggest(n_suggestions=1).iloc[0]\n            params_dict = params_func(params_suggestion)\n\n            score = self._objective(\n                model=model,\n                params_dict=params_dict,\n                X_train=X_train,\n                y_train=y_train,\n                X_val=X_val,\n                y_val=y_val,\n                outer_splits=outer_splits,\n            )\n            optimizer.observe(pd.DataFrame([params_suggestion]), np.array([score]))\n\n            if self.verbose:\n                self._print_iteration_info(\n                    iteration=i, model=model, params_dict=params_dict, score=score\n                )\n\n        best_params_idx = optimizer.y.argmin()\n        best_params_df = optimizer.X.iloc[best_params_idx]\n        best_params = params_func(best_params_df)\n        best_threshold = None\n        if self.classification == \"binary\" and self.threshold_tuning:\n            model_clone = clone(model).set_params(**best_params)\n            if self.criterion == \"f1\":\n                if self.tuning == \"holdout\":\n                    model_clone.fit(X_train, y_train)\n                    probs = model_clone.predict_proba(X_val)[:, 1]\n                    _, best_threshold = self.trainer.evaluate(\n                        y_val, probs, self.threshold_tuning\n                    )\n\n                elif self.tuning == \"cv\":\n                    best_threshold = self.trainer.optimize_threshold(\n                        model=model_clone, outer_splits=outer_splits, n_jobs=self.n_jobs\n                    )\n\n        return best_params, best_threshold\n\n    def _objective(\n        self,\n        model: Any,\n        params_dict: Dict[str, Union[float, int]],\n        X_train: Optional[pd.DataFrame],\n        y_train: Optional[pd.Series],\n        X_val: Optional[pd.DataFrame],\n        y_val: Optional[pd.Series],\n        outer_splits: Optional[List[Tuple[pd.DataFrame, pd.DataFrame]]],\n    ) -&gt; float:\n        \"\"\"Evaluate the model performance for both holdout and cross-validation.\n\n        Args:\n            model (Any): The machine learning model to evaluate.\n            params_dict (Dict[str, Union[float, int]]): The suggested hyperparameters\n                as a dictionary.\n            X_train (Optional[pd.DataFrame]): Training features for the holdout set\n                (None for CV).\n            y_train (Optional[pd.Series]): Training labels for the holdout set\n                (None for CV).\n            X_val (Optional[pd.DataFrame]): Validation features for the holdout set\n                (None for CV).\n            y_val (Optional[pd.Series]): Validation labels for the holdout set\n                (None for CV).\n            outer_splits (Optional[List[Tuple[pd.DataFrame, pd.DataFrame]]]):\n                Cross-validation folds (None for holdout).\n\n        Returns:\n            float: The evaluation score to be minimized by HEBO.\n        \"\"\"\n        model_clone = clone(model)\n        model_clone.set_params(**params_dict)\n\n        if \"n_jobs\" in model_clone.get_params():\n            model_clone.set_params(n_jobs=self.n_jobs)\n\n        score = self._evaluate_objective(\n            model=model_clone,\n            X_train=X_train,\n            y_train=y_train,\n            X_val=X_val,\n            y_val=y_val,\n            outer_splits=outer_splits,\n        )\n\n        return -score if self.criterion in [\"f1\", \"macro_f1\"] else score\n\n    def _evaluate_objective(\n        self,\n        model: Any,\n        X_train: pd.DataFrame,\n        y_train: pd.Series,\n        X_val: pd.DataFrame,\n        y_val: pd.Series,\n        outer_splits: Optional[List[Tuple[pd.DataFrame, pd.Series]]],\n    ) -&gt; float:\n        \"\"\"Evaluates the model's performance based on the tuning strategy.\n\n        The tuning strategy can be either 'holdout' or 'cv' (cross-validation).\n\n        Args:\n            model (Any): The cloned machine learning model to be\n                evaluated.\n            X_train (pd.DataFrame): Training features for the holdout set.\n            y_train (pd.Series): Training labels for the holdout set.\n            X_val (pd.DataFrame): Validation features for the holdout set (used for\n                'holdout' tuning).\n            y_val (pd.Series): Validation labels for the holdout set (used for\n                'holdout' tuning).\n            outer_splits (List[tuple]): List of cross-validation folds, each a tuple\n                containing (X_train_fold, y_train_fold).\n\n        Returns:\n            float: The model's performance metric based on tuning strategy.\n\n        Raises:\n            ValueError: If outer_splits is None.\n        \"\"\"\n        if self.tuning == \"holdout\":\n            score, _, _ = self.trainer.train(\n                model=model, X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val\n            )\n            return score\n\n        elif self.tuning == \"cv\":\n            if outer_splits is None:\n                raise ValueError(\n                    \"outer_splits cannot be None when using cross-validation.\"\n                )\n            scores = Parallel(n_jobs=self.n_jobs)(\n                delayed(self.trainer.evaluate_cv)(deepcopy(model), fold)\n                for fold in outer_splits\n            )\n            return np.mean(scores)\n\n        raise ValueError(f\"Unsupported criterion: {self.tuning}\")\n</code></pre>"},{"location":"reference/tuning/hebotuner/#periomod.tuning.HEBOTuner.__init__","title":"<code>__init__(classification, criterion, tuning, hpo='hebo', n_configs=10, n_jobs=1, verbose=True, trainer=None, mlp_training=True, threshold_tuning=True)</code>","text":"<p>Initialize HEBOTuner.</p> Source code in <code>periomod/tuning/_hebo.py</code> <pre><code>def __init__(\n    self,\n    classification: str,\n    criterion: str,\n    tuning: str,\n    hpo: str = \"hebo\",\n    n_configs: int = 10,\n    n_jobs: int = 1,\n    verbose: bool = True,\n    trainer: Optional[Trainer] = None,\n    mlp_training: bool = True,\n    threshold_tuning: bool = True,\n) -&gt; None:\n    \"\"\"Initialize HEBOTuner.\"\"\"\n    super().__init__(\n        classification=classification,\n        criterion=criterion,\n        tuning=tuning,\n        hpo=hpo,\n        n_configs=n_configs,\n        n_jobs=n_jobs,\n        verbose=verbose,\n        trainer=trainer,\n        mlp_training=mlp_training,\n        threshold_tuning=threshold_tuning,\n    )\n</code></pre>"},{"location":"reference/tuning/hebotuner/#periomod.tuning.HEBOTuner.cv","title":"<code>cv(learner, outer_splits, racing_folds=None)</code>","text":"<p>Perform Bayesian Optimization using HEBO with cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>learner</code> <code>str</code> <p>The machine learning model to evaluate.</p> required <code>outer_splits</code> <code>List[Tuple[DataFrame, DataFrame]]</code> <p>List of cross-validation folds.</p> required <code>racing_folds</code> <code>Optional[int]</code> <p>Number of racing folds; if None, regular cross-validation is performed.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[Dict[str, Union[float, int]], Optional[float]]</code> <p>The best hyperparameters and the best threshold.</p> Source code in <code>periomod/tuning/_hebo.py</code> <pre><code>def cv(\n    self,\n    learner: str,\n    outer_splits: List[Tuple[pd.DataFrame, pd.DataFrame]],\n    racing_folds: Optional[int] = None,\n) -&gt; Tuple[Dict[str, Union[float, int]], Optional[float]]:\n    \"\"\"Perform Bayesian Optimization using HEBO with cross-validation.\n\n    Args:\n        learner (str): The machine learning model to evaluate.\n        outer_splits (List[Tuple[pd.DataFrame, pd.DataFrame]]):\n            List of cross-validation folds.\n        racing_folds (Optional[int]): Number of racing folds; if None, regular\n            cross-validation is performed.\n\n    Returns:\n        Tuple: The best hyperparameters and the best threshold.\n    \"\"\"\n    return self._run_optimization(\n        learner=learner,\n        X_train=None,\n        y_train=None,\n        X_val=None,\n        y_val=None,\n        outer_splits=outer_splits,\n    )\n</code></pre>"},{"location":"reference/tuning/hebotuner/#periomod.tuning.HEBOTuner.holdout","title":"<code>holdout(learner, X_train, y_train, X_val, y_val)</code>","text":"<p>Perform Bayesian Optimization using hebo for holdout validation.</p> <p>Parameters:</p> Name Type Description Default <code>learner</code> <code>str</code> <p>The machine learning model to evaluate.</p> required <code>X_train</code> <code>DataFrame</code> <p>The training features for the holdout set.</p> required <code>y_train</code> <code>Series</code> <p>The training labels for the holdout set.</p> required <code>X_val</code> <code>DataFrame</code> <p>The validation features for the holdout set.</p> required <code>y_val</code> <code>Series</code> <p>The validation labels for the holdout set.</p> required <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[Dict[str, Union[float, int]], Optional[float]]</code> <p>The best hyperparameters and the best threshold.</p> Source code in <code>periomod/tuning/_hebo.py</code> <pre><code>def holdout(\n    self,\n    learner: str,\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_val: pd.DataFrame,\n    y_val: pd.Series,\n) -&gt; Tuple[Dict[str, Union[float, int]], Optional[float]]:\n    \"\"\"Perform Bayesian Optimization using hebo for holdout validation.\n\n    Args:\n        learner (str): The machine learning model to evaluate.\n        X_train (pd.DataFrame): The training features for the holdout set.\n        y_train (pd.Series): The training labels for the holdout set.\n        X_val (pd.DataFrame): The validation features for the holdout set.\n        y_val (pd.Series): The validation labels for the holdout set.\n\n    Returns:\n        Tuple: The best hyperparameters and the best threshold.\n    \"\"\"\n    return self._run_optimization(\n        learner=learner,\n        X_train=X_train,\n        y_train=y_train,\n        X_val=X_val,\n        y_val=y_val,\n        outer_splits=None,\n    )\n</code></pre>"},{"location":"reference/tuning/rstuner/","title":"RandomSearchTuner","text":"<p>               Bases: <code>BaseTuner</code></p> <p>Random Search hyperparameter tuning class.</p> <p>This class performs hyperparameter tuning using random search, supporting both holdout and cross-validation (CV) tuning methods.</p> Inherits <ul> <li><code>BaseTuner</code>: Provides a framework for implementing HPO strategies,   including shared evaluation and logging functions.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>classification</code> <code>str</code> <p>The type of classification ('binary' or 'multiclass').</p> required <code>criterion</code> <code>str</code> <p>The evaluation criterion (e.g., 'f1', 'brier_score').</p> required <code>tuning</code> <code>str</code> <p>The type of tuning ('holdout' or 'cv').</p> required <code>hpo</code> <code>str</code> <p>The hyperparameter optimization method, default is 'rs'.</p> <code>'rs'</code> <code>n_configs</code> <code>int</code> <p>Number of configurations to evaluate. Defaults to 10.</p> <code>10</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for model training. Defaults to 1.</p> <code>1</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed logs during optimization. Defaults to True.</p> <code>True</code> <code>trainer</code> <code>Optional[Trainer]</code> <p>Trainer instance for model training.</p> <code>None</code> <code>mlp_training</code> <code>bool</code> <p>Enables MLP-specific training with early stopping.</p> <code>True</code> <code>threshold_tuning</code> <code>bool</code> <p>Enables threshold tuning for binary classification when the criterion is \"f1\".</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>classification</code> <code>str</code> <p>Type of classification ('binary' or 'multiclass').</p> <code>criterion</code> <code>str</code> <p>Performance criterion for optimization ('f1', 'brier_score' or 'macro_f1').</p> <code>tuning</code> <code>str</code> <p>Tuning approach ('holdout' or 'cv').</p> <code>hpo</code> <code>str</code> <p>Hyperparameter optimization method (default is 'rs').</p> <code>n_configs</code> <code>int</code> <p>Number of configurations to evaluate.</p> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for training.</p> <code>verbose</code> <code>bool</code> <p>Flag to enable detailed logs during optimization.</p> <code>mlp_training</code> <code>bool</code> <p>Enables MLP training with early stopping.</p> <code>threshold_tuning</code> <code>bool</code> <p>Enables threshold tuning if criterion is 'f1'.</p> <code>trainer</code> <code>Trainer</code> <p>Trainer instance for model evaluation.</p> <p>Methods:</p> Name Description <code>holdout</code> <p>Optimizes hyperparameters using random search for holdout validation.</p> <code>cv</code> <p>Optimizes hyperparameters using random search with cross-validation.</p> Example <pre><code>trainer = Trainer(\n    classification=\"binary\",\n    criterion=\"f1\",\n    tuning=\"cv\",\n    hpo=\"rs\",\n    mlp_training=True,\n    threshold_tuning=True,\n)\n\ntuner = RandomSearchTuner(\n    classification=\"binary\",\n    criterion=\"f1\",\n    tuning=\"cv\",\n    hpo=\"rs\",\n    n_configs=15,\n    n_jobs=4,\n    verbose=True,\n    trainer=trainer,\n    mlp_training=True,\n    threshold_tuning=True,\n)\n\n# Running holdout-based tuning\nbest_params, best_threshold = tuner.holdout(\n    learner=\"rf\",\n    X_train=X_train,\n    y_train=y_train,\n    X_val=X_val,\n    y_val=y_val\n)\n\n# Running cross-validation tuning\nbest_params, best_threshold = tuner.cv(\n    learner=\"rf\",\n    outer_splits=cross_val_splits\n)\n</code></pre> Source code in <code>periomod/tuning/_randomsearch.py</code> <pre><code>class RandomSearchTuner(BaseTuner):\n    \"\"\"Random Search hyperparameter tuning class.\n\n    This class performs hyperparameter tuning using random search, supporting\n    both holdout and cross-validation (CV) tuning methods.\n\n    Inherits:\n        - `BaseTuner`: Provides a framework for implementing HPO strategies,\n          including shared evaluation and logging functions.\n\n    Args:\n        classification (str): The type of classification ('binary' or 'multiclass').\n        criterion (str): The evaluation criterion (e.g., 'f1', 'brier_score').\n        tuning (str): The type of tuning ('holdout' or 'cv').\n        hpo (str): The hyperparameter optimization method, default is 'rs'.\n        n_configs (int): Number of configurations to evaluate. Defaults to 10.\n        n_jobs (int): Number of parallel jobs for model training.\n            Defaults to 1.\n        verbose (bool): Whether to print detailed logs during optimization.\n            Defaults to True.\n        trainer (Optional[Trainer]): Trainer instance for model training.\n        mlp_training (bool): Enables MLP-specific training with early stopping.\n        threshold_tuning (bool): Enables threshold tuning for binary classification\n            when the criterion is \"f1\".\n\n    Attributes:\n        classification (str): Type of classification ('binary' or 'multiclass').\n        criterion (str): Performance criterion for optimization\n            ('f1', 'brier_score' or 'macro_f1').\n        tuning (str): Tuning approach ('holdout' or 'cv').\n        hpo (str): Hyperparameter optimization method (default is 'rs').\n        n_configs (int): Number of configurations to evaluate.\n        n_jobs (int): Number of parallel jobs for training.\n        verbose (bool): Flag to enable detailed logs during optimization.\n        mlp_training (bool): Enables MLP training with early stopping.\n        threshold_tuning (bool): Enables threshold tuning if criterion is 'f1'.\n        trainer (Trainer): Trainer instance for model evaluation.\n\n    Methods:\n        holdout: Optimizes hyperparameters using random search for\n            holdout validation.\n        cv: Optimizes hyperparameters using random search with\n            cross-validation.\n\n    Example:\n        ```\n        trainer = Trainer(\n            classification=\"binary\",\n            criterion=\"f1\",\n            tuning=\"cv\",\n            hpo=\"rs\",\n            mlp_training=True,\n            threshold_tuning=True,\n        )\n\n        tuner = RandomSearchTuner(\n            classification=\"binary\",\n            criterion=\"f1\",\n            tuning=\"cv\",\n            hpo=\"rs\",\n            n_configs=15,\n            n_jobs=4,\n            verbose=True,\n            trainer=trainer,\n            mlp_training=True,\n            threshold_tuning=True,\n        )\n\n        # Running holdout-based tuning\n        best_params, best_threshold = tuner.holdout(\n            learner=\"rf\",\n            X_train=X_train,\n            y_train=y_train,\n            X_val=X_val,\n            y_val=y_val\n        )\n\n        # Running cross-validation tuning\n        best_params, best_threshold = tuner.cv(\n            learner=\"rf\",\n            outer_splits=cross_val_splits\n        )\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        classification: str,\n        criterion: str,\n        tuning: str,\n        hpo: str = \"rs\",\n        n_configs: int = 10,\n        n_jobs: int = 1,\n        verbose: bool = True,\n        trainer: Optional[Trainer] = None,\n        mlp_training: bool = True,\n        threshold_tuning: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize RandomSearchTuner.\"\"\"\n        super().__init__(\n            classification=classification,\n            criterion=criterion,\n            tuning=tuning,\n            hpo=hpo,\n            n_configs=n_configs,\n            n_jobs=n_jobs,\n            verbose=verbose,\n            trainer=trainer,\n            mlp_training=mlp_training,\n            threshold_tuning=threshold_tuning,\n        )\n\n    def holdout(\n        self,\n        learner: str,\n        X_train: pd.DataFrame,\n        y_train: pd.Series,\n        X_val: pd.DataFrame,\n        y_val: pd.Series,\n    ) -&gt; Tuple[Dict[str, Union[float, int]], Union[float, None]]:\n        \"\"\"Perform random search on the holdout set for binary and multiclass .\n\n        Args:\n            learner (str): The machine learning model used for evaluation.\n            X_train (pd.DataFrame): Training features for the holdout set.\n            y_train (pd.Series): Training labels for the holdout set.\n            X_val (pd.DataFrame): Validation features for the holdout set.\n            y_val (pd.Series): Validation labels for the holdout set.\n\n        Returns:\n            tuple:\n                - Best score (float)\n                - Best hyperparameters (dict)\n                - Best threshold (float or None, applicable for binary classification).\n        \"\"\"\n        (\n            best_score,\n            best_threshold,\n            best_params,\n            param_grid,\n            model,\n        ) = self._initialize_search(learner=learner, random_state=self.rs_state)\n\n        for i in range(self.n_configs):\n            params = self._sample_params(\n                param_grid=param_grid, iteration=i, random_state=self.rs_state\n            )\n            model_clone = clone(model).set_params(**params)\n            if \"n_jobs\" in model_clone.get_params():\n                model_clone.set_params(n_jobs=self.n_jobs)\n\n            score, model_clone, threshold = self.trainer.train(\n                model_clone, X_train, y_train, X_val, y_val\n            )\n\n            best_score, best_params, best_threshold = self._update_best(\n                current_score=score,\n                params=params,\n                threshold=threshold,\n                best_score=best_score,\n                best_params=best_params,\n                best_threshold=best_threshold,\n            )\n\n            if self.verbose:\n                self._print_iteration_info(\n                    iteration=i,\n                    model=model_clone,\n                    params_dict=params,\n                    score=score,\n                    threshold=best_threshold,\n                )\n\n        return best_params, best_threshold\n\n    def cv(\n        self,\n        learner: str,\n        outer_splits: List[Tuple[pd.DataFrame, pd.DataFrame]],\n        racing_folds: Union[int, None],\n    ) -&gt; Tuple[dict, Union[float, None]]:\n        \"\"\"Perform cross-validation with optional racing and hyperparameter tuning.\n\n        Args:\n            learner: The machine learning model to evaluate.\n            outer_splits: List of training and validation splits.\n            racing_folds (int or None): Number of folds for racing; None uses all folds.\n\n        Returns:\n            tuple: Best hyperparameters, and optimal threshold (if applicable).\n        \"\"\"\n        best_score, _, best_params, param_grid, model = self._initialize_search(\n            learner=learner, random_state=self.rs_state\n        )\n\n        for i in range(self.n_configs):\n            params = self._sample_params(param_grid, random_state=self.rs_state)\n            model_clone = clone(model).set_params(**params)\n            if \"n_jobs\" in model_clone.get_params():\n                model_clone.set_params(n_jobs=self.n_jobs)\n            scores = self._evaluate_folds(\n                model=model_clone,\n                best_score=best_score,\n                outer_splits=outer_splits,\n                racing_folds=racing_folds,\n            )\n            avg_score = np.mean(scores)\n            best_score, best_params, _ = self._update_best(\n                current_score=avg_score,\n                params=params,\n                threshold=None,\n                best_score=best_score,\n                best_params=best_params,\n                best_threshold=None,\n            )\n\n            if self.verbose:\n                self._print_iteration_info(\n                    iteration=i, model=model_clone, params_dict=params, score=avg_score\n                )\n\n        if (\n            self.classification == \"binary\"\n            and self.criterion == \"f1\"\n            and self.threshold_tuning\n        ):\n            optimal_threshold = self.trainer.optimize_threshold(\n                model=model_clone, outer_splits=outer_splits, n_jobs=self.n_jobs\n            )\n        else:\n            optimal_threshold = None\n\n        return best_params, optimal_threshold\n\n    def _evaluate_folds(\n        self,\n        model: Any,\n        best_score: float,\n        outer_splits: List[Tuple[pd.DataFrame, pd.DataFrame]],\n        racing_folds: Union[int, None],\n    ) -&gt; list:\n        \"\"\"Evaluate the model across folds using cross-validation or racing strategy.\n\n        Args:\n            model (Any): The cloned model to evaluate.\n            best_score (float): The best score recorded so far.\n            outer_splits (list of tuples): List of training/validation folds.\n            racing_folds (int or None): Number of folds to use for the racing strategy.\n\n        Returns:\n            scores: Scores from each fold evaluation.\n        \"\"\"\n        num_folds = len(outer_splits)\n        if racing_folds is None or racing_folds &gt;= num_folds:\n            scores = Parallel(n_jobs=self.n_jobs)(\n                delayed(self.trainer.evaluate_cv)(deepcopy(model), fold)\n                for fold in outer_splits\n            )\n        else:\n            selected_indices = random.sample(range(num_folds), racing_folds)\n            selected_folds = [outer_splits[i] for i in selected_indices]\n            initial_scores = Parallel(n_jobs=self.n_jobs)(\n                delayed(self.trainer.evaluate_cv)(deepcopy(model), fold)\n                for fold in selected_folds\n            )\n            avg_initial_score = np.mean(initial_scores)\n\n            if (\n                self.criterion in [\"f1\", \"macro_f1\"] and avg_initial_score &gt; best_score\n            ) or (self.criterion == \"brier_score\" and avg_initial_score &lt; best_score):\n                remaining_folds = [\n                    outer_splits[i]\n                    for i in range(num_folds)\n                    if i not in selected_indices\n                ]\n                continued_scores = Parallel(n_jobs=self.n_jobs)(\n                    delayed(self.trainer.evaluate_cv)(deepcopy(model), fold)\n                    for fold in remaining_folds\n                )\n                scores = initial_scores + continued_scores\n            else:\n                scores = initial_scores\n\n        return scores\n\n    def _initialize_search(\n        self, learner: str, random_state: int\n    ) -&gt; Tuple[float, Union[float, None], Dict[str, Union[float, int]], dict, object]:\n        \"\"\"Initialize search with random seed, best score, parameters, and model.\n\n        Args:\n            learner (str): The learner type to be used for training the model.\n            random_state (int): Random state.\n\n        Returns:\n            Tuple:\n                - best_score: Initialized best score based on the criterion.\n                - best_threshold: None or a placeholder threshold for binary\n                    classification.\n                - param_grid: The parameter grid for the specified model.\n                - model: The model instance.\n        \"\"\"\n        random.seed(random_state)\n        best_score = (\n            -float(\"inf\") if self.criterion in [\"f1\", \"macro_f1\"] else float(\"inf\")\n        )\n        best_threshold = None\n        best_params: Dict[str, Union[float, int]] = {}\n        model, param_grid = Model.get(\n            learner=learner, classification=self.classification, hpo=self.hpo\n        )\n\n        return best_score, best_threshold, best_params, param_grid, model\n\n    def _update_best(\n        self,\n        current_score: float,\n        params: dict,\n        threshold: Union[float, None],\n        best_score: float,\n        best_params: dict,\n        best_threshold: Union[float, None],\n    ) -&gt; Tuple[float, dict, Union[float, None]]:\n        \"\"\"Update best score, parameters, and threshold if current score is better.\n\n        Args:\n            current_score (float): The current score obtained.\n            params (dict): The parameters associated with the current score.\n            threshold (float or None): The threshold associated with the current score.\n            best_score (float): The best score recorded so far.\n            best_params (dict): The best parameters recorded so far.\n            best_threshold (float or None): The best threshold recorded so far.\n\n        Returns:\n            tuple: Updated best score, best parameters, and best threshold (optional).\n        \"\"\"\n        if (self.criterion in [\"f1\", \"macro_f1\"] and current_score &gt; best_score) or (\n            self.criterion == \"brier_score\" and current_score &lt; best_score\n        ):\n            best_score = current_score\n            best_params = params\n            best_threshold = threshold if self.classification == \"binary\" else None\n\n        return best_score, best_params, best_threshold\n\n    def _sample_params(\n        self,\n        param_grid: Dict[str, Union[list, object]],\n        iteration: Optional[int] = None,\n        random_state: Optional[int] = None,\n    ) -&gt; Dict[str, Union[float, int]]:\n        \"\"\"Sample a set of hyperparameters from the provided grid.\n\n        Args:\n            param_grid (dict): Hyperparameter grid.\n            iteration (Optional[int]): Current iteration index for random seed\n                adjustment. If None, the iteration seed will not be adjusted.\n            random_state (Optional[int]): Random state\n\n        Returns:\n            dict: Sampled hyperparameters.\n\n        Raises:\n            TypeError: If type for parameter k is not supported.\n        \"\"\"\n        iteration_seed = (\n            random_state + iteration\n            if random_state is not None and iteration is not None\n            else None\n        )\n\n        params = {}\n        for k, v in param_grid.items():\n            if hasattr(v, \"rvs\"):\n                params[k] = v.rvs(random_state=iteration_seed)\n            elif isinstance(v, list):\n                if iteration_seed is not None:\n                    random.seed(iteration_seed)\n                params[k] = random.choice(v)\n            elif isinstance(v, np.ndarray):\n                if iteration_seed is not None:\n                    random.seed(iteration_seed)\n                params[k] = random.choice(v.tolist())\n            else:\n                raise TypeError(f\"Unsupported type for parameter '{k}': {type(v)}\")\n\n        return params\n</code></pre>"},{"location":"reference/tuning/rstuner/#periomod.tuning.RandomSearchTuner.__init__","title":"<code>__init__(classification, criterion, tuning, hpo='rs', n_configs=10, n_jobs=1, verbose=True, trainer=None, mlp_training=True, threshold_tuning=True)</code>","text":"<p>Initialize RandomSearchTuner.</p> Source code in <code>periomod/tuning/_randomsearch.py</code> <pre><code>def __init__(\n    self,\n    classification: str,\n    criterion: str,\n    tuning: str,\n    hpo: str = \"rs\",\n    n_configs: int = 10,\n    n_jobs: int = 1,\n    verbose: bool = True,\n    trainer: Optional[Trainer] = None,\n    mlp_training: bool = True,\n    threshold_tuning: bool = True,\n) -&gt; None:\n    \"\"\"Initialize RandomSearchTuner.\"\"\"\n    super().__init__(\n        classification=classification,\n        criterion=criterion,\n        tuning=tuning,\n        hpo=hpo,\n        n_configs=n_configs,\n        n_jobs=n_jobs,\n        verbose=verbose,\n        trainer=trainer,\n        mlp_training=mlp_training,\n        threshold_tuning=threshold_tuning,\n    )\n</code></pre>"},{"location":"reference/tuning/rstuner/#periomod.tuning.RandomSearchTuner.cv","title":"<code>cv(learner, outer_splits, racing_folds)</code>","text":"<p>Perform cross-validation with optional racing and hyperparameter tuning.</p> <p>Parameters:</p> Name Type Description Default <code>learner</code> <code>str</code> <p>The machine learning model to evaluate.</p> required <code>outer_splits</code> <code>List[Tuple[DataFrame, DataFrame]]</code> <p>List of training and validation splits.</p> required <code>racing_folds</code> <code>int or None</code> <p>Number of folds for racing; None uses all folds.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[dict, Union[float, None]]</code> <p>Best hyperparameters, and optimal threshold (if applicable).</p> Source code in <code>periomod/tuning/_randomsearch.py</code> <pre><code>def cv(\n    self,\n    learner: str,\n    outer_splits: List[Tuple[pd.DataFrame, pd.DataFrame]],\n    racing_folds: Union[int, None],\n) -&gt; Tuple[dict, Union[float, None]]:\n    \"\"\"Perform cross-validation with optional racing and hyperparameter tuning.\n\n    Args:\n        learner: The machine learning model to evaluate.\n        outer_splits: List of training and validation splits.\n        racing_folds (int or None): Number of folds for racing; None uses all folds.\n\n    Returns:\n        tuple: Best hyperparameters, and optimal threshold (if applicable).\n    \"\"\"\n    best_score, _, best_params, param_grid, model = self._initialize_search(\n        learner=learner, random_state=self.rs_state\n    )\n\n    for i in range(self.n_configs):\n        params = self._sample_params(param_grid, random_state=self.rs_state)\n        model_clone = clone(model).set_params(**params)\n        if \"n_jobs\" in model_clone.get_params():\n            model_clone.set_params(n_jobs=self.n_jobs)\n        scores = self._evaluate_folds(\n            model=model_clone,\n            best_score=best_score,\n            outer_splits=outer_splits,\n            racing_folds=racing_folds,\n        )\n        avg_score = np.mean(scores)\n        best_score, best_params, _ = self._update_best(\n            current_score=avg_score,\n            params=params,\n            threshold=None,\n            best_score=best_score,\n            best_params=best_params,\n            best_threshold=None,\n        )\n\n        if self.verbose:\n            self._print_iteration_info(\n                iteration=i, model=model_clone, params_dict=params, score=avg_score\n            )\n\n    if (\n        self.classification == \"binary\"\n        and self.criterion == \"f1\"\n        and self.threshold_tuning\n    ):\n        optimal_threshold = self.trainer.optimize_threshold(\n            model=model_clone, outer_splits=outer_splits, n_jobs=self.n_jobs\n        )\n    else:\n        optimal_threshold = None\n\n    return best_params, optimal_threshold\n</code></pre>"},{"location":"reference/tuning/rstuner/#periomod.tuning.RandomSearchTuner.holdout","title":"<code>holdout(learner, X_train, y_train, X_val, y_val)</code>","text":"<p>Perform random search on the holdout set for binary and multiclass .</p> <p>Parameters:</p> Name Type Description Default <code>learner</code> <code>str</code> <p>The machine learning model used for evaluation.</p> required <code>X_train</code> <code>DataFrame</code> <p>Training features for the holdout set.</p> required <code>y_train</code> <code>Series</code> <p>Training labels for the holdout set.</p> required <code>X_val</code> <code>DataFrame</code> <p>Validation features for the holdout set.</p> required <code>y_val</code> <code>Series</code> <p>Validation labels for the holdout set.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Dict[str, Union[float, int]], Union[float, None]]</code> <ul> <li>Best score (float)</li> <li>Best hyperparameters (dict)</li> <li>Best threshold (float or None, applicable for binary classification).</li> </ul> Source code in <code>periomod/tuning/_randomsearch.py</code> <pre><code>def holdout(\n    self,\n    learner: str,\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_val: pd.DataFrame,\n    y_val: pd.Series,\n) -&gt; Tuple[Dict[str, Union[float, int]], Union[float, None]]:\n    \"\"\"Perform random search on the holdout set for binary and multiclass .\n\n    Args:\n        learner (str): The machine learning model used for evaluation.\n        X_train (pd.DataFrame): Training features for the holdout set.\n        y_train (pd.Series): Training labels for the holdout set.\n        X_val (pd.DataFrame): Validation features for the holdout set.\n        y_val (pd.Series): Validation labels for the holdout set.\n\n    Returns:\n        tuple:\n            - Best score (float)\n            - Best hyperparameters (dict)\n            - Best threshold (float or None, applicable for binary classification).\n    \"\"\"\n    (\n        best_score,\n        best_threshold,\n        best_params,\n        param_grid,\n        model,\n    ) = self._initialize_search(learner=learner, random_state=self.rs_state)\n\n    for i in range(self.n_configs):\n        params = self._sample_params(\n            param_grid=param_grid, iteration=i, random_state=self.rs_state\n        )\n        model_clone = clone(model).set_params(**params)\n        if \"n_jobs\" in model_clone.get_params():\n            model_clone.set_params(n_jobs=self.n_jobs)\n\n        score, model_clone, threshold = self.trainer.train(\n            model_clone, X_train, y_train, X_val, y_val\n        )\n\n        best_score, best_params, best_threshold = self._update_best(\n            current_score=score,\n            params=params,\n            threshold=threshold,\n            best_score=best_score,\n            best_params=best_params,\n            best_threshold=best_threshold,\n        )\n\n        if self.verbose:\n            self._print_iteration_info(\n                iteration=i,\n                model=model_clone,\n                params_dict=params,\n                score=score,\n                threshold=best_threshold,\n            )\n\n    return best_params, best_threshold\n</code></pre>"},{"location":"reference/wrapper/","title":"periomod.wrapper Overview","text":"<p>The <code>periomod.wrapper</code> module contains wrapper classes for evaluation, benchmarking, and model extraction, encapsulating core functionality.</p>"},{"location":"reference/wrapper/#available-components","title":"Available Components","text":"Component Description BaseEvaluatorWrapper Base wrapper class for model evaluation and metric calculations. BenchmarkWrapper Wrapper for handling benchmarking processes and configurations. EvaluatorWrapper Wrapper for managing model evaluation and metrics computation. load_benchmark Function for loading model benchmark after training load_learners Function for loading and managing multiple learners for evaluation. ModelExtractor Class for selecting the best-ranked model based on a specified criterion. Validator Class for validating trained model on separate validation data."},{"location":"reference/wrapper/baseevaluatorwrapper/","title":"BaseEvaluatorWrapper","text":"<p>               Bases: <code>ModelExtractor</code>, <code>ABC</code></p> <p>Base class for wrappers handling model evaluation processes.</p> <p>This class serves as a foundational structure for evaluator wrappers, offering methods to initialize, prepare, and evaluate models according to specified parameters. It provides core functionality to streamline evaluation, feature importance analysis, patient inference, and jackknife resampling.</p> Inherits <ul> <li><code>BaseModelExtractor</code>: Loads configuration parameters and model extraction.</li> <li><code>ABC</code>: Specifies abstract methods that must be implemented by subclasses.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>learners_dict</code> <code>Dict</code> <p>Dictionary containing models and their metadata.</p> required <code>criterion</code> <code>str</code> <p>Criterion for selecting models (e.g., 'f1', 'brier_score').</p> required <code>aggregate</code> <code>bool</code> <p>Whether to aggregate metrics.</p> required <code>verbose</code> <code>bool</code> <p>Controls verbose in the evaluation process.</p> required <code>random_state</code> <code>int</code> <p>Random state for resampling.</p> required <code>test_size</code> <code>float</code> <p>Size of grouped train test split.</p> required <code>path</code> <code>Path</code> <p>Path to the directory containing processed data files.</p> required <p>Attributes:</p> Name Type Description <code>learners_dict</code> <code>Dict</code> <p>Holds learners and metadata.</p> <code>criterion</code> <code>str</code> <p>Evaluation criterion to select the optimal model.</p> <code>aggregate</code> <code>bool</code> <p>Indicates if metrics should be aggregated.</p> <code>verbose</code> <code>bool</code> <p>Flag for controlling logging verbose.</p> <code>random_state</code> <code>int</code> <p>Random state for resampling.</p> <code>test_size</code> <code>float</code> <p>Size of grouped train test split.</p> <code>model</code> <code>object</code> <p>Best-ranked model for the given criterion.</p> <code>encoding</code> <code>str</code> <p>Encoding type, either 'one_hot' or 'target'.</p> <code>learner</code> <code>str</code> <p>The learner associated with the best model.</p> <code>task</code> <code>str</code> <p>Task associated with the model ('pocketclosure', 'improve', etc.).</p> <code>factor</code> <code>Optional[float]</code> <p>Resampling factor if applicable.</p> <code>sampling</code> <code>Optional[str]</code> <p>Resampling strategy used (e.g., 'smote').</p> <code>classification</code> <code>str</code> <p>Classification type ('binary' or 'multiclass').</p> <code>dataloader</code> <code>ProcessedDataLoader</code> <p>Data loader and transformer.</p> <code>resampler</code> <code>Resampler</code> <p>Resampling strategy for training and testing.</p> <code>df</code> <code>DataFrame</code> <p>Loaded dataset.</p> <code>df_processed</code> <code>DataFrame</code> <p>Processed dataset.</p> <code>train_df</code> <code>DataFrame</code> <p>Training data after splitting.</p> <code>test_df</code> <code>DataFrame</code> <p>Test data after splitting.</p> <code>X_train</code> <code>DataFrame</code> <p>Training features.</p> <code>y_train</code> <code>Series</code> <p>Training labels.</p> <code>X_test</code> <code>DataFrame</code> <p>Test features.</p> <code>y_test</code> <code>Series</code> <p>Test labels.</p> <code>base_target</code> <code>Optional[ndarray]</code> <p>Baseline target for evaluations.</p> <code>baseline</code> <code>Baseline</code> <p>Basline class for model analysis.</p> <code>evaluator</code> <code>ModelEvaluator</code> <p>Evaluator for model metrics and feature importance.</p> <code>inference_engine</code> <code>ModelInference</code> <p>Model inference manager.</p> <code>trainer</code> <code>Trainer</code> <p>Trainer for model evaluation and optimization.</p> Inherited Properties <ul> <li><code>criterion (str)</code>: Retrieves or sets current evaluation criterion for model     selection. Supports 'f1', 'brier_score', and 'macro_f1'.</li> <li><code>model (object)</code>: Retrieves best-ranked model dynamically based on the current     criterion. Recalculates when criterion is updated.</li> </ul> Abstract Methods <ul> <li><code>wrapped_evaluation</code>: Performs model evaluation and generates specified plots.</li> <li><code>evaluate_cluster</code>: Performs clustering and calculates Brier scores.</li> <li><code>evaluate_feature_importance</code>: Computes feature importance using specified   methods.</li> <li><code>average_over_splits</code>: Aggregates metrics over multiple splits for model   robustness.</li> <li><code>wrapped_patient_inference</code>: Runs inference on individual patient data.</li> <li><code>wrapped_jackknife</code>: Executes jackknife resampling on patient data for   confidence interval estimation.</li> </ul> Source code in <code>periomod/wrapper/_basewrapper.py</code> <pre><code>class BaseEvaluatorWrapper(ModelExtractor, ABC):\n    \"\"\"Base class for wrappers handling model evaluation processes.\n\n    This class serves as a foundational structure for evaluator wrappers, offering\n    methods to initialize, prepare, and evaluate models according to specified\n    parameters. It provides core functionality to streamline evaluation, feature\n    importance analysis, patient inference, and jackknife resampling.\n\n    Inherits:\n        - `BaseModelExtractor`: Loads configuration parameters and model extraction.\n        - `ABC`: Specifies abstract methods that must be implemented by subclasses.\n\n    Args:\n        learners_dict (Dict): Dictionary containing models and their metadata.\n        criterion (str): Criterion for selecting models (e.g., 'f1', 'brier_score').\n        aggregate (bool): Whether to aggregate metrics.\n        verbose (bool): Controls verbose in the evaluation process.\n        random_state (int): Random state for resampling.\n        test_size (float): Size of grouped train test split.\n        path (Path): Path to the directory containing processed data files.\n\n    Attributes:\n        learners_dict (Dict): Holds learners and metadata.\n        criterion (str): Evaluation criterion to select the optimal model.\n        aggregate (bool): Indicates if metrics should be aggregated.\n        verbose (bool): Flag for controlling logging verbose.\n        random_state (int): Random state for resampling.\n        test_size (float): Size of grouped train test split.\n        model (object): Best-ranked model for the given criterion.\n        encoding (str): Encoding type, either 'one_hot' or 'target'.\n        learner (str): The learner associated with the best model.\n        task (str): Task associated with the model ('pocketclosure', 'improve', etc.).\n        factor (Optional[float]): Resampling factor if applicable.\n        sampling (Optional[str]): Resampling strategy used (e.g., 'smote').\n        classification (str): Classification type ('binary' or 'multiclass').\n        dataloader (ProcessedDataLoader): Data loader and transformer.\n        resampler (Resampler): Resampling strategy for training and testing.\n        df (pd.DataFrame): Loaded dataset.\n        df_processed (pd.DataFrame): Processed dataset.\n        train_df (pd.DataFrame): Training data after splitting.\n        test_df (pd.DataFrame): Test data after splitting.\n        X_train (pd.DataFrame): Training features.\n        y_train (pd.Series): Training labels.\n        X_test (pd.DataFrame): Test features.\n        y_test (pd.Series): Test labels.\n        base_target (Optional[np.ndarray]): Baseline target for evaluations.\n        baseline (Baseline): Basline class for model analysis.\n        evaluator (ModelEvaluator): Evaluator for model metrics and feature importance.\n        inference_engine (ModelInference): Model inference manager.\n        trainer (Trainer): Trainer for model evaluation and optimization.\n\n    Inherited Properties:\n        - `criterion (str)`: Retrieves or sets current evaluation criterion for model\n            selection. Supports 'f1', 'brier_score', and 'macro_f1'.\n        - `model (object)`: Retrieves best-ranked model dynamically based on the current\n            criterion. Recalculates when criterion is updated.\n\n    Abstract Methods:\n        - `wrapped_evaluation`: Performs model evaluation and generates specified plots.\n        - `evaluate_cluster`: Performs clustering and calculates Brier scores.\n        - `evaluate_feature_importance`: Computes feature importance using specified\n          methods.\n        - `average_over_splits`: Aggregates metrics over multiple splits for model\n          robustness.\n        - `wrapped_patient_inference`: Runs inference on individual patient data.\n        - `wrapped_jackknife`: Executes jackknife resampling on patient data for\n          confidence interval estimation.\n    \"\"\"\n\n    def __init__(\n        self,\n        learners_dict: Dict,\n        criterion: str,\n        aggregate: bool,\n        verbose: bool,\n        random_state: int,\n        test_size: float,\n        path: Path,\n    ):\n        \"\"\"Base class for EvaluatorWrapper, initializing common parameters.\"\"\"\n        super().__init__(\n            learners_dict=learners_dict,\n            criterion=criterion,\n            aggregate=aggregate,\n            verbose=verbose,\n            random_state=random_state,\n        )\n        self.path = path\n        self.test_size = test_size\n        self.dataloader = ProcessedDataLoader(task=self.task, encoding=self.encoding)\n        self.resampler = Resampler(\n            classification=self.classification, encoding=self.encoding\n        )\n        (\n            self.df,\n            self.df_processed,\n            self.train_df,\n            self.test_df,\n            self.X_train,\n            self.y_train,\n            self.X_test,\n            self.y_test,\n            self.base_target,\n        ) = self._prepare_data_for_evaluation()\n        self.baseline = Baseline(\n            task=self.task,\n            encoding=self.encoding,\n            random_state=self.random_state,\n            path=self.path,\n        )\n        self.evaluator = ModelEvaluator(\n            model=self.model,\n            X=self.X_test,\n            y=self.y_test,\n            encoding=self.encoding,\n            aggregate=self.aggregate,\n        )\n        self.inference_engine = ModelInference(\n            classification=self.classification,\n            model=self.model,\n            verbose=self.verbose,\n        )\n        self.trainer = Trainer(\n            classification=self.classification,\n            criterion=self.criterion,\n            tuning=None,\n            hpo=None,\n        )\n\n    def _prepare_data_for_evaluation(\n        self,\n    ) -&gt; Tuple[\n        pd.DataFrame,\n        pd.DataFrame,\n        pd.DataFrame,\n        pd.DataFrame,\n        pd.DataFrame,\n        pd.DataFrame,\n        pd.DataFrame,\n        pd.DataFrame,\n        Optional[np.ndarray],\n    ]:\n        \"\"\"Prepares data for evaluation.\n\n        Returns:\n            Tuple: df, df_processed, train_df, test_df, X_train, y_train, X_test,\n                y_test, and optionally base_target.\n        \"\"\"\n        data = self.dataloader.load_data(path=self.path)\n\n        task = \"pocketclosure\" if self.task == \"pocketclosureinf\" else self.task\n\n        if task in [\"pocketclosure\", \"pdgrouprevaluation\"]:\n            base_target = self._generate_base_target(df=data)\n        else:\n            base_target = None\n\n        df_processed = self.dataloader.transform_data(data=data)\n        train_df, test_df = self.resampler.split_train_test_df(\n            df=df_processed, seed=self.random_state, test_size=self.test_size\n        )\n        if task in [\"pocketclosure\", \"pdgrouprevaluation\"] and base_target is not None:\n            test_patient_ids = test_df[self.group_col]\n            base_target = (\n                base_target.reindex(df_processed.index)\n                .loc[df_processed[self.group_col].isin(test_patient_ids)]\n                .to_numpy()\n            )\n\n        X_train, y_train, X_test, y_test = self.resampler.split_x_y(\n            train_df=train_df, test_df=test_df\n        )\n\n        return (\n            data,\n            df_processed,\n            train_df,\n            test_df,\n            X_train,\n            y_train,\n            X_test,\n            y_test,\n            base_target,\n        )\n\n    def _generate_base_target(self, df: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Generates the target column before treatment based on the task.\n\n        Args:\n            df (pd.DataFrame): The input dataframe.\n\n        Returns:\n            pd.Series: The target before column for evaluation.\n\n        Raises:\n            ValueError: If self.task is invalid.\n        \"\"\"\n        if self.task in [\"pocketclosure\", \"pocketclosureinf\"]:\n            return df.apply(\n                lambda row: (\n                    0\n                    if row[\"pdbaseline\"] == 4\n                    and row[\"bop\"] == 2\n                    or row[\"pdbaseline\"] &gt; 4\n                    else 1\n                ),\n                axis=1,\n            )\n        elif self.task == \"pdgrouprevaluation\":\n            return df[\"pdgroupbase\"]\n        else:\n            raise ValueError(f\"Task '{self.task}' is not recognized.\")\n\n    def _train_and_get_metrics(\n        self, seed: int, learner: str, test_set_size: float = 0.2, n_jobs: int = -1\n    ) -&gt; dict:\n        \"\"\"Helper function to run `train_final_model` with a specific seed.\n\n        Args:\n            seed (int): Seed value for train-test split.\n            learner (str): Type of learner, used for MLP-specific training logic.\n            test_set_size (float): Size of test set. Defaults to 0.2.\n            n_jobs (int): Number of parallel jobs. Defaults to -1 (use all processors).\n\n        Returns:\n            dict: Metrics from `train_final_model`.\n        \"\"\"\n        best_params = (\n            self.model.get_params() if hasattr(self.model, \"get_params\") else {}\n        )\n        best_threshold = getattr(self.model, \"best_threshold\", None)\n        model_tuple = (learner, best_params, best_threshold)\n\n        result = self.trainer.train_final_model(\n            df=self.df_processed,\n            resampler=self.resampler,\n            model=model_tuple,\n            sampling=self.sampling,\n            factor=self.factor,\n            n_jobs=n_jobs,\n            seed=seed,\n            test_size=test_set_size,\n            verbose=self.verbose,\n        )\n        return result[\"metrics\"]\n\n    def _subset_test_set(\n        self, base: str, revaluation: str\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"Creates a subset of the test set based on differences in raw data variables.\n\n        Args:\n            base (str): Baseline variable to compare against in `df_raw`.\n            revaluation (str): Revaluation variable to check for changes in `df_raw`.\n\n        Returns:\n            Tuple: Subsets of X_test and y_test where\n                `revaluation` differs from `base`.\n        \"\"\"\n        changed_indices = self.df.index[self.df[revaluation] != self.df[base]]\n        X_test_subset = self.X_test.reindex(changed_indices)\n        y_test_subset = self.y_test.reindex(changed_indices)\n        return X_test_subset, y_test_subset\n\n    def _test_filters(\n        self,\n        X: pd.DataFrame,\n        y: pd.Series,\n        base: Optional[str],\n        revaluation: Optional[str],\n        true_preds: bool,\n        brier_threshold: Optional[float],\n    ) -&gt; Tuple[pd.DataFrame, pd.Series, int]:\n        \"\"\"Applies subsetting filters to the evaluator's test set.\n\n        Args:\n            X (pd.DataFrame): Feature set.\n            y (pd.Series): Label set.\n            base (Optional[str]): Baseline variable for comparison. If provided with\n                `revaluation`, subsets to cases where `revaluation` differs from `base`.\n            revaluation (Optional[str]): Revaluation variable for comparison. Used only\n                if `base` is also provided.\n            true_preds (bool): If True, further subsets to cases where the model's\n                predictions match the true labels.\n            brier_threshold (Optional[float]): Threshold for filtering Brier scores. If\n                provided, further subsets to cases with Brier scores below threshold.\n\n        Returns:\n            Tuple: Filtered feature set, labels and number of unique patients.\n        \"\"\"\n        if base and revaluation:\n            X, y = self._subset_test_set(base=base, revaluation=revaluation)\n            X, y = X.dropna(), y.dropna()\n\n        if true_preds:\n            pred = self.evaluator.model_predictions().reindex(y.index)\n            correct_indices = y.index[pred == y]\n            X, y = X.loc[correct_indices].dropna(), y.loc[correct_indices].dropna()\n\n        if brier_threshold is not None:\n            brier_scores = self.evaluator.brier_scores().reindex(y.index)\n            threshold_indices = brier_scores[brier_scores &lt; brier_threshold].index\n            X, y = X.loc[threshold_indices].dropna(), y.loc[threshold_indices].dropna()\n\n        subset_patient_ids = self.test_df.loc[y.index, self.group_col]\n        num_patients = subset_patient_ids.nunique()\n\n        return X, y, num_patients\n\n    @abstractmethod\n    def wrapped_evaluation(\n        self,\n        cm: bool,\n        cm_normalization: str,\n        cm_base: bool,\n        brier_groups: bool,\n        calibration: bool,\n        tight_layout: bool,\n        save: bool,\n        name: Optional[str],\n    ):\n        \"\"\"Runs evaluation on the best-ranked model based on specified criteria.\n\n        Args:\n            cm (bool): If True, plots the confusion matrix.\n            cm_base (bool): If True, plots the confusion matrix against the\n                value before treatment. Only applicable for specific tasks.\n            cm_normalization (str): Normalization type for confusion matrix.\n                (rows, columns)\n            brier_groups (bool): If True, calculates Brier score groups.\n            calibration (bool): If True, plots model calibration.\n            tight_layout (bool): If True, applies tight layout to the plot.\n            save (bool): If True, saves the plot to disk.\n            name (Optional[str]): Name for the saved plot.\n        \"\"\"\n\n    @abstractmethod\n    def evaluate_cluster(\n        self,\n        n_cluster: int,\n        base: Optional[str],\n        revaluation: Optional[str],\n        true_preds: bool,\n        brier_threshold: Optional[float],\n        tight_layout: bool,\n    ):\n        \"\"\"Performs cluster analysis with Brier scores, with optional subsetting.\n\n        Args:\n            n_cluster (int): Number of clusters for Brier score clustering analysis.\n            base (Optional[str]): Baseline variable for comparison.\n            revaluation (Optional[str]): Revaluation variable for comparison.\n            true_preds (bool): If True, further subsets to cases where model predictions\n                match the true labels.\n            brier_threshold (Optional[float]): Threshold for Brier score filtering.\n            tight_layout (bool): If True, applies tight layout to the plot.\n        \"\"\"\n\n    @abstractmethod\n    def evaluate_feature_importance(\n        self,\n        fi_types: List[str],\n        base: Optional[str],\n        revaluation: Optional[str],\n        true_preds: bool,\n        brier_threshold: Optional[float],\n    ):\n        \"\"\"Evaluates feature importance using specified types, with optional subsetting.\n\n        Args:\n            fi_types (List[str]): List of feature importance types to evaluate.\n            base (Optional[str]): Baseline variable for comparison.\n            revaluation (Optional[str]): Revaluation variable for comparison.\n            true_preds (bool): If True, further subsets to cases where model predictions\n                match the true labels.\n            brier_threshold (Optional[float]): Threshold for Brier score filtering.\n        \"\"\"\n\n    @abstractmethod\n    def average_over_splits(self, num_splits: int, n_jobs: int):\n        \"\"\"Trains the final model over multiple splits with different seeds.\n\n        Args:\n            num_splits (int): Number of random seeds/splits to train the model on.\n            n_jobs (int): Number of parallel jobs.\n        \"\"\"\n\n    @abstractmethod\n    def wrapped_patient_inference(\n        self,\n        patient: Patient,\n    ):\n        \"\"\"Runs inference on the patient's data using the best-ranked model.\n\n        Args:\n            patient (Patient): A `Patient` dataclass instance containing patient-level,\n                tooth-level, and side-level information.\n        \"\"\"\n\n    @abstractmethod\n    def wrapped_jackknife(\n        self,\n        patient: Patient,\n        results: pd.DataFrame,\n        sample_fraction: float,\n        n_jobs: int,\n        max_plots: int,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Runs jackknife resampling for inference on a given patient's data.\n\n        Args:\n            patient (Patient): `Patient` dataclass instance containing patient-level\n                information, tooth-level, and side-level details.\n            results (pd.DataFrame): DataFrame to store results from jackknife inference.\n            sample_fraction (float, optional): The fraction of patient data to use for\n                jackknife resampling.\n            n_jobs (int, optional): The number of parallel jobs to run.\n            max_plots (int): Maximum number of plots for jackknife intervals.\n        \"\"\"\n</code></pre>"},{"location":"reference/wrapper/baseevaluatorwrapper/#periomod.wrapper.BaseEvaluatorWrapper.__init__","title":"<code>__init__(learners_dict, criterion, aggregate, verbose, random_state, test_size, path)</code>","text":"<p>Base class for EvaluatorWrapper, initializing common parameters.</p> Source code in <code>periomod/wrapper/_basewrapper.py</code> <pre><code>def __init__(\n    self,\n    learners_dict: Dict,\n    criterion: str,\n    aggregate: bool,\n    verbose: bool,\n    random_state: int,\n    test_size: float,\n    path: Path,\n):\n    \"\"\"Base class for EvaluatorWrapper, initializing common parameters.\"\"\"\n    super().__init__(\n        learners_dict=learners_dict,\n        criterion=criterion,\n        aggregate=aggregate,\n        verbose=verbose,\n        random_state=random_state,\n    )\n    self.path = path\n    self.test_size = test_size\n    self.dataloader = ProcessedDataLoader(task=self.task, encoding=self.encoding)\n    self.resampler = Resampler(\n        classification=self.classification, encoding=self.encoding\n    )\n    (\n        self.df,\n        self.df_processed,\n        self.train_df,\n        self.test_df,\n        self.X_train,\n        self.y_train,\n        self.X_test,\n        self.y_test,\n        self.base_target,\n    ) = self._prepare_data_for_evaluation()\n    self.baseline = Baseline(\n        task=self.task,\n        encoding=self.encoding,\n        random_state=self.random_state,\n        path=self.path,\n    )\n    self.evaluator = ModelEvaluator(\n        model=self.model,\n        X=self.X_test,\n        y=self.y_test,\n        encoding=self.encoding,\n        aggregate=self.aggregate,\n    )\n    self.inference_engine = ModelInference(\n        classification=self.classification,\n        model=self.model,\n        verbose=self.verbose,\n    )\n    self.trainer = Trainer(\n        classification=self.classification,\n        criterion=self.criterion,\n        tuning=None,\n        hpo=None,\n    )\n</code></pre>"},{"location":"reference/wrapper/baseevaluatorwrapper/#periomod.wrapper.BaseEvaluatorWrapper.average_over_splits","title":"<code>average_over_splits(num_splits, n_jobs)</code>  <code>abstractmethod</code>","text":"<p>Trains the final model over multiple splits with different seeds.</p> <p>Parameters:</p> Name Type Description Default <code>num_splits</code> <code>int</code> <p>Number of random seeds/splits to train the model on.</p> required <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs.</p> required Source code in <code>periomod/wrapper/_basewrapper.py</code> <pre><code>@abstractmethod\ndef average_over_splits(self, num_splits: int, n_jobs: int):\n    \"\"\"Trains the final model over multiple splits with different seeds.\n\n    Args:\n        num_splits (int): Number of random seeds/splits to train the model on.\n        n_jobs (int): Number of parallel jobs.\n    \"\"\"\n</code></pre>"},{"location":"reference/wrapper/baseevaluatorwrapper/#periomod.wrapper.BaseEvaluatorWrapper.evaluate_cluster","title":"<code>evaluate_cluster(n_cluster, base, revaluation, true_preds, brier_threshold, tight_layout)</code>  <code>abstractmethod</code>","text":"<p>Performs cluster analysis with Brier scores, with optional subsetting.</p> <p>Parameters:</p> Name Type Description Default <code>n_cluster</code> <code>int</code> <p>Number of clusters for Brier score clustering analysis.</p> required <code>base</code> <code>Optional[str]</code> <p>Baseline variable for comparison.</p> required <code>revaluation</code> <code>Optional[str]</code> <p>Revaluation variable for comparison.</p> required <code>true_preds</code> <code>bool</code> <p>If True, further subsets to cases where model predictions match the true labels.</p> required <code>brier_threshold</code> <code>Optional[float]</code> <p>Threshold for Brier score filtering.</p> required <code>tight_layout</code> <code>bool</code> <p>If True, applies tight layout to the plot.</p> required Source code in <code>periomod/wrapper/_basewrapper.py</code> <pre><code>@abstractmethod\ndef evaluate_cluster(\n    self,\n    n_cluster: int,\n    base: Optional[str],\n    revaluation: Optional[str],\n    true_preds: bool,\n    brier_threshold: Optional[float],\n    tight_layout: bool,\n):\n    \"\"\"Performs cluster analysis with Brier scores, with optional subsetting.\n\n    Args:\n        n_cluster (int): Number of clusters for Brier score clustering analysis.\n        base (Optional[str]): Baseline variable for comparison.\n        revaluation (Optional[str]): Revaluation variable for comparison.\n        true_preds (bool): If True, further subsets to cases where model predictions\n            match the true labels.\n        brier_threshold (Optional[float]): Threshold for Brier score filtering.\n        tight_layout (bool): If True, applies tight layout to the plot.\n    \"\"\"\n</code></pre>"},{"location":"reference/wrapper/baseevaluatorwrapper/#periomod.wrapper.BaseEvaluatorWrapper.evaluate_feature_importance","title":"<code>evaluate_feature_importance(fi_types, base, revaluation, true_preds, brier_threshold)</code>  <code>abstractmethod</code>","text":"<p>Evaluates feature importance using specified types, with optional subsetting.</p> <p>Parameters:</p> Name Type Description Default <code>fi_types</code> <code>List[str]</code> <p>List of feature importance types to evaluate.</p> required <code>base</code> <code>Optional[str]</code> <p>Baseline variable for comparison.</p> required <code>revaluation</code> <code>Optional[str]</code> <p>Revaluation variable for comparison.</p> required <code>true_preds</code> <code>bool</code> <p>If True, further subsets to cases where model predictions match the true labels.</p> required <code>brier_threshold</code> <code>Optional[float]</code> <p>Threshold for Brier score filtering.</p> required Source code in <code>periomod/wrapper/_basewrapper.py</code> <pre><code>@abstractmethod\ndef evaluate_feature_importance(\n    self,\n    fi_types: List[str],\n    base: Optional[str],\n    revaluation: Optional[str],\n    true_preds: bool,\n    brier_threshold: Optional[float],\n):\n    \"\"\"Evaluates feature importance using specified types, with optional subsetting.\n\n    Args:\n        fi_types (List[str]): List of feature importance types to evaluate.\n        base (Optional[str]): Baseline variable for comparison.\n        revaluation (Optional[str]): Revaluation variable for comparison.\n        true_preds (bool): If True, further subsets to cases where model predictions\n            match the true labels.\n        brier_threshold (Optional[float]): Threshold for Brier score filtering.\n    \"\"\"\n</code></pre>"},{"location":"reference/wrapper/baseevaluatorwrapper/#periomod.wrapper.BaseEvaluatorWrapper.wrapped_evaluation","title":"<code>wrapped_evaluation(cm, cm_normalization, cm_base, brier_groups, calibration, tight_layout, save, name)</code>  <code>abstractmethod</code>","text":"<p>Runs evaluation on the best-ranked model based on specified criteria.</p> <p>Parameters:</p> Name Type Description Default <code>cm</code> <code>bool</code> <p>If True, plots the confusion matrix.</p> required <code>cm_base</code> <code>bool</code> <p>If True, plots the confusion matrix against the value before treatment. Only applicable for specific tasks.</p> required <code>cm_normalization</code> <code>str</code> <p>Normalization type for confusion matrix. (rows, columns)</p> required <code>brier_groups</code> <code>bool</code> <p>If True, calculates Brier score groups.</p> required <code>calibration</code> <code>bool</code> <p>If True, plots model calibration.</p> required <code>tight_layout</code> <code>bool</code> <p>If True, applies tight layout to the plot.</p> required <code>save</code> <code>bool</code> <p>If True, saves the plot to disk.</p> required <code>name</code> <code>Optional[str]</code> <p>Name for the saved plot.</p> required Source code in <code>periomod/wrapper/_basewrapper.py</code> <pre><code>@abstractmethod\ndef wrapped_evaluation(\n    self,\n    cm: bool,\n    cm_normalization: str,\n    cm_base: bool,\n    brier_groups: bool,\n    calibration: bool,\n    tight_layout: bool,\n    save: bool,\n    name: Optional[str],\n):\n    \"\"\"Runs evaluation on the best-ranked model based on specified criteria.\n\n    Args:\n        cm (bool): If True, plots the confusion matrix.\n        cm_base (bool): If True, plots the confusion matrix against the\n            value before treatment. Only applicable for specific tasks.\n        cm_normalization (str): Normalization type for confusion matrix.\n            (rows, columns)\n        brier_groups (bool): If True, calculates Brier score groups.\n        calibration (bool): If True, plots model calibration.\n        tight_layout (bool): If True, applies tight layout to the plot.\n        save (bool): If True, saves the plot to disk.\n        name (Optional[str]): Name for the saved plot.\n    \"\"\"\n</code></pre>"},{"location":"reference/wrapper/baseevaluatorwrapper/#periomod.wrapper.BaseEvaluatorWrapper.wrapped_jackknife","title":"<code>wrapped_jackknife(patient, results, sample_fraction, n_jobs, max_plots)</code>  <code>abstractmethod</code>","text":"<p>Runs jackknife resampling for inference on a given patient's data.</p> <p>Parameters:</p> Name Type Description Default <code>patient</code> <code>Patient</code> <p><code>Patient</code> dataclass instance containing patient-level information, tooth-level, and side-level details.</p> required <code>results</code> <code>DataFrame</code> <p>DataFrame to store results from jackknife inference.</p> required <code>sample_fraction</code> <code>float</code> <p>The fraction of patient data to use for jackknife resampling.</p> required <code>n_jobs</code> <code>int</code> <p>The number of parallel jobs to run.</p> required <code>max_plots</code> <code>int</code> <p>Maximum number of plots for jackknife intervals.</p> required Source code in <code>periomod/wrapper/_basewrapper.py</code> <pre><code>@abstractmethod\ndef wrapped_jackknife(\n    self,\n    patient: Patient,\n    results: pd.DataFrame,\n    sample_fraction: float,\n    n_jobs: int,\n    max_plots: int,\n) -&gt; pd.DataFrame:\n    \"\"\"Runs jackknife resampling for inference on a given patient's data.\n\n    Args:\n        patient (Patient): `Patient` dataclass instance containing patient-level\n            information, tooth-level, and side-level details.\n        results (pd.DataFrame): DataFrame to store results from jackknife inference.\n        sample_fraction (float, optional): The fraction of patient data to use for\n            jackknife resampling.\n        n_jobs (int, optional): The number of parallel jobs to run.\n        max_plots (int): Maximum number of plots for jackknife intervals.\n    \"\"\"\n</code></pre>"},{"location":"reference/wrapper/baseevaluatorwrapper/#periomod.wrapper.BaseEvaluatorWrapper.wrapped_patient_inference","title":"<code>wrapped_patient_inference(patient)</code>  <code>abstractmethod</code>","text":"<p>Runs inference on the patient's data using the best-ranked model.</p> <p>Parameters:</p> Name Type Description Default <code>patient</code> <code>Patient</code> <p>A <code>Patient</code> dataclass instance containing patient-level, tooth-level, and side-level information.</p> required Source code in <code>periomod/wrapper/_basewrapper.py</code> <pre><code>@abstractmethod\ndef wrapped_patient_inference(\n    self,\n    patient: Patient,\n):\n    \"\"\"Runs inference on the patient's data using the best-ranked model.\n\n    Args:\n        patient (Patient): A `Patient` dataclass instance containing patient-level,\n            tooth-level, and side-level information.\n    \"\"\"\n</code></pre>"},{"location":"reference/wrapper/benchmarkwrapper/","title":"BenchmarkWrapper","text":"<p>               Bases: <code>BaseBenchmark</code></p> <p>Wrapper class for model benchmarking, baseline evaluation, and result storage.</p> Inherits <ul> <li><code>BaseBenchmark</code>: Initializes parameters for benchmarking models and provides   configuration for task, learners, tuning methods, HPO, and criteria.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task for evaluation ('pocketclosure', 'pocketclosureinf', 'improvement', or 'pdgrouprevaluation'.).</p> required <code>learners</code> <code>List[str]</code> <p>List of learners to benchmark ('xgb', 'rf', 'lr' or 'mlp').</p> required <code>tuning_methods</code> <code>List[str]</code> <p>Tuning methods for each learner ('holdout', 'cv').</p> required <code>hpo_methods</code> <code>List[str]</code> <p>HPO methods ('hebo' or 'rs').</p> required <code>criteria</code> <code>List[str]</code> <p>List of evaluation criteria ('f1', 'macro_f1', 'brier_score').</p> required <code>encodings</code> <code>List[str]</code> <p>List of encodings ('one_hot' or 'target').</p> required <code>sampling</code> <code>Optional[List[str]]</code> <p>Sampling strategies to handle class imbalance. Includes None, 'upsampling', 'downsampling', and 'smote'.</p> <code>None</code> <code>factor</code> <code>Optional[float]</code> <p>Factor to apply during resampling.</p> <code>None</code> <code>n_configs</code> <code>int</code> <p>Number of configurations for hyperparameter tuning. Defaults to 10.</p> <code>10</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for processing.</p> <code>1</code> <code>cv_folds</code> <code>int</code> <p>Number of folds for cross-validation. Defaults to 10.</p> <code>10</code> <code>racing_folds</code> <code>Optional[int]</code> <p>Number of racing folds for Random Search (RS). Defaults to None.</p> <code>None</code> <code>test_seed</code> <code>int</code> <p>Random seed for test splitting. Defaults to 0.</p> <code>0</code> <code>test_size</code> <code>float</code> <p>Proportion of data used for testing. Defaults to 0.2.</p> <code>0.2</code> <code>val_size</code> <code>Optional[float]</code> <p>Size of validation set in holdout tuning. Defaults to 0.2.</p> <code>0.2</code> <code>cv_seed</code> <code>int</code> <p>Random seed for cross-validation. Defaults to 0</p> <code>0</code> <code>mlp_flag</code> <code>Optional[bool]</code> <p>Enables MLP training with early stopping. Defaults to True.</p> <code>None</code> <code>threshold_tuning</code> <code>Optional[bool]</code> <p>Enables threshold tuning for binary classification. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, enables detailed logging during benchmarking. Defaults to False.</p> <code>False</code> <code>path</code> <code>Path</code> <p>Path to the directory containing processed data files. Defaults to Path(\"data/processed\").</p> <code>Path('data/processed/processed_data.csv')</code> <p>Attributes:</p> Name Type Description <code>task</code> <code>str</code> <p>The specified task for evaluation.</p> <code>learners</code> <code>List[str]</code> <p>List of learners to evaluate.</p> <code>tuning_methods</code> <code>List[str]</code> <p>Tuning methods for model evaluation.</p> <code>hpo_methods</code> <code>List[str]</code> <p>HPO methods for hyperparameter tuning.</p> <code>criteria</code> <code>List[str]</code> <p>List of evaluation metrics.</p> <code>encodings</code> <code>List[str]</code> <p>Encoding types for categorical features.</p> <code>sampling</code> <code>List[str]</code> <p>Resampling strategies for class balancing.</p> <code>factor</code> <code>float</code> <p>Resampling factor for balancing.</p> <code>n_configs</code> <code>int</code> <p>Number of configurations for hyperparameter tuning.</p> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for model training.</p> <code>cv_folds</code> <code>int</code> <p>Number of cross-validation folds.</p> <code>racing_folds</code> <code>int</code> <p>Number of racing folds for random search.</p> <code>test_seed</code> <code>int</code> <p>Seed for reproducible train-test splits.</p> <code>test_size</code> <code>float</code> <p>Size of the test split.</p> <code>val_size</code> <code>float</code> <p>Size of the validation split in holdout tuning.</p> <code>cv_seed</code> <code>int</code> <p>Seed for cross-validation splits.</p> <code>mlp_flag</code> <code>bool</code> <p>Indicates if MLP training with early stopping is used.</p> <code>threshold_tuning</code> <code>bool</code> <p>Enables threshold tuning for binary classification.</p> <code>verbose</code> <code>bool</code> <p>Enables detailed logging during benchmarking.</p> <code>path</code> <code>Path</code> <p>Directory path for processed data.</p> <code>classification</code> <code>str</code> <p>'binary' or 'multiclass' based on the task.</p> <p>Methods:</p> Name Description <code>baseline</code> <p>Evaluates baseline models for each encoding and returns metrics.</p> <code>wrapped_benchmark</code> <p>Runs benchmarks with various learners, encodings, and tuning methods.</p> <code>save_benchmark</code> <p>Saves benchmark results to a specified directory.</p> <code>save_learners</code> <p>Saves learners to a specified directory as serialized files.</p> Example <pre><code>from periomod.wrapper import BenchmarkWrapper\n\n# Initialize the BenchmarkWrapper\nbenchmarker = BenchmarkWrapper(\n    task=\"pocketclosure\",\n    encodings=[\"one_hot\", \"target\"],\n    learners=[\"rf\", \"xgb\", \"lr\", \"mlp\"],\n    tuning_methods=[\"holdout\", \"cv\"],\n    hpo_methods=[\"rs\", \"hebo\"],\n    criteria=[\"f1\", \"brier_score\"],\n    sampling=[\"upsampling\"],\n    factor=2,\n    path=\"/data/processed/processed_data.csv\",\n)\n\n# Run baseline benchmarking\nbaseline_df = benchmarker.baseline()\n\n# Run full benchmark and retrieve results\nbenchmark, learners = benchmarker.wrapped_benchmark()\n\n# Save the benchmark results\nbenchmarker.save_benchmark(\n    benchmark_df=benchmark,\n    path=\"reports/experiment/benchmark.csv\",\n)\n\n# Save the trained learners\nbenchmarker.save_learners(learners_dict=learners, path=\"models/experiment\")\n</code></pre> Source code in <code>periomod/wrapper/_wrapper.py</code> <pre><code>class BenchmarkWrapper(BaseBenchmark):\n    \"\"\"Wrapper class for model benchmarking, baseline evaluation, and result storage.\n\n    Inherits:\n        - `BaseBenchmark`: Initializes parameters for benchmarking models and provides\n          configuration for task, learners, tuning methods, HPO, and criteria.\n\n    Args:\n        task (str): Task for evaluation ('pocketclosure', 'pocketclosureinf',\n            'improvement', or 'pdgrouprevaluation'.).\n        learners (List[str]): List of learners to benchmark ('xgb', 'rf', 'lr' or\n            'mlp').\n        tuning_methods (List[str]): Tuning methods for each learner ('holdout', 'cv').\n        hpo_methods (List[str]): HPO methods ('hebo' or 'rs').\n        criteria (List[str]): List of evaluation criteria ('f1', 'macro_f1',\n            'brier_score').\n        encodings (List[str]): List of encodings ('one_hot' or 'target').\n        sampling (Optional[List[str]]): Sampling strategies to handle class imbalance.\n            Includes None, 'upsampling', 'downsampling', and 'smote'.\n        factor (Optional[float]): Factor to apply during resampling.\n        n_configs (int): Number of configurations for hyperparameter tuning.\n            Defaults to 10.\n        n_jobs (int): Number of parallel jobs for processing.\n        cv_folds (int): Number of folds for cross-validation. Defaults to 10.\n        racing_folds (Optional[int]): Number of racing folds for Random Search (RS).\n            Defaults to None.\n        test_seed (int): Random seed for test splitting. Defaults to 0.\n        test_size (float): Proportion of data used for testing. Defaults to\n            0.2.\n        val_size (Optional[float]): Size of validation set in holdout tuning.\n            Defaults to 0.2.\n        cv_seed (int): Random seed for cross-validation. Defaults to 0\n        mlp_flag (Optional[bool]): Enables MLP training with early stopping.\n            Defaults to True.\n        threshold_tuning (Optional[bool]): Enables threshold tuning for binary\n            classification. Defaults to None.\n        verbose (bool): If True, enables detailed logging during benchmarking.\n            Defaults to False.\n        path (Path): Path to the directory containing processed data files.\n            Defaults to Path(\"data/processed\").\n\n    Attributes:\n        task (str): The specified task for evaluation.\n        learners (List[str]): List of learners to evaluate.\n        tuning_methods (List[str]): Tuning methods for model evaluation.\n        hpo_methods (List[str]): HPO methods for hyperparameter tuning.\n        criteria (List[str]): List of evaluation metrics.\n        encodings (List[str]): Encoding types for categorical features.\n        sampling (List[str]): Resampling strategies for class balancing.\n        factor (float): Resampling factor for balancing.\n        n_configs (int): Number of configurations for hyperparameter tuning.\n        n_jobs (int): Number of parallel jobs for model training.\n        cv_folds (int): Number of cross-validation folds.\n        racing_folds (int): Number of racing folds for random search.\n        test_seed (int): Seed for reproducible train-test splits.\n        test_size (float): Size of the test split.\n        val_size (float): Size of the validation split in holdout tuning.\n        cv_seed (int): Seed for cross-validation splits.\n        mlp_flag (bool): Indicates if MLP training with early stopping is used.\n        threshold_tuning (bool): Enables threshold tuning for binary classification.\n        verbose (bool): Enables detailed logging during benchmarking.\n        path (Path): Directory path for processed data.\n        classification (str): 'binary' or 'multiclass' based on the task.\n\n    Methods:\n        baseline: Evaluates baseline models for each encoding and returns metrics.\n        wrapped_benchmark: Runs benchmarks with various learners, encodings, and\n            tuning methods.\n        save_benchmark: Saves benchmark results to a specified directory.\n        save_learners: Saves learners to a specified directory as serialized files.\n\n    Example:\n        ```\n        from periomod.wrapper import BenchmarkWrapper\n\n        # Initialize the BenchmarkWrapper\n        benchmarker = BenchmarkWrapper(\n            task=\"pocketclosure\",\n            encodings=[\"one_hot\", \"target\"],\n            learners=[\"rf\", \"xgb\", \"lr\", \"mlp\"],\n            tuning_methods=[\"holdout\", \"cv\"],\n            hpo_methods=[\"rs\", \"hebo\"],\n            criteria=[\"f1\", \"brier_score\"],\n            sampling=[\"upsampling\"],\n            factor=2,\n            path=\"/data/processed/processed_data.csv\",\n        )\n\n        # Run baseline benchmarking\n        baseline_df = benchmarker.baseline()\n\n        # Run full benchmark and retrieve results\n        benchmark, learners = benchmarker.wrapped_benchmark()\n\n        # Save the benchmark results\n        benchmarker.save_benchmark(\n            benchmark_df=benchmark,\n            path=\"reports/experiment/benchmark.csv\",\n        )\n\n        # Save the trained learners\n        benchmarker.save_learners(learners_dict=learners, path=\"models/experiment\")\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        learners: List[str],\n        tuning_methods: List[str],\n        hpo_methods: List[str],\n        criteria: List[str],\n        encodings: List[str],\n        sampling: Optional[List[Union[str, None]]] = None,\n        factor: Optional[float] = None,\n        n_configs: int = 10,\n        n_jobs: int = 1,\n        cv_folds: Optional[int] = 10,\n        racing_folds: Optional[int] = None,\n        test_seed: int = 0,\n        test_size: float = 0.2,\n        val_size: Optional[float] = 0.2,\n        cv_seed: Optional[int] = 0,\n        mlp_flag: Optional[bool] = None,\n        threshold_tuning: Optional[bool] = None,\n        verbose: bool = False,\n        path: Path = Path(\"data/processed/processed_data.csv\"),\n    ) -&gt; None:\n        \"\"\"Initializes the BenchmarkWrapper.\n\n        Args:\n            task (str): Task for evaluation ('pocketclosure', 'pocketclosureinf',\n                'improvement', or 'pdgrouprevaluation'.).\n            learners (List[str]): List of learners to benchmark ('xgb', 'rf', 'lr' or\n                'mlp').\n            tuning_methods (List[str]): Tuning methods for each learner\n                ('holdout', 'cv').\n            hpo_methods (List[str]): HPO methods ('hebo' or 'rs').\n            criteria (List[str]): List of evaluation criteria ('f1', 'macro_f1',\n                'brier_score').\n            encodings (List[str]): List of encodings ('one_hot' or 'target').\n            sampling (Optional[List[str]]): Sampling strategies to handle class\n                imbalance. Includes None, 'upsampling', 'downsampling', and 'smote'.\n            factor (Optional[float]): Factor to apply during resampling.\n            n_configs (int): Number of configurations for hyperparameter tuning.\n                Defaults to 10.\n            n_jobs (int): Number of parallel jobs for processing.\n            cv_folds (int): Number of folds for cross-validation. Defaults to 10.\n            racing_folds (Optional[int]): Number of racing folds for Random Search (RS).\n                Defaults to None.\n            test_seed (int): Random seed for test splitting. Defaults to 0.\n            test_size (float): Proportion of data used for testing. Defaults to\n                0.2.\n            val_size (Optional[float]): Size of validation set in holdout tuning.\n                Defaults to 0.2.\n            cv_seed (int): Random seed for cross-validation. Defaults to 0\n            mlp_flag (Optional[bool]): Enables MLP training with early stopping.\n                Defaults to True.\n            threshold_tuning (Optional[bool]): Enables threshold tuning for binary\n                classification. Defaults to None.\n            verbose (bool): If True, enables detailed logging during benchmarking.\n                Defaults to False.\n            path (Path): Path to the directory containing processed data files.\n                Defaults to Path(\"data/processed\").\n        \"\"\"\n        super().__init__(\n            task=task,\n            learners=learners,\n            tuning_methods=tuning_methods,\n            hpo_methods=hpo_methods,\n            criteria=criteria,\n            encodings=encodings,\n            sampling=sampling,\n            factor=factor,\n            n_configs=n_configs,\n            n_jobs=n_jobs,\n            cv_folds=cv_folds,\n            racing_folds=racing_folds,\n            test_seed=test_seed,\n            test_size=test_size,\n            val_size=val_size,\n            cv_seed=cv_seed,\n            mlp_flag=mlp_flag,\n            threshold_tuning=threshold_tuning,\n            verbose=verbose,\n            path=path,\n        )\n        self.classification = \"multiclass\" if task == \"pdgrouprevaluation\" else \"binary\"\n\n    def baseline(self) -&gt; pd.DataFrame:\n        \"\"\"Runs baseline benchmark for each encoding type.\n\n        Returns:\n            pd.DataFrame: Combined baseline benchmark dataframe with encoding info.\n        \"\"\"\n        baseline_dfs = []\n\n        for encoding in self.encodings:\n            baseline_df = Baseline(\n                task=self.task,\n                encoding=encoding,\n                path=self.path,\n                random_state=self.test_seed,\n            ).baseline()\n            baseline_df[\"Encoding\"] = encoding\n            baseline_dfs.append(baseline_df)\n\n        combined_baseline_df = pd.concat(baseline_dfs, ignore_index=True)\n        column_order = [\"Model\", \"Encoding\"] + [\n            col\n            for col in combined_baseline_df.columns\n            if col not in [\"Model\", \"Encoding\"]\n        ]\n        combined_baseline_df = combined_baseline_df[column_order]\n\n        return combined_baseline_df\n\n    def wrapped_benchmark(self) -&gt; Tuple[pd.DataFrame, dict]:\n        \"\"\"Runs baseline and benchmarking tasks.\n\n        Returns:\n            Tuple: Benchmark and learners used for evaluation.\n        \"\"\"\n        benchmarker = Benchmarker(\n            task=self.task,\n            learners=self.learners,\n            tuning_methods=self.tuning_methods,\n            hpo_methods=self.hpo_methods,\n            criteria=self.criteria,\n            encodings=self.encodings,\n            sampling=self.sampling,\n            factor=self.factor,\n            n_configs=self.n_configs,\n            n_jobs=self.n_jobs,\n            cv_folds=self.cv_folds,\n            test_size=self.test_size,\n            val_size=self.val_size,\n            test_seed=self.test_seed,\n            cv_seed=self.cv_seed,\n            mlp_flag=self.mlp_flag,\n            threshold_tuning=self.threshold_tuning,\n            verbose=self.verbose,\n            path=self.path,\n        )\n\n        return benchmarker.run_benchmarks()\n\n    @staticmethod\n    def save_benchmark(benchmark_df: pd.DataFrame, path: Union[str, Path]) -&gt; None:\n        \"\"\"Saves the benchmark DataFrame to the specified path as a CSV file.\n\n        Args:\n            benchmark_df (pd.DataFrame): The benchmark DataFrame to save.\n            path (Union[str, Path]): Path (including filename) where CSV file will be\n                saved.\n\n        Raises:\n            ValueError: If the benchmark DataFrame is empty.\n        \"\"\"\n        path = Path(path)\n        if not path.is_absolute():\n            path = Path.cwd() / path\n\n        if benchmark_df.empty:\n            raise ValueError(\"Benchmark DataFrame is empty and cannot be saved.\")\n\n        if not path.suffix == \".csv\":\n            path = path.with_suffix(\".csv\")\n\n        os.makedirs(path.parent, exist_ok=True)\n        benchmark_df.to_csv(path, index=False)\n        print(f\"Saved benchmark report to {path}\")\n\n    @staticmethod\n    def save_learners(learners_dict: dict, path: Union[str, Path]) -&gt; None:\n        \"\"\"Saves the learners to the specified directory.\n\n        Args:\n            learners_dict (dict): Dictionary containing learners to save.\n            path (Union[str, Path]): Path to the directory where models will be saved.\n\n        Raises:\n            ValueError: If the learners dictionary is empty.\n        \"\"\"\n        path = Path(path)\n        if not path.is_absolute():\n            path = Path.cwd() / path\n\n        if not learners_dict:\n            raise ValueError(\"Learners dictionary is empty and cannot be saved.\")\n\n        os.makedirs(path, exist_ok=True)\n\n        for model_name, model in learners_dict.items():\n            model_file_name = f\"{model_name}.pkl\"\n            model_path = path / model_file_name\n            joblib.dump(model, model_path)\n            print(f\"Saved model {model_name} to {model_path}\")\n</code></pre>"},{"location":"reference/wrapper/benchmarkwrapper/#periomod.wrapper.BenchmarkWrapper.__init__","title":"<code>__init__(task, learners, tuning_methods, hpo_methods, criteria, encodings, sampling=None, factor=None, n_configs=10, n_jobs=1, cv_folds=10, racing_folds=None, test_seed=0, test_size=0.2, val_size=0.2, cv_seed=0, mlp_flag=None, threshold_tuning=None, verbose=False, path=Path('data/processed/processed_data.csv'))</code>","text":"<p>Initializes the BenchmarkWrapper.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task for evaluation ('pocketclosure', 'pocketclosureinf', 'improvement', or 'pdgrouprevaluation'.).</p> required <code>learners</code> <code>List[str]</code> <p>List of learners to benchmark ('xgb', 'rf', 'lr' or 'mlp').</p> required <code>tuning_methods</code> <code>List[str]</code> <p>Tuning methods for each learner ('holdout', 'cv').</p> required <code>hpo_methods</code> <code>List[str]</code> <p>HPO methods ('hebo' or 'rs').</p> required <code>criteria</code> <code>List[str]</code> <p>List of evaluation criteria ('f1', 'macro_f1', 'brier_score').</p> required <code>encodings</code> <code>List[str]</code> <p>List of encodings ('one_hot' or 'target').</p> required <code>sampling</code> <code>Optional[List[str]]</code> <p>Sampling strategies to handle class imbalance. Includes None, 'upsampling', 'downsampling', and 'smote'.</p> <code>None</code> <code>factor</code> <code>Optional[float]</code> <p>Factor to apply during resampling.</p> <code>None</code> <code>n_configs</code> <code>int</code> <p>Number of configurations for hyperparameter tuning. Defaults to 10.</p> <code>10</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for processing.</p> <code>1</code> <code>cv_folds</code> <code>int</code> <p>Number of folds for cross-validation. Defaults to 10.</p> <code>10</code> <code>racing_folds</code> <code>Optional[int]</code> <p>Number of racing folds for Random Search (RS). Defaults to None.</p> <code>None</code> <code>test_seed</code> <code>int</code> <p>Random seed for test splitting. Defaults to 0.</p> <code>0</code> <code>test_size</code> <code>float</code> <p>Proportion of data used for testing. Defaults to 0.2.</p> <code>0.2</code> <code>val_size</code> <code>Optional[float]</code> <p>Size of validation set in holdout tuning. Defaults to 0.2.</p> <code>0.2</code> <code>cv_seed</code> <code>int</code> <p>Random seed for cross-validation. Defaults to 0</p> <code>0</code> <code>mlp_flag</code> <code>Optional[bool]</code> <p>Enables MLP training with early stopping. Defaults to True.</p> <code>None</code> <code>threshold_tuning</code> <code>Optional[bool]</code> <p>Enables threshold tuning for binary classification. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, enables detailed logging during benchmarking. Defaults to False.</p> <code>False</code> <code>path</code> <code>Path</code> <p>Path to the directory containing processed data files. Defaults to Path(\"data/processed\").</p> <code>Path('data/processed/processed_data.csv')</code> Source code in <code>periomod/wrapper/_wrapper.py</code> <pre><code>def __init__(\n    self,\n    task: str,\n    learners: List[str],\n    tuning_methods: List[str],\n    hpo_methods: List[str],\n    criteria: List[str],\n    encodings: List[str],\n    sampling: Optional[List[Union[str, None]]] = None,\n    factor: Optional[float] = None,\n    n_configs: int = 10,\n    n_jobs: int = 1,\n    cv_folds: Optional[int] = 10,\n    racing_folds: Optional[int] = None,\n    test_seed: int = 0,\n    test_size: float = 0.2,\n    val_size: Optional[float] = 0.2,\n    cv_seed: Optional[int] = 0,\n    mlp_flag: Optional[bool] = None,\n    threshold_tuning: Optional[bool] = None,\n    verbose: bool = False,\n    path: Path = Path(\"data/processed/processed_data.csv\"),\n) -&gt; None:\n    \"\"\"Initializes the BenchmarkWrapper.\n\n    Args:\n        task (str): Task for evaluation ('pocketclosure', 'pocketclosureinf',\n            'improvement', or 'pdgrouprevaluation'.).\n        learners (List[str]): List of learners to benchmark ('xgb', 'rf', 'lr' or\n            'mlp').\n        tuning_methods (List[str]): Tuning methods for each learner\n            ('holdout', 'cv').\n        hpo_methods (List[str]): HPO methods ('hebo' or 'rs').\n        criteria (List[str]): List of evaluation criteria ('f1', 'macro_f1',\n            'brier_score').\n        encodings (List[str]): List of encodings ('one_hot' or 'target').\n        sampling (Optional[List[str]]): Sampling strategies to handle class\n            imbalance. Includes None, 'upsampling', 'downsampling', and 'smote'.\n        factor (Optional[float]): Factor to apply during resampling.\n        n_configs (int): Number of configurations for hyperparameter tuning.\n            Defaults to 10.\n        n_jobs (int): Number of parallel jobs for processing.\n        cv_folds (int): Number of folds for cross-validation. Defaults to 10.\n        racing_folds (Optional[int]): Number of racing folds for Random Search (RS).\n            Defaults to None.\n        test_seed (int): Random seed for test splitting. Defaults to 0.\n        test_size (float): Proportion of data used for testing. Defaults to\n            0.2.\n        val_size (Optional[float]): Size of validation set in holdout tuning.\n            Defaults to 0.2.\n        cv_seed (int): Random seed for cross-validation. Defaults to 0\n        mlp_flag (Optional[bool]): Enables MLP training with early stopping.\n            Defaults to True.\n        threshold_tuning (Optional[bool]): Enables threshold tuning for binary\n            classification. Defaults to None.\n        verbose (bool): If True, enables detailed logging during benchmarking.\n            Defaults to False.\n        path (Path): Path to the directory containing processed data files.\n            Defaults to Path(\"data/processed\").\n    \"\"\"\n    super().__init__(\n        task=task,\n        learners=learners,\n        tuning_methods=tuning_methods,\n        hpo_methods=hpo_methods,\n        criteria=criteria,\n        encodings=encodings,\n        sampling=sampling,\n        factor=factor,\n        n_configs=n_configs,\n        n_jobs=n_jobs,\n        cv_folds=cv_folds,\n        racing_folds=racing_folds,\n        test_seed=test_seed,\n        test_size=test_size,\n        val_size=val_size,\n        cv_seed=cv_seed,\n        mlp_flag=mlp_flag,\n        threshold_tuning=threshold_tuning,\n        verbose=verbose,\n        path=path,\n    )\n    self.classification = \"multiclass\" if task == \"pdgrouprevaluation\" else \"binary\"\n</code></pre>"},{"location":"reference/wrapper/benchmarkwrapper/#periomod.wrapper.BenchmarkWrapper.baseline","title":"<code>baseline()</code>","text":"<p>Runs baseline benchmark for each encoding type.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Combined baseline benchmark dataframe with encoding info.</p> Source code in <code>periomod/wrapper/_wrapper.py</code> <pre><code>def baseline(self) -&gt; pd.DataFrame:\n    \"\"\"Runs baseline benchmark for each encoding type.\n\n    Returns:\n        pd.DataFrame: Combined baseline benchmark dataframe with encoding info.\n    \"\"\"\n    baseline_dfs = []\n\n    for encoding in self.encodings:\n        baseline_df = Baseline(\n            task=self.task,\n            encoding=encoding,\n            path=self.path,\n            random_state=self.test_seed,\n        ).baseline()\n        baseline_df[\"Encoding\"] = encoding\n        baseline_dfs.append(baseline_df)\n\n    combined_baseline_df = pd.concat(baseline_dfs, ignore_index=True)\n    column_order = [\"Model\", \"Encoding\"] + [\n        col\n        for col in combined_baseline_df.columns\n        if col not in [\"Model\", \"Encoding\"]\n    ]\n    combined_baseline_df = combined_baseline_df[column_order]\n\n    return combined_baseline_df\n</code></pre>"},{"location":"reference/wrapper/benchmarkwrapper/#periomod.wrapper.BenchmarkWrapper.save_benchmark","title":"<code>save_benchmark(benchmark_df, path)</code>  <code>staticmethod</code>","text":"<p>Saves the benchmark DataFrame to the specified path as a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_df</code> <code>DataFrame</code> <p>The benchmark DataFrame to save.</p> required <code>path</code> <code>Union[str, Path]</code> <p>Path (including filename) where CSV file will be saved.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the benchmark DataFrame is empty.</p> Source code in <code>periomod/wrapper/_wrapper.py</code> <pre><code>@staticmethod\ndef save_benchmark(benchmark_df: pd.DataFrame, path: Union[str, Path]) -&gt; None:\n    \"\"\"Saves the benchmark DataFrame to the specified path as a CSV file.\n\n    Args:\n        benchmark_df (pd.DataFrame): The benchmark DataFrame to save.\n        path (Union[str, Path]): Path (including filename) where CSV file will be\n            saved.\n\n    Raises:\n        ValueError: If the benchmark DataFrame is empty.\n    \"\"\"\n    path = Path(path)\n    if not path.is_absolute():\n        path = Path.cwd() / path\n\n    if benchmark_df.empty:\n        raise ValueError(\"Benchmark DataFrame is empty and cannot be saved.\")\n\n    if not path.suffix == \".csv\":\n        path = path.with_suffix(\".csv\")\n\n    os.makedirs(path.parent, exist_ok=True)\n    benchmark_df.to_csv(path, index=False)\n    print(f\"Saved benchmark report to {path}\")\n</code></pre>"},{"location":"reference/wrapper/benchmarkwrapper/#periomod.wrapper.BenchmarkWrapper.save_learners","title":"<code>save_learners(learners_dict, path)</code>  <code>staticmethod</code>","text":"<p>Saves the learners to the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>learners_dict</code> <code>dict</code> <p>Dictionary containing learners to save.</p> required <code>path</code> <code>Union[str, Path]</code> <p>Path to the directory where models will be saved.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the learners dictionary is empty.</p> Source code in <code>periomod/wrapper/_wrapper.py</code> <pre><code>@staticmethod\ndef save_learners(learners_dict: dict, path: Union[str, Path]) -&gt; None:\n    \"\"\"Saves the learners to the specified directory.\n\n    Args:\n        learners_dict (dict): Dictionary containing learners to save.\n        path (Union[str, Path]): Path to the directory where models will be saved.\n\n    Raises:\n        ValueError: If the learners dictionary is empty.\n    \"\"\"\n    path = Path(path)\n    if not path.is_absolute():\n        path = Path.cwd() / path\n\n    if not learners_dict:\n        raise ValueError(\"Learners dictionary is empty and cannot be saved.\")\n\n    os.makedirs(path, exist_ok=True)\n\n    for model_name, model in learners_dict.items():\n        model_file_name = f\"{model_name}.pkl\"\n        model_path = path / model_file_name\n        joblib.dump(model, model_path)\n        print(f\"Saved model {model_name} to {model_path}\")\n</code></pre>"},{"location":"reference/wrapper/benchmarkwrapper/#periomod.wrapper.BenchmarkWrapper.wrapped_benchmark","title":"<code>wrapped_benchmark()</code>","text":"<p>Runs baseline and benchmarking tasks.</p> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[DataFrame, dict]</code> <p>Benchmark and learners used for evaluation.</p> Source code in <code>periomod/wrapper/_wrapper.py</code> <pre><code>def wrapped_benchmark(self) -&gt; Tuple[pd.DataFrame, dict]:\n    \"\"\"Runs baseline and benchmarking tasks.\n\n    Returns:\n        Tuple: Benchmark and learners used for evaluation.\n    \"\"\"\n    benchmarker = Benchmarker(\n        task=self.task,\n        learners=self.learners,\n        tuning_methods=self.tuning_methods,\n        hpo_methods=self.hpo_methods,\n        criteria=self.criteria,\n        encodings=self.encodings,\n        sampling=self.sampling,\n        factor=self.factor,\n        n_configs=self.n_configs,\n        n_jobs=self.n_jobs,\n        cv_folds=self.cv_folds,\n        test_size=self.test_size,\n        val_size=self.val_size,\n        test_seed=self.test_seed,\n        cv_seed=self.cv_seed,\n        mlp_flag=self.mlp_flag,\n        threshold_tuning=self.threshold_tuning,\n        verbose=self.verbose,\n        path=self.path,\n    )\n\n    return benchmarker.run_benchmarks()\n</code></pre>"},{"location":"reference/wrapper/evaluatorwrapper/","title":"EvaluatorWrapper","text":"<p>               Bases: <code>BaseEvaluatorWrapper</code></p> <p>Wrapper class for model evaluation, feature importance, and inference.</p> <p>Extends the base evaluation functionality to enable comprehensive model evaluation, feature importance analysis, patient inference, and jackknife resampling for confidence interval estimation.</p> Inherits <ul> <li><code>BaseEvaluatorWrapper</code>: Provides foundational methods and attributes for   model evaluation, data preparation, and inference.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>learners_dict</code> <code>Dict</code> <p>Dictionary containing trained models and their metadata.</p> required <code>criterion</code> <code>str</code> <p>The criterion used to select the best model ('f1', 'macro_f1', 'brier_score').</p> required <code>aggregate</code> <code>bool</code> <p>Whether to aggregate one-hot encoding. Defaults to True.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>If True, enables verbose logging during evaluation and inference. Defaults to False.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Random state for resampling. Defaults to 0.</p> <code>0</code> <code>test_size</code> <code>float</code> <p>Size of grouped train test split.     Defaults to 0.2.</p> <code>0.2</code> <code>path</code> <code>Path</code> <p>Path to the directory containing processed data files. Defaults to Path(\"data/processed/processed_data.csv\").</p> <code>Path('data/processed/processed_data.csv')</code> <p>Attributes:</p> Name Type Description <code>learners_dict</code> <code>Dict</code> <p>Contains metadata about trained models.</p> <code>criterion</code> <code>str</code> <p>Criterion used for model selection.</p> <code>aggregate</code> <code>bool</code> <p>Flag for aggregating one-hot encoded metrics.</p> <code>verbose</code> <code>bool</code> <p>Controls verbose in evaluation processes.</p> <code>test_size</code> <code>float</code> <p>Size of grouped train test split.</p> <code>model</code> <code>object</code> <p>Best-ranked model based on the criterion.</p> <code>encoding</code> <code>str</code> <p>Encoding method ('one_hot' or 'target').</p> <code>learner</code> <code>str</code> <p>Type of model (learner) used in training.</p> <code>task</code> <code>str</code> <p>Task associated with the extracted model.</p> <code>factor</code> <code>Optional[float]</code> <p>Resampling factor if applicable.</p> <code>sampling</code> <code>Optional[str]</code> <p>Resampling strategy ('upsampling', 'smote', etc.).</p> <code>classification</code> <code>str</code> <p>Classification type ('binary' or 'multiclass').</p> <code>dataloader</code> <code>ProcessedDataLoader</code> <p>Data loader and transformer.</p> <code>resampler</code> <code>Resampler</code> <p>Resampling strategy for training and testing.</p> <code>df</code> <code>DataFrame</code> <p>Loaded dataset.</p> <code>df_processed</code> <code>DataFrame</code> <p>Processed dataset.</p> <code>train_df</code> <code>DataFrame</code> <p>Training data after splitting.</p> <code>test_df</code> <code>DataFrame</code> <p>Test data after splitting.</p> <code>X_train</code> <code>DataFrame</code> <p>Training features.</p> <code>y_train</code> <code>Series</code> <p>Training labels.</p> <code>X_test</code> <code>DataFrame</code> <p>Test features.</p> <code>y_test</code> <code>Series</code> <p>Test labels.</p> <code>base_target</code> <code>Optional[ndarray]</code> <p>Baseline target for evaluations.</p> <code>baseline</code> <code>Baseline</code> <p>Basline class for model analysis.</p> <code>evaluator</code> <code>ModelEvaluator</code> <p>Evaluator for model metrics and feature importance.</p> <code>inference_engine</code> <code>ModelInference</code> <p>Model inference manager.</p> <code>trainer</code> <code>Trainer</code> <p>Trainer for model evaluation and optimization.</p> <p>Methods:</p> Name Description <code>wrapped_evaluation</code> <p>Runs comprehensive evaluation with optional plots for metrics such as confusion matrix and Brier scores.</p> <code>evaluate_cluster</code> <p>Performs clustering and calculates Brier scores. Allows subsetting of test set.</p> <code>evaluate_feature_importance</code> <p>Computes feature importance using specified methods (e.g., SHAP, permutation importance). Allows subsetting of test set.</p> <code>average_over_splits</code> <p>Aggregates metrics across multiple data splits for robust evaluation.</p> <code>wrapped_patient_inference</code> <p>Conducts inference on individual patient data.</p> <code>wrapped_jackknife</code> <p>Executes jackknife resampling on patient data to estimate confidence intervals.</p> Inherited Properties <ul> <li><code>criterion (str):</code> Retrieves or sets current evaluation criterion for model     selection. Supports 'f1', 'brier_score', and 'macro_f1'.</li> <li><code>model (object):</code> Retrieves best-ranked model dynamically based on the current     criterion. Recalculates when criterion is updated.</li> </ul> <p>Examples:</p> <pre><code>from periomod.base import Patient, patient_to_dataframe\nfrom periomod.wrapper import EvaluatorWrapper, load_benchmark, load_learners\n\nbenchmark = load_benchmark(path=\"reports/experiment/benchmark.csv\")\nlearners = load_learners(path=\"models/experiments\")\n\n# Initialize evaluator with learners from BenchmarkWrapper and f1 criterion\nevaluator = EvaluatorWrapper(\n    learners_dict=learners,\n    criterion=\"f1\",\n    path=\"data/processed/processed_data.csv\"\n)\n\n# Evaluate the model and generate plots\nevaluator.wrapped_evaluation()\n\n# Cluster analysis on predictions with brier score smaller than threshold\nevaluator.evaluate_cluster(brier_threshold=0.15)\n\n# Calculate feature importance\nevaluator.evaluate_feature_importance(fi_types=[\"shap\", \"permutation\"])\n\n# Train and average over multiple random splits\navg_metrics_df = evaluator.average_over_splits(num_splits=5, n_jobs=-1)\n\n# Define a patient instance\npatient = Patient()\npatient_df = patient_to_df(patient=patient)\n\n# Run inference on a specific patient's data\npredict_data, output, results = evaluator.wrapped_patient_inference(\n    patient=patient\n    )\n\n# Execute jackknife resampling for robust inference\njackknife_results, ci_plots = evaluator.wrapped_jackknife(\n    patient=my_patient, results=results_df, sample_fraction=0.8, n_jobs=-1\n)\n</code></pre> Source code in <code>periomod/wrapper/_wrapper.py</code> <pre><code>class EvaluatorWrapper(BaseEvaluatorWrapper):\n    \"\"\"Wrapper class for model evaluation, feature importance, and inference.\n\n    Extends the base evaluation functionality to enable comprehensive model\n    evaluation, feature importance analysis, patient inference, and jackknife\n    resampling for confidence interval estimation.\n\n    Inherits:\n        - `BaseEvaluatorWrapper`: Provides foundational methods and attributes for\n          model evaluation, data preparation, and inference.\n\n    Args:\n        learners_dict (Dict): Dictionary containing trained models and their metadata.\n        criterion (str): The criterion used to select the best model ('f1', 'macro_f1',\n            'brier_score').\n        aggregate (bool): Whether to aggregate one-hot encoding. Defaults\n            to True.\n        verbose (bool): If True, enables verbose logging during evaluation\n            and inference. Defaults to False.\n        random_state (int): Random state for resampling. Defaults to 0.\n        test_size (float): Size of grouped train test split.\n                Defaults to 0.2.\n        path (Path): Path to the directory containing processed data files.\n            Defaults to Path(\"data/processed/processed_data.csv\").\n\n    Attributes:\n        learners_dict (Dict): Contains metadata about trained models.\n        criterion (str): Criterion used for model selection.\n        aggregate (bool): Flag for aggregating one-hot encoded metrics.\n        verbose (bool): Controls verbose in evaluation processes.\n        test_size (float): Size of grouped train test split.\n        model (object): Best-ranked model based on the criterion.\n        encoding (str): Encoding method ('one_hot' or 'target').\n        learner (str): Type of model (learner) used in training.\n        task (str): Task associated with the extracted model.\n        factor (Optional[float]): Resampling factor if applicable.\n        sampling (Optional[str]): Resampling strategy ('upsampling', 'smote', etc.).\n        classification (str): Classification type ('binary' or 'multiclass').\n        dataloader (ProcessedDataLoader): Data loader and transformer.\n        resampler (Resampler): Resampling strategy for training and testing.\n        df (pd.DataFrame): Loaded dataset.\n        df_processed (pd.DataFrame): Processed dataset.\n        train_df (pd.DataFrame): Training data after splitting.\n        test_df (pd.DataFrame): Test data after splitting.\n        X_train (pd.DataFrame): Training features.\n        y_train (pd.Series): Training labels.\n        X_test (pd.DataFrame): Test features.\n        y_test (pd.Series): Test labels.\n        base_target (Optional[np.ndarray]): Baseline target for evaluations.\n        baseline (Baseline): Basline class for model analysis.\n        evaluator (ModelEvaluator): Evaluator for model metrics and feature importance.\n        inference_engine (ModelInference): Model inference manager.\n        trainer (Trainer): Trainer for model evaluation and optimization.\n\n    Methods:\n        wrapped_evaluation: Runs comprehensive evaluation with optional\n            plots for metrics such as confusion matrix and Brier scores.\n        evaluate_cluster: Performs clustering and calculates Brier scores.\n            Allows subsetting of test set.\n        evaluate_feature_importance: Computes feature importance using\n            specified methods (e.g., SHAP, permutation importance). Allows subsetting\n            of test set.\n        average_over_splits: Aggregates metrics across multiple data\n            splits for robust evaluation.\n        wrapped_patient_inference: Conducts inference on individual patient data.\n        wrapped_jackknife: Executes jackknife resampling on patient data to\n            estimate confidence intervals.\n\n    Inherited Properties:\n        - `criterion (str):` Retrieves or sets current evaluation criterion for model\n            selection. Supports 'f1', 'brier_score', and 'macro_f1'.\n        - `model (object):` Retrieves best-ranked model dynamically based on the current\n            criterion. Recalculates when criterion is updated.\n\n    Examples:\n        ```\n        from periomod.base import Patient, patient_to_dataframe\n        from periomod.wrapper import EvaluatorWrapper, load_benchmark, load_learners\n\n        benchmark = load_benchmark(path=\"reports/experiment/benchmark.csv\")\n        learners = load_learners(path=\"models/experiments\")\n\n        # Initialize evaluator with learners from BenchmarkWrapper and f1 criterion\n        evaluator = EvaluatorWrapper(\n            learners_dict=learners,\n            criterion=\"f1\",\n            path=\"data/processed/processed_data.csv\"\n        )\n\n        # Evaluate the model and generate plots\n        evaluator.wrapped_evaluation()\n\n        # Cluster analysis on predictions with brier score smaller than threshold\n        evaluator.evaluate_cluster(brier_threshold=0.15)\n\n        # Calculate feature importance\n        evaluator.evaluate_feature_importance(fi_types=[\"shap\", \"permutation\"])\n\n        # Train and average over multiple random splits\n        avg_metrics_df = evaluator.average_over_splits(num_splits=5, n_jobs=-1)\n\n        # Define a patient instance\n        patient = Patient()\n        patient_df = patient_to_df(patient=patient)\n\n        # Run inference on a specific patient's data\n        predict_data, output, results = evaluator.wrapped_patient_inference(\n            patient=patient\n            )\n\n        # Execute jackknife resampling for robust inference\n        jackknife_results, ci_plots = evaluator.wrapped_jackknife(\n            patient=my_patient, results=results_df, sample_fraction=0.8, n_jobs=-1\n        )\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        learners_dict: Dict,\n        criterion: str,\n        aggregate: bool = True,\n        verbose: bool = False,\n        random_state: int = 0,\n        test_size: float = 0.2,\n        path: Path = Path(\"data/processed/processed_data.csv\"),\n    ) -&gt; None:\n        \"\"\"Initializes EvaluatorWrapper with model, evaluation, and inference setup.\n\n        Args:\n            learners_dict (Dict): Dictionary containing trained models.\n            criterion (str): The criterion used to select the best model ('f1',\n                'macro_f1', 'brier_score').\n            aggregate (bool): Whether to aggregate one-hot encoding. Defaults\n                to True.\n            verbose (bool): If True, enables verbose logging during evaluation\n                and inference. Defaults to False.\n            random_state (int): Random state for resampling. Defaults to 0\n            test_size (float): Size of grouped train test split. Defaults to 0.2.\n            path (Path): Path to the directory containing processed data files.\n                Defaults to Path(\"data/processed/processed_data.csv\").\n\n        \"\"\"\n        super().__init__(\n            learners_dict=learners_dict,\n            criterion=criterion,\n            aggregate=aggregate,\n            verbose=verbose,\n            random_state=random_state,\n            test_size=test_size,\n            path=path,\n        )\n\n    def wrapped_evaluation(\n        self,\n        cm: bool = True,\n        cm_normalization: str = \"rows\",\n        cm_base: bool = True,\n        brier_groups: bool = True,\n        calibration: bool = True,\n        tight_layout: bool = False,\n        save: bool = False,\n        name: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Runs evaluation on the best-ranked model.\n\n        Args:\n            cm (bool): Plot the confusion matrix. Defaults to True.\n            cm_normalization (str): Normalization for confusion matrix.\n                Defaults to \"rows\".\n            cm_base (bool): Plot confusion matrix vs value before treatment.\n                Defaults to True.\n            brier_groups (bool): Calculate Brier score groups. Defaults to True.\n            calibration (bool): Plots model calibration. Defaults to True.\n            tight_layout (bool): If True, applies tight layout to the plot.\n                Defaults to False.\n            save (bool): If True, saves the evaluation plots. Defaults to False.\n            name (Optional[str]): Name for the saved evaluation plots.\n        \"\"\"\n        if cm:\n            self.evaluator.plot_confusion_matrix(\n                tight_layout=tight_layout,\n                normalize=cm_normalization,\n                task=self.task,\n                save=save,\n                name=(\n                    f\"{name}_cm_predictionEval\"\n                    if name is not None\n                    else \"cm_predictionEval\"\n                ),\n            )\n        if cm_base:\n            if self.task in [\n                \"pocketclosure\",\n                \"pocketclosureinf\",\n                \"pdgrouprevaluation\",\n            ]:\n                self.evaluator.plot_confusion_matrix(\n                    col=self.base_target,\n                    y_label=\"Baseline PPD\",\n                    tight_layout=tight_layout,\n                    task=self.task,\n                    save=save,\n                    name=(\n                        f\"{name}_cm_baseToPredicted\"\n                        if name is not None\n                        else \"cm_baseToPredicted\"\n                    ),\n                )\n\n        if brier_groups:\n            self.evaluator.brier_score_groups(\n                tight_layout=tight_layout, task=self.task, save=save, name=name\n            )\n        if calibration:\n            self.evaluator.calibration_plot(\n                task=self.task, tight_layout=tight_layout, save=save, name=name\n            )\n\n    def compare_bss(\n        self,\n        base: Optional[str] = None,\n        revaluation: Optional[str] = None,\n        true_preds: bool = False,\n        brier_threshold: Optional[float] = None,\n        tight_layout: bool = False,\n        save: bool = False,\n        name: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Compares Brier Skill Score of model with baseline on test set.\n\n        Args:\n            base (Optional[str]): Baseline variable for comparison. Defaults to None.\n            revaluation (Optional[str]): Revaluation variable. Defaults to None.\n            true_preds (bool): Subset by correct predictions. Defaults to False.\n            brier_threshold (Optional[float]): Filters observations ny Brier score\n                threshold. Defaults to None.\n            tight_layout (bool): If True, applies tight layout to the plot.\n                Defaults to False.\n            save (bool): If True, saves the BSS comparison plot. Defaults to False.\n            name (Optional[str]): Name for the saved BSS comparison plot.\n        \"\"\"\n        baseline_models, _, _ = self.baseline.train_baselines()\n        self.evaluator.X, self.evaluator.y, patients = self._test_filters(\n            X=self.evaluator.X,\n            y=self.evaluator.y,\n            base=base,\n            revaluation=revaluation,\n            true_preds=true_preds,\n            brier_threshold=brier_threshold,\n        )\n        self.evaluator.bss_comparison(\n            baseline_models=baseline_models,\n            classification=self.classification,\n            num_patients=patients,\n            tight_layout=tight_layout,\n            save=save,\n            name=name,\n        )\n        self.evaluator.X, self.evaluator.y = self.X_test, self.y_test\n\n    def evaluate_cluster(\n        self,\n        n_cluster: int = 3,\n        base: Optional[str] = None,\n        revaluation: Optional[str] = None,\n        true_preds: bool = False,\n        brier_threshold: Optional[float] = None,\n        tight_layout: bool = False,\n    ) -&gt; None:\n        \"\"\"Performs cluster analysis with Brier scores, optionally applying subsetting.\n\n        This method allows detailed feature analysis by offering multiple subsetting\n        options for the test set. The base and revaluation columns allow filtering of\n        observations that have not changed after treatment. With true_preds, only\n        observations that were correctly predicted are considered. The brier_threshold\n        enables filtering of observations that achieved a smaller Brier score at\n        prediction time than the threshold.\n\n        Args:\n            n_cluster (int): Number of clusters for Brier score clustering analysis.\n                Defaults to 3.\n            base (Optional[str]): Baseline variable for comparison. Defaults to None.\n            revaluation (Optional[str]): Revaluation variable. Defaults to None.\n            true_preds (bool): Subset by correct predictions. Defaults to False.\n            brier_threshold (Optional[float]): Filters observations ny Brier score\n                threshold. Defaults to None.\n            tight_layout (bool): If True, applies tight layout to the plot.\n                Defaults to False.\n        \"\"\"\n        self.evaluator.X, self.evaluator.y, patients = self._test_filters(\n            X=self.evaluator.X,\n            y=self.evaluator.y,\n            base=base,\n            revaluation=revaluation,\n            true_preds=true_preds,\n            brier_threshold=brier_threshold,\n        )\n        print(f\"Number of patients in test set: {patients}\")\n        print(f\"Number of tooth sites: {len(self.evaluator.y)}\")\n        self.evaluator.analyze_brier_within_clusters(\n            n_clusters=n_cluster,\n            tight_layout=tight_layout,\n        )\n        self.evaluator.X, self.evaluator.y = self.X_test, self.y_test\n\n    def evaluate_feature_importance(\n        self,\n        fi_types: List[str],\n        base: Optional[str] = None,\n        revaluation: Optional[str] = None,\n        true_preds: bool = False,\n        brier_threshold: Optional[float] = None,\n        save: bool = False,\n        name: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Evaluates feature importance using the evaluator, with optional subsetting.\n\n        This method allows detailed feature analysis by offering multiple subsetting\n        options for the test set. The base and revaluation columns allow filtering of\n        observations that have not changed after treatment. With true_preds, only\n        observations that were correctly predicted are considered. The brier_threshold\n        enables filtering of observations that achieved a smaller Brier score at\n        prediction time than the threshold.\n\n        Args:\n            fi_types (List[str]): List of feature importance types to evaluate.\n            base (Optional[str]): Baseline variable for comparison. Defaults to None.\n            revaluation (Optional[str]): Revaluation variable. Defaults to None.\n            true_preds (bool): Subset by correct predictions. Defaults to False.\n            brier_threshold (Optional[float]): Filters observations ny Brier score\n                threshold. Defaults to None.\n            save (bool): If True, saves the feature importance plots. Defaults to False.\n            name (Optional[str]): Name for the saved feature importance plots.\n        \"\"\"\n        self.evaluator.X, self.evaluator.y, patients = self._test_filters(\n            X=self.evaluator.X,\n            y=self.evaluator.y,\n            base=base,\n            revaluation=revaluation,\n            true_preds=true_preds,\n            brier_threshold=brier_threshold,\n        )\n        print(f\"Number of patients in test set: {patients}\")\n        print(f\"Number of tooth sites: {len(self.evaluator.y)}\")\n        self.evaluator.evaluate_feature_importance(\n            fi_types=fi_types, save=save, name=name\n        )\n        self.evaluator.X, self.evaluator.y = self.X_test, self.y_test\n\n    def average_over_splits(\n        self, num_splits: int = 5, n_jobs: int = -1\n    ) -&gt; pd.DataFrame:\n        \"\"\"Trains the final model over multiple splits with different seeds.\n\n        Args:\n            num_splits (int): Number of random seeds/splits to train the model on.\n                Defaults to 5.\n            n_jobs (int): Number of parallel jobs. Defaults to -1 (use all processors).\n\n        Returns:\n            DataFrame: DataFrame containing average performance metrics.\n        \"\"\"\n        seeds = range(num_splits)\n        metrics_list = Parallel(n_jobs=n_jobs)(\n            delayed(self._train_and_get_metrics)(seed, self.learner) for seed in seeds\n        )\n        avg_metrics = {}\n        for metric in metrics_list[0]:\n            if metric == \"Confusion Matrix\":\n                continue\n            values = [d[metric] for d in metrics_list if d[metric] is not None]\n            avg_metrics[metric] = sum(values) / len(values) if values else None\n\n        avg_confusion_matrix = None\n        if self.classification == \"binary\" and \"Confusion Matrix\" in metrics_list[0]:\n            avg_confusion_matrix = (\n                np.mean([d[\"Confusion Matrix\"] for d in metrics_list], axis=0)\n                .astype(int)\n                .tolist()\n            )\n\n        results = {\n            \"Task\": self.task,\n            \"Learner\": self.learner,\n            \"Criterion\": self.criterion,\n            \"Sampling\": self.sampling,\n            \"Factor\": self.factor,\n            **{\n                metric: round(value, 4) if isinstance(value, (int, float)) else value\n                for metric, value in avg_metrics.items()\n            },\n        }\n\n        if avg_confusion_matrix is not None:\n            results[\"Confusion Matrix\"] = avg_confusion_matrix\n\n        return pd.DataFrame([results])\n\n    def wrapped_patient_inference(\n        self,\n        patient: Patient,\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n        \"\"\"Runs inference on the patient's data using the best-ranked model.\n\n        Args:\n            patient (Patient): A `Patient` dataclass instance containing patient-level,\n                tooth-level, and side-level information.\n\n        Returns:\n            DataFrame: DataFrame with predictions and probabilities for each side\n                of the patient's teeth.\n        \"\"\"\n        patient_data = patient_to_df(patient=patient)\n        predict_data, patient_data = self.inference_engine.prepare_inference(\n            task=self.task,\n            patient_data=patient_data,\n            encoding=self.encoding,\n            X_train=self.X_train,\n            y_train=self.y_train,\n        )\n\n        return self.inference_engine.patient_inference(\n            predict_data=predict_data, patient_data=patient_data\n        )\n\n    def wrapped_jackknife(\n        self,\n        patient: Patient,\n        results: pd.DataFrame,\n        sample_fraction: float = 1.0,\n        n_jobs: int = -1,\n        max_plots: int = 192,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Runs jackknife resampling for inference on a given patient's data.\n\n        Args:\n            patient (Patient): `Patient` dataclass instance containing patient-level\n                information, tooth-level, and side-level details.\n            results (pd.DataFrame): DataFrame to store results from jackknife inference.\n            sample_fraction (float, optional): The fraction of patient data to use for\n                jackknife resampling. Defaults to 1.0.\n            n_jobs (int, optional): The number of parallel jobs to run. Defaults to -1.\n            max_plots (int): Maximum number of plots for jackknife intervals.\n\n        Returns:\n            DataFrame: The results of jackknife inference.\n        \"\"\"\n        patient_data = patient_to_df(patient=patient)\n        patient_data, _ = self.inference_engine.prepare_inference(\n            task=self.task,\n            patient_data=patient_data,\n            encoding=self.encoding,\n            X_train=self.X_train,\n            y_train=self.y_train,\n        )\n        return self.inference_engine.jackknife_inference(\n            model=self.model,\n            train_df=self.train_df,\n            patient_data=patient_data,\n            encoding=self.encoding,\n            inference_results=results,\n            sample_fraction=sample_fraction,\n            n_jobs=n_jobs,\n            max_plots=max_plots,\n        )\n</code></pre>"},{"location":"reference/wrapper/evaluatorwrapper/#periomod.wrapper.EvaluatorWrapper.__init__","title":"<code>__init__(learners_dict, criterion, aggregate=True, verbose=False, random_state=0, test_size=0.2, path=Path('data/processed/processed_data.csv'))</code>","text":"<p>Initializes EvaluatorWrapper with model, evaluation, and inference setup.</p> <p>Parameters:</p> Name Type Description Default <code>learners_dict</code> <code>Dict</code> <p>Dictionary containing trained models.</p> required <code>criterion</code> <code>str</code> <p>The criterion used to select the best model ('f1', 'macro_f1', 'brier_score').</p> required <code>aggregate</code> <code>bool</code> <p>Whether to aggregate one-hot encoding. Defaults to True.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>If True, enables verbose logging during evaluation and inference. Defaults to False.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Random state for resampling. Defaults to 0</p> <code>0</code> <code>test_size</code> <code>float</code> <p>Size of grouped train test split. Defaults to 0.2.</p> <code>0.2</code> <code>path</code> <code>Path</code> <p>Path to the directory containing processed data files. Defaults to Path(\"data/processed/processed_data.csv\").</p> <code>Path('data/processed/processed_data.csv')</code> Source code in <code>periomod/wrapper/_wrapper.py</code> <pre><code>def __init__(\n    self,\n    learners_dict: Dict,\n    criterion: str,\n    aggregate: bool = True,\n    verbose: bool = False,\n    random_state: int = 0,\n    test_size: float = 0.2,\n    path: Path = Path(\"data/processed/processed_data.csv\"),\n) -&gt; None:\n    \"\"\"Initializes EvaluatorWrapper with model, evaluation, and inference setup.\n\n    Args:\n        learners_dict (Dict): Dictionary containing trained models.\n        criterion (str): The criterion used to select the best model ('f1',\n            'macro_f1', 'brier_score').\n        aggregate (bool): Whether to aggregate one-hot encoding. Defaults\n            to True.\n        verbose (bool): If True, enables verbose logging during evaluation\n            and inference. Defaults to False.\n        random_state (int): Random state for resampling. Defaults to 0\n        test_size (float): Size of grouped train test split. Defaults to 0.2.\n        path (Path): Path to the directory containing processed data files.\n            Defaults to Path(\"data/processed/processed_data.csv\").\n\n    \"\"\"\n    super().__init__(\n        learners_dict=learners_dict,\n        criterion=criterion,\n        aggregate=aggregate,\n        verbose=verbose,\n        random_state=random_state,\n        test_size=test_size,\n        path=path,\n    )\n</code></pre>"},{"location":"reference/wrapper/evaluatorwrapper/#periomod.wrapper.EvaluatorWrapper.average_over_splits","title":"<code>average_over_splits(num_splits=5, n_jobs=-1)</code>","text":"<p>Trains the final model over multiple splits with different seeds.</p> <p>Parameters:</p> Name Type Description Default <code>num_splits</code> <code>int</code> <p>Number of random seeds/splits to train the model on. Defaults to 5.</p> <code>5</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs. Defaults to -1 (use all processors).</p> <code>-1</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame containing average performance metrics.</p> Source code in <code>periomod/wrapper/_wrapper.py</code> <pre><code>def average_over_splits(\n    self, num_splits: int = 5, n_jobs: int = -1\n) -&gt; pd.DataFrame:\n    \"\"\"Trains the final model over multiple splits with different seeds.\n\n    Args:\n        num_splits (int): Number of random seeds/splits to train the model on.\n            Defaults to 5.\n        n_jobs (int): Number of parallel jobs. Defaults to -1 (use all processors).\n\n    Returns:\n        DataFrame: DataFrame containing average performance metrics.\n    \"\"\"\n    seeds = range(num_splits)\n    metrics_list = Parallel(n_jobs=n_jobs)(\n        delayed(self._train_and_get_metrics)(seed, self.learner) for seed in seeds\n    )\n    avg_metrics = {}\n    for metric in metrics_list[0]:\n        if metric == \"Confusion Matrix\":\n            continue\n        values = [d[metric] for d in metrics_list if d[metric] is not None]\n        avg_metrics[metric] = sum(values) / len(values) if values else None\n\n    avg_confusion_matrix = None\n    if self.classification == \"binary\" and \"Confusion Matrix\" in metrics_list[0]:\n        avg_confusion_matrix = (\n            np.mean([d[\"Confusion Matrix\"] for d in metrics_list], axis=0)\n            .astype(int)\n            .tolist()\n        )\n\n    results = {\n        \"Task\": self.task,\n        \"Learner\": self.learner,\n        \"Criterion\": self.criterion,\n        \"Sampling\": self.sampling,\n        \"Factor\": self.factor,\n        **{\n            metric: round(value, 4) if isinstance(value, (int, float)) else value\n            for metric, value in avg_metrics.items()\n        },\n    }\n\n    if avg_confusion_matrix is not None:\n        results[\"Confusion Matrix\"] = avg_confusion_matrix\n\n    return pd.DataFrame([results])\n</code></pre>"},{"location":"reference/wrapper/evaluatorwrapper/#periomod.wrapper.EvaluatorWrapper.compare_bss","title":"<code>compare_bss(base=None, revaluation=None, true_preds=False, brier_threshold=None, tight_layout=False, save=False, name=None)</code>","text":"<p>Compares Brier Skill Score of model with baseline on test set.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>Optional[str]</code> <p>Baseline variable for comparison. Defaults to None.</p> <code>None</code> <code>revaluation</code> <code>Optional[str]</code> <p>Revaluation variable. Defaults to None.</p> <code>None</code> <code>true_preds</code> <code>bool</code> <p>Subset by correct predictions. Defaults to False.</p> <code>False</code> <code>brier_threshold</code> <code>Optional[float]</code> <p>Filters observations ny Brier score threshold. Defaults to None.</p> <code>None</code> <code>tight_layout</code> <code>bool</code> <p>If True, applies tight layout to the plot. Defaults to False.</p> <code>False</code> <code>save</code> <code>bool</code> <p>If True, saves the BSS comparison plot. Defaults to False.</p> <code>False</code> <code>name</code> <code>Optional[str]</code> <p>Name for the saved BSS comparison plot.</p> <code>None</code> Source code in <code>periomod/wrapper/_wrapper.py</code> <pre><code>def compare_bss(\n    self,\n    base: Optional[str] = None,\n    revaluation: Optional[str] = None,\n    true_preds: bool = False,\n    brier_threshold: Optional[float] = None,\n    tight_layout: bool = False,\n    save: bool = False,\n    name: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Compares Brier Skill Score of model with baseline on test set.\n\n    Args:\n        base (Optional[str]): Baseline variable for comparison. Defaults to None.\n        revaluation (Optional[str]): Revaluation variable. Defaults to None.\n        true_preds (bool): Subset by correct predictions. Defaults to False.\n        brier_threshold (Optional[float]): Filters observations ny Brier score\n            threshold. Defaults to None.\n        tight_layout (bool): If True, applies tight layout to the plot.\n            Defaults to False.\n        save (bool): If True, saves the BSS comparison plot. Defaults to False.\n        name (Optional[str]): Name for the saved BSS comparison plot.\n    \"\"\"\n    baseline_models, _, _ = self.baseline.train_baselines()\n    self.evaluator.X, self.evaluator.y, patients = self._test_filters(\n        X=self.evaluator.X,\n        y=self.evaluator.y,\n        base=base,\n        revaluation=revaluation,\n        true_preds=true_preds,\n        brier_threshold=brier_threshold,\n    )\n    self.evaluator.bss_comparison(\n        baseline_models=baseline_models,\n        classification=self.classification,\n        num_patients=patients,\n        tight_layout=tight_layout,\n        save=save,\n        name=name,\n    )\n    self.evaluator.X, self.evaluator.y = self.X_test, self.y_test\n</code></pre>"},{"location":"reference/wrapper/evaluatorwrapper/#periomod.wrapper.EvaluatorWrapper.evaluate_cluster","title":"<code>evaluate_cluster(n_cluster=3, base=None, revaluation=None, true_preds=False, brier_threshold=None, tight_layout=False)</code>","text":"<p>Performs cluster analysis with Brier scores, optionally applying subsetting.</p> <p>This method allows detailed feature analysis by offering multiple subsetting options for the test set. The base and revaluation columns allow filtering of observations that have not changed after treatment. With true_preds, only observations that were correctly predicted are considered. The brier_threshold enables filtering of observations that achieved a smaller Brier score at prediction time than the threshold.</p> <p>Parameters:</p> Name Type Description Default <code>n_cluster</code> <code>int</code> <p>Number of clusters for Brier score clustering analysis. Defaults to 3.</p> <code>3</code> <code>base</code> <code>Optional[str]</code> <p>Baseline variable for comparison. Defaults to None.</p> <code>None</code> <code>revaluation</code> <code>Optional[str]</code> <p>Revaluation variable. Defaults to None.</p> <code>None</code> <code>true_preds</code> <code>bool</code> <p>Subset by correct predictions. Defaults to False.</p> <code>False</code> <code>brier_threshold</code> <code>Optional[float]</code> <p>Filters observations ny Brier score threshold. Defaults to None.</p> <code>None</code> <code>tight_layout</code> <code>bool</code> <p>If True, applies tight layout to the plot. Defaults to False.</p> <code>False</code> Source code in <code>periomod/wrapper/_wrapper.py</code> <pre><code>def evaluate_cluster(\n    self,\n    n_cluster: int = 3,\n    base: Optional[str] = None,\n    revaluation: Optional[str] = None,\n    true_preds: bool = False,\n    brier_threshold: Optional[float] = None,\n    tight_layout: bool = False,\n) -&gt; None:\n    \"\"\"Performs cluster analysis with Brier scores, optionally applying subsetting.\n\n    This method allows detailed feature analysis by offering multiple subsetting\n    options for the test set. The base and revaluation columns allow filtering of\n    observations that have not changed after treatment. With true_preds, only\n    observations that were correctly predicted are considered. The brier_threshold\n    enables filtering of observations that achieved a smaller Brier score at\n    prediction time than the threshold.\n\n    Args:\n        n_cluster (int): Number of clusters for Brier score clustering analysis.\n            Defaults to 3.\n        base (Optional[str]): Baseline variable for comparison. Defaults to None.\n        revaluation (Optional[str]): Revaluation variable. Defaults to None.\n        true_preds (bool): Subset by correct predictions. Defaults to False.\n        brier_threshold (Optional[float]): Filters observations ny Brier score\n            threshold. Defaults to None.\n        tight_layout (bool): If True, applies tight layout to the plot.\n            Defaults to False.\n    \"\"\"\n    self.evaluator.X, self.evaluator.y, patients = self._test_filters(\n        X=self.evaluator.X,\n        y=self.evaluator.y,\n        base=base,\n        revaluation=revaluation,\n        true_preds=true_preds,\n        brier_threshold=brier_threshold,\n    )\n    print(f\"Number of patients in test set: {patients}\")\n    print(f\"Number of tooth sites: {len(self.evaluator.y)}\")\n    self.evaluator.analyze_brier_within_clusters(\n        n_clusters=n_cluster,\n        tight_layout=tight_layout,\n    )\n    self.evaluator.X, self.evaluator.y = self.X_test, self.y_test\n</code></pre>"},{"location":"reference/wrapper/evaluatorwrapper/#periomod.wrapper.EvaluatorWrapper.evaluate_feature_importance","title":"<code>evaluate_feature_importance(fi_types, base=None, revaluation=None, true_preds=False, brier_threshold=None, save=False, name=None)</code>","text":"<p>Evaluates feature importance using the evaluator, with optional subsetting.</p> <p>This method allows detailed feature analysis by offering multiple subsetting options for the test set. The base and revaluation columns allow filtering of observations that have not changed after treatment. With true_preds, only observations that were correctly predicted are considered. The brier_threshold enables filtering of observations that achieved a smaller Brier score at prediction time than the threshold.</p> <p>Parameters:</p> Name Type Description Default <code>fi_types</code> <code>List[str]</code> <p>List of feature importance types to evaluate.</p> required <code>base</code> <code>Optional[str]</code> <p>Baseline variable for comparison. Defaults to None.</p> <code>None</code> <code>revaluation</code> <code>Optional[str]</code> <p>Revaluation variable. Defaults to None.</p> <code>None</code> <code>true_preds</code> <code>bool</code> <p>Subset by correct predictions. Defaults to False.</p> <code>False</code> <code>brier_threshold</code> <code>Optional[float]</code> <p>Filters observations ny Brier score threshold. Defaults to None.</p> <code>None</code> <code>save</code> <code>bool</code> <p>If True, saves the feature importance plots. Defaults to False.</p> <code>False</code> <code>name</code> <code>Optional[str]</code> <p>Name for the saved feature importance plots.</p> <code>None</code> Source code in <code>periomod/wrapper/_wrapper.py</code> <pre><code>def evaluate_feature_importance(\n    self,\n    fi_types: List[str],\n    base: Optional[str] = None,\n    revaluation: Optional[str] = None,\n    true_preds: bool = False,\n    brier_threshold: Optional[float] = None,\n    save: bool = False,\n    name: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Evaluates feature importance using the evaluator, with optional subsetting.\n\n    This method allows detailed feature analysis by offering multiple subsetting\n    options for the test set. The base and revaluation columns allow filtering of\n    observations that have not changed after treatment. With true_preds, only\n    observations that were correctly predicted are considered. The brier_threshold\n    enables filtering of observations that achieved a smaller Brier score at\n    prediction time than the threshold.\n\n    Args:\n        fi_types (List[str]): List of feature importance types to evaluate.\n        base (Optional[str]): Baseline variable for comparison. Defaults to None.\n        revaluation (Optional[str]): Revaluation variable. Defaults to None.\n        true_preds (bool): Subset by correct predictions. Defaults to False.\n        brier_threshold (Optional[float]): Filters observations ny Brier score\n            threshold. Defaults to None.\n        save (bool): If True, saves the feature importance plots. Defaults to False.\n        name (Optional[str]): Name for the saved feature importance plots.\n    \"\"\"\n    self.evaluator.X, self.evaluator.y, patients = self._test_filters(\n        X=self.evaluator.X,\n        y=self.evaluator.y,\n        base=base,\n        revaluation=revaluation,\n        true_preds=true_preds,\n        brier_threshold=brier_threshold,\n    )\n    print(f\"Number of patients in test set: {patients}\")\n    print(f\"Number of tooth sites: {len(self.evaluator.y)}\")\n    self.evaluator.evaluate_feature_importance(\n        fi_types=fi_types, save=save, name=name\n    )\n    self.evaluator.X, self.evaluator.y = self.X_test, self.y_test\n</code></pre>"},{"location":"reference/wrapper/evaluatorwrapper/#periomod.wrapper.EvaluatorWrapper.wrapped_evaluation","title":"<code>wrapped_evaluation(cm=True, cm_normalization='rows', cm_base=True, brier_groups=True, calibration=True, tight_layout=False, save=False, name=None)</code>","text":"<p>Runs evaluation on the best-ranked model.</p> <p>Parameters:</p> Name Type Description Default <code>cm</code> <code>bool</code> <p>Plot the confusion matrix. Defaults to True.</p> <code>True</code> <code>cm_normalization</code> <code>str</code> <p>Normalization for confusion matrix. Defaults to \"rows\".</p> <code>'rows'</code> <code>cm_base</code> <code>bool</code> <p>Plot confusion matrix vs value before treatment. Defaults to True.</p> <code>True</code> <code>brier_groups</code> <code>bool</code> <p>Calculate Brier score groups. Defaults to True.</p> <code>True</code> <code>calibration</code> <code>bool</code> <p>Plots model calibration. Defaults to True.</p> <code>True</code> <code>tight_layout</code> <code>bool</code> <p>If True, applies tight layout to the plot. Defaults to False.</p> <code>False</code> <code>save</code> <code>bool</code> <p>If True, saves the evaluation plots. Defaults to False.</p> <code>False</code> <code>name</code> <code>Optional[str]</code> <p>Name for the saved evaluation plots.</p> <code>None</code> Source code in <code>periomod/wrapper/_wrapper.py</code> <pre><code>def wrapped_evaluation(\n    self,\n    cm: bool = True,\n    cm_normalization: str = \"rows\",\n    cm_base: bool = True,\n    brier_groups: bool = True,\n    calibration: bool = True,\n    tight_layout: bool = False,\n    save: bool = False,\n    name: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Runs evaluation on the best-ranked model.\n\n    Args:\n        cm (bool): Plot the confusion matrix. Defaults to True.\n        cm_normalization (str): Normalization for confusion matrix.\n            Defaults to \"rows\".\n        cm_base (bool): Plot confusion matrix vs value before treatment.\n            Defaults to True.\n        brier_groups (bool): Calculate Brier score groups. Defaults to True.\n        calibration (bool): Plots model calibration. Defaults to True.\n        tight_layout (bool): If True, applies tight layout to the plot.\n            Defaults to False.\n        save (bool): If True, saves the evaluation plots. Defaults to False.\n        name (Optional[str]): Name for the saved evaluation plots.\n    \"\"\"\n    if cm:\n        self.evaluator.plot_confusion_matrix(\n            tight_layout=tight_layout,\n            normalize=cm_normalization,\n            task=self.task,\n            save=save,\n            name=(\n                f\"{name}_cm_predictionEval\"\n                if name is not None\n                else \"cm_predictionEval\"\n            ),\n        )\n    if cm_base:\n        if self.task in [\n            \"pocketclosure\",\n            \"pocketclosureinf\",\n            \"pdgrouprevaluation\",\n        ]:\n            self.evaluator.plot_confusion_matrix(\n                col=self.base_target,\n                y_label=\"Baseline PPD\",\n                tight_layout=tight_layout,\n                task=self.task,\n                save=save,\n                name=(\n                    f\"{name}_cm_baseToPredicted\"\n                    if name is not None\n                    else \"cm_baseToPredicted\"\n                ),\n            )\n\n    if brier_groups:\n        self.evaluator.brier_score_groups(\n            tight_layout=tight_layout, task=self.task, save=save, name=name\n        )\n    if calibration:\n        self.evaluator.calibration_plot(\n            task=self.task, tight_layout=tight_layout, save=save, name=name\n        )\n</code></pre>"},{"location":"reference/wrapper/evaluatorwrapper/#periomod.wrapper.EvaluatorWrapper.wrapped_jackknife","title":"<code>wrapped_jackknife(patient, results, sample_fraction=1.0, n_jobs=-1, max_plots=192)</code>","text":"<p>Runs jackknife resampling for inference on a given patient's data.</p> <p>Parameters:</p> Name Type Description Default <code>patient</code> <code>Patient</code> <p><code>Patient</code> dataclass instance containing patient-level information, tooth-level, and side-level details.</p> required <code>results</code> <code>DataFrame</code> <p>DataFrame to store results from jackknife inference.</p> required <code>sample_fraction</code> <code>float</code> <p>The fraction of patient data to use for jackknife resampling. Defaults to 1.0.</p> <code>1.0</code> <code>n_jobs</code> <code>int</code> <p>The number of parallel jobs to run. Defaults to -1.</p> <code>-1</code> <code>max_plots</code> <code>int</code> <p>Maximum number of plots for jackknife intervals.</p> <code>192</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The results of jackknife inference.</p> Source code in <code>periomod/wrapper/_wrapper.py</code> <pre><code>def wrapped_jackknife(\n    self,\n    patient: Patient,\n    results: pd.DataFrame,\n    sample_fraction: float = 1.0,\n    n_jobs: int = -1,\n    max_plots: int = 192,\n) -&gt; pd.DataFrame:\n    \"\"\"Runs jackknife resampling for inference on a given patient's data.\n\n    Args:\n        patient (Patient): `Patient` dataclass instance containing patient-level\n            information, tooth-level, and side-level details.\n        results (pd.DataFrame): DataFrame to store results from jackknife inference.\n        sample_fraction (float, optional): The fraction of patient data to use for\n            jackknife resampling. Defaults to 1.0.\n        n_jobs (int, optional): The number of parallel jobs to run. Defaults to -1.\n        max_plots (int): Maximum number of plots for jackknife intervals.\n\n    Returns:\n        DataFrame: The results of jackknife inference.\n    \"\"\"\n    patient_data = patient_to_df(patient=patient)\n    patient_data, _ = self.inference_engine.prepare_inference(\n        task=self.task,\n        patient_data=patient_data,\n        encoding=self.encoding,\n        X_train=self.X_train,\n        y_train=self.y_train,\n    )\n    return self.inference_engine.jackknife_inference(\n        model=self.model,\n        train_df=self.train_df,\n        patient_data=patient_data,\n        encoding=self.encoding,\n        inference_results=results,\n        sample_fraction=sample_fraction,\n        n_jobs=n_jobs,\n        max_plots=max_plots,\n    )\n</code></pre>"},{"location":"reference/wrapper/evaluatorwrapper/#periomod.wrapper.EvaluatorWrapper.wrapped_patient_inference","title":"<code>wrapped_patient_inference(patient)</code>","text":"<p>Runs inference on the patient's data using the best-ranked model.</p> <p>Parameters:</p> Name Type Description Default <code>patient</code> <code>Patient</code> <p>A <code>Patient</code> dataclass instance containing patient-level, tooth-level, and side-level information.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>Tuple[DataFrame, DataFrame, DataFrame]</code> <p>DataFrame with predictions and probabilities for each side of the patient's teeth.</p> Source code in <code>periomod/wrapper/_wrapper.py</code> <pre><code>def wrapped_patient_inference(\n    self,\n    patient: Patient,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Runs inference on the patient's data using the best-ranked model.\n\n    Args:\n        patient (Patient): A `Patient` dataclass instance containing patient-level,\n            tooth-level, and side-level information.\n\n    Returns:\n        DataFrame: DataFrame with predictions and probabilities for each side\n            of the patient's teeth.\n    \"\"\"\n    patient_data = patient_to_df(patient=patient)\n    predict_data, patient_data = self.inference_engine.prepare_inference(\n        task=self.task,\n        patient_data=patient_data,\n        encoding=self.encoding,\n        X_train=self.X_train,\n        y_train=self.y_train,\n    )\n\n    return self.inference_engine.patient_inference(\n        predict_data=predict_data, patient_data=patient_data\n    )\n</code></pre>"},{"location":"reference/wrapper/loadbenchmark/","title":"load_benchmark","text":"<p>Loads the benchmark DataFrame from a specified CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the benchmark CSV file.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Loaded benchmark DataFrame.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> Source code in <code>periomod/wrapper/_wrapper.py</code> <pre><code>def load_benchmark(path: Union[str, Path]) -&gt; pd.DataFrame:\n    \"\"\"Loads the benchmark DataFrame from a specified CSV file.\n\n    Args:\n        path (Union[str, Path]): Path to the benchmark CSV file.\n\n    Returns:\n        pd.DataFrame: Loaded benchmark DataFrame.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n    \"\"\"\n    path = Path(path)\n    if not path.is_absolute():\n        path = Path.cwd() / path\n\n    if not path.exists():\n        raise FileNotFoundError(f\"The file {path} does not exist.\")\n\n    return pd.read_csv(path)\n</code></pre>"},{"location":"reference/wrapper/loadlearners/","title":"load_learners","text":"<p>Loads the learners from a specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the directory where models are stored.</p> required <code>verbose</code> <code>bool</code> <p>Prints loaded models. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing loaded learners.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified directory does not exist.</p> Source code in <code>periomod/wrapper/_wrapper.py</code> <pre><code>def load_learners(path: Union[str, Path], verbose: bool = False) -&gt; dict:\n    \"\"\"Loads the learners from a specified directory.\n\n    Args:\n        path (Union[str, Path]): Path to the directory where models are stored.\n        verbose (bool): Prints loaded models. Defaults to False.\n\n    Returns:\n        dict: Dictionary containing loaded learners.\n\n    Raises:\n        FileNotFoundError: If the specified directory does not exist.\n    \"\"\"\n    path = Path(path)\n    if not path.is_absolute():\n        path = Path.cwd() / path\n\n    if not path.exists():\n        raise FileNotFoundError(f\"The directory {path} does not exist.\")\n\n    learners_dict = {}\n    for model_file in path.glob(\"*.pkl\"):\n        model = joblib.load(model_file)\n        learners_dict[model_file.stem] = model\n        if verbose:\n            print(f\"Loaded model {model_file.stem} from {model_file}\")\n\n    return learners_dict\n</code></pre>"},{"location":"reference/wrapper/modelextractor/","title":"ModelExtractor","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Extracts best model from learner dictionary.</p> Inherits <p><code>BaseConfig</code>: Loads configuration parameters.</p> <p>Parameters:</p> Name Type Description Default <code>learners_dict</code> <code>Dict</code> <p>Dictionary containing models and their metadata.</p> required <code>criterion</code> <code>str</code> <p>Criterion for selecting models (e.g., 'f1', 'brier_score').</p> required <code>aggregate</code> <code>bool</code> <p>Whether to aggregate metrics.</p> required <code>verbose</code> <code>bool</code> <p>Controls verbose in the evaluation process.</p> required <code>random_state</code> <code>int</code> <p>Random state for resampling.</p> required <p>Attributes:</p> Name Type Description <code>learners_dict</code> <code>Dict</code> <p>Holds learners and metadata.</p> <code>criterion</code> <code>str</code> <p>Evaluation criterion to select the optimal model.</p> <code>aggregate</code> <code>bool</code> <p>Indicates if metrics should be aggregated.</p> <code>verbose</code> <code>bool</code> <p>Flag for controlling logging verbose.</p> <code>random_state</code> <code>int</code> <p>Random state for resampling.</p> <code>classification</code> <code>str</code> <p>Classification type ('binary' or 'multiclass').</p> Properties <ul> <li><code>criterion (str)</code>: Retrieves or sets current evaluation criterion for model     selection. Supports 'f1', 'brier_score', and 'macro_f1'.</li> <li><code>model (object)</code>: Retrieves best-ranked model dynamically based on the current     criterion. Recalculates when criterion is updated.</li> </ul> Source code in <code>periomod/wrapper/_basewrapper.py</code> <pre><code>class ModelExtractor(BaseConfig):\n    \"\"\"Extracts best model from learner dictionary.\n\n    Inherits:\n        `BaseConfig`: Loads configuration parameters.\n\n    Args:\n        learners_dict (Dict): Dictionary containing models and their metadata.\n        criterion (str): Criterion for selecting models (e.g., 'f1', 'brier_score').\n        aggregate (bool): Whether to aggregate metrics.\n        verbose (bool): Controls verbose in the evaluation process.\n        random_state (int): Random state for resampling.\n\n    Attributes:\n        learners_dict (Dict): Holds learners and metadata.\n        criterion (str): Evaluation criterion to select the optimal model.\n        aggregate (bool): Indicates if metrics should be aggregated.\n        verbose (bool): Flag for controlling logging verbose.\n        random_state (int): Random state for resampling.\n        classification (str): Classification type ('binary' or 'multiclass').\n\n    Properties:\n        - `criterion (str)`: Retrieves or sets current evaluation criterion for model\n            selection. Supports 'f1', 'brier_score', and 'macro_f1'.\n        - `model (object)`: Retrieves best-ranked model dynamically based on the current\n            criterion. Recalculates when criterion is updated.\n    \"\"\"\n\n    def __init__(\n        self,\n        learners_dict: Dict,\n        criterion: str,\n        aggregate: bool,\n        verbose: bool,\n        random_state: int,\n    ):\n        \"\"\"Initializes ModelExtractor.\"\"\"\n        super().__init__()\n        self.learners_dict = learners_dict\n        self.criterion = criterion\n        self.aggregate = aggregate\n        self.verbose = verbose\n        self.random_state = random_state\n        self._update_best_model()\n        self.classification = (\n            \"multiclass\" if self.task == \"pdgrouprevaluation\" else \"binary\"\n        )\n\n    @property\n    def criterion(self) -&gt; str:\n        \"\"\"The current evaluation criterion used to select the best model.\n\n        Returns:\n            str: The current criterion for model selection (e.g., 'f1', 'brier_score').\n        \"\"\"\n        return self._criterion\n\n    @criterion.setter\n    def criterion(self, value: str) -&gt; None:\n        \"\"\"Sets the evaluation criterion and updates related attributes accordingly.\n\n        Args:\n            value (str): The criterion for selecting the model ('f1', 'brier_score',\n                or 'macro_f1').\n\n        Raises:\n            ValueError: If the provided criterion is unsupported.\n        \"\"\"\n        if value not in [\"f1\", \"brier_score\", \"macro_f1\"]:\n            raise ValueError(\n                \"Unsupported criterion. Choose 'f1', 'macro_f1', or 'brier_score'.\"\n            )\n        self._criterion = value\n        self._update_best_model()\n\n    @property\n    def model(self) -&gt; Any:\n        \"\"\"Retrieves the best model based on the current criterion.\n\n        Returns:\n            Any: The model object selected according to the current criterion.\n        \"\"\"\n        return self._model\n\n    def _update_best_model(self) -&gt; None:\n        \"\"\"Retrieves and updates the best model based on the current criterion.\"\"\"\n        (\n            best_model,\n            self.encoding,\n            self.learner,\n            self.task,\n            self.factor,\n            self.sampling,\n        ) = self._get_best()\n        self._model = best_model\n\n    def _get_best(self) -&gt; Tuple[Any, str, str, str, Optional[float], Optional[str]]:\n        \"\"\"Retrieves best model entities.\n\n        Returns:\n            Tuple: A tuple containing the best model, encoding ('one_hot' or 'target'),\n                learner, task, factor, and sampling type (if applicable).\n\n        Raises:\n            ValueError: If model with rank1 is not found, or any component cannot be\n                determined.\n        \"\"\"\n        best_model_key = next(\n            (\n                key\n                for key in self.learners_dict\n                if f\"_{self.criterion}_\" in key and \"rank1\" in key\n            ),\n            None,\n        )\n\n        if not best_model_key:\n            raise ValueError(\n                f\"No model with rank1 found for criterion '{self.criterion}' in dict.\"\n            )\n\n        best_model = self.learners_dict[best_model_key]\n\n        if \"one_hot\" in best_model_key:\n            encoding = \"one_hot\"\n        elif \"target\" in best_model_key:\n            encoding = \"target\"\n        else:\n            raise ValueError(\"Unable to determine encoding from the model key.\")\n\n        if \"upsampling\" in best_model_key:\n            sampling = \"upsampling\"\n        elif \"downsampling\" in best_model_key:\n            sampling = \"downsampling\"\n        elif \"smote\" in best_model_key:\n            sampling = \"smote\"\n        else:\n            sampling = None\n\n        key_parts = best_model_key.split(\"_\")\n        task = key_parts[0]\n        learner = key_parts[1]\n\n        for part in key_parts:\n            if part.startswith(\"factor\"):\n                factor_value = part.replace(\"factor\", \"\")\n                if factor_value.isdigit():\n                    factor = float(factor_value)\n                else:\n                    factor = None\n\n        return best_model, encoding, learner, task, factor, sampling\n</code></pre>"},{"location":"reference/wrapper/modelextractor/#periomod.wrapper.ModelExtractor.criterion","title":"<code>criterion: str</code>  <code>property</code> <code>writable</code>","text":"<p>The current evaluation criterion used to select the best model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The current criterion for model selection (e.g., 'f1', 'brier_score').</p>"},{"location":"reference/wrapper/modelextractor/#periomod.wrapper.ModelExtractor.model","title":"<code>model: Any</code>  <code>property</code>","text":"<p>Retrieves the best model based on the current criterion.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The model object selected according to the current criterion.</p>"},{"location":"reference/wrapper/modelextractor/#periomod.wrapper.ModelExtractor.__init__","title":"<code>__init__(learners_dict, criterion, aggregate, verbose, random_state)</code>","text":"<p>Initializes ModelExtractor.</p> Source code in <code>periomod/wrapper/_basewrapper.py</code> <pre><code>def __init__(\n    self,\n    learners_dict: Dict,\n    criterion: str,\n    aggregate: bool,\n    verbose: bool,\n    random_state: int,\n):\n    \"\"\"Initializes ModelExtractor.\"\"\"\n    super().__init__()\n    self.learners_dict = learners_dict\n    self.criterion = criterion\n    self.aggregate = aggregate\n    self.verbose = verbose\n    self.random_state = random_state\n    self._update_best_model()\n    self.classification = (\n        \"multiclass\" if self.task == \"pdgrouprevaluation\" else \"binary\"\n    )\n</code></pre>"},{"location":"reference/wrapper/validator/","title":"Validator","text":"<p>               Bases: <code>ModelExtractor</code></p> <p>Validator class for evaluating trained models on a separate validation dataset.</p> <p>This class loads a validation dataset, applies necessary transformations, handles encoding, and evaluates a pre-trained model.</p> Inherits <ul> <li><code>ModelExtractor</code>: Base class for extracting trained models and evaluation.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>learners_dict</code> <code>Dict</code> <p>Dictionary containing trained models.</p> required <code>criterion</code> <code>str</code> <p>Performance criterion for evaluation (e.g., \"f1\", \"brier_score\").</p> required <code>aggregate</code> <code>bool</code> <p>Whether to aggregate results across multiple models.</p> required <code>path_train</code> <code>Path</code> <p>Path to the training dataset used for encoding reference.</p> required <code>path_val</code> <code>Path</code> <p>Path to the validation dataset.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print detailed logs. Defaults to False.</p> <code>False</code> <code>random_state</code> <code>Optional[int]</code> <p>Random seed for reproducibility. Defaults to 0.</p> <code>0</code> <code>test_size</code> <code>float</code> <p>Proportion of the dataset to use as a test set when performing target encoding. Defaults to 0.2.</p> <code>0.2</code> <p>Attributes:</p> Name Type Description <code>dataloader</code> <code>ProcessedDataLoader</code> <p>Data loader for preprocessing input data.</p> <code>resampler</code> <code>Resampler</code> <p>Resampler for handling encoding and dataset splitting.</p> <code>path_train</code> <code>Path</code> <p>Path to the training dataset.</p> <code>path_val</code> <code>Path</code> <p>Path to the validation dataset.</p> <code>test_size</code> <code>float</code> <p>Proportion of training data used for validation when encoding.</p> <code>data</code> <code>DataFrame</code> <p>Raw validation dataset.</p> <code>data_processed</code> <code>DataFrame</code> <p>Processed validation dataset.</p> <code>X</code> <code>DataFrame</code> <p>Features from the validation dataset.</p> <code>y</code> <code>Series</code> <p>Target labels from the validation dataset.</p> <p>Methods:</p> Name Description <code>_prepare_validation_data</code> <p>Loads, processes, and encodes validation data to match training features.</p> <code>perform_validation</code> <p>Runs model evaluation on the validation dataset, returning performance metrics.</p> Inherited Methods <ul> <li><code>load_learners</code>: Loads trained models from a specified directory.</li> <li><code>apply_target_encoding</code>: Applies target encoding to categorical variables.</li> <li><code>apply_sampling</code>: Applies specified sampling strategy to balance the dataset.</li> <li><code>validate_dataframe</code>: Validates that input data meets requirements.</li> <li><code>get_probs</code>: Computes model prediction probabilities.</li> <li><code>final_metrics</code>: Computes final evaluation metrics based on task.</li> </ul> Example <pre><code>from periomod.wrapper import Validator\n\nvalidator = Validator(\n    learners_dict=learners,\n    criterion=\"f1\",\n    aggregate=True,\n    path_train=Path(\"../data/processed/processed_data.csv\"),\n    path_val=Path(\"../data/processed/processed_data_val.csv\"),\n    random_state=42,\n    test_size=0.2\n)\n\nresults = validator.perform_validation(verbose=True)\n</code></pre> Source code in <code>periomod/wrapper/_val.py</code> <pre><code>class Validator(ModelExtractor):\n    \"\"\"Validator class for evaluating trained models on a separate validation dataset.\n\n    This class loads a validation dataset, applies necessary transformations,\n    handles encoding, and evaluates a pre-trained model.\n\n    Inherits:\n        - `ModelExtractor`: Base class for extracting trained models and evaluation.\n\n    Args:\n        learners_dict (Dict): Dictionary containing trained models.\n        criterion (str): Performance criterion for evaluation\n            (e.g., \"f1\", \"brier_score\").\n        aggregate (bool): Whether to aggregate results across multiple models.\n        path_train (Path): Path to the training dataset used for encoding reference.\n        path_val (Path): Path to the validation dataset.\n        verbose (bool, optional): Whether to print detailed logs. Defaults to False.\n        random_state (Optional[int], optional): Random seed for reproducibility.\n            Defaults to 0.\n        test_size (float, optional): Proportion of the dataset to use as a test set when\n            performing target encoding. Defaults to 0.2.\n\n    Attributes:\n        dataloader (ProcessedDataLoader): Data loader for preprocessing input data.\n        resampler (Resampler): Resampler for handling encoding and dataset splitting.\n        path_train (Path): Path to the training dataset.\n        path_val (Path): Path to the validation dataset.\n        test_size (float): Proportion of training data used for validation when\n            encoding.\n        data (pd.DataFrame): Raw validation dataset.\n        data_processed (pd.DataFrame): Processed validation dataset.\n        X (pd.DataFrame): Features from the validation dataset.\n        y (pd.Series): Target labels from the validation dataset.\n\n    Methods:\n        _prepare_validation_data: Loads, processes, and encodes validation data to match\n            training features.\n        perform_validation: Runs model evaluation on the validation dataset, returning\n            performance metrics.\n\n    Inherited Methods:\n        - `load_learners`: Loads trained models from a specified directory.\n        - `apply_target_encoding`: Applies target encoding to categorical variables.\n        - `apply_sampling`: Applies specified sampling strategy to balance the dataset.\n        - `validate_dataframe`: Validates that input data meets requirements.\n        - `get_probs`: Computes model prediction probabilities.\n        - `final_metrics`: Computes final evaluation metrics based on task.\n\n    Example:\n        ```\n        from periomod.wrapper import Validator\n\n        validator = Validator(\n            learners_dict=learners,\n            criterion=\"f1\",\n            aggregate=True,\n            path_train=Path(\"../data/processed/processed_data.csv\"),\n            path_val=Path(\"../data/processed/processed_data_val.csv\"),\n            random_state=42,\n            test_size=0.2\n        )\n\n        results = validator.perform_validation(verbose=True)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        learners_dict: Dict,\n        criterion: str,\n        aggregate: bool,\n        path_train: Path,\n        path_val: Path,\n        verbose: bool = False,\n        random_state: int = 0,\n        test_size: float = 0.2,\n    ):\n        \"\"\"Initializes the Validator class.\n\n        Args:\n            learners_dict (Dict): Dictionary containing trained models.\n            criterion (str): Performance criterion for evaluation\n                (e.g., \"f1\", \"brier_score\").\n            aggregate (bool): Whether to aggregate results across multiple models.\n            path_train (Path): Path to the training dataset used for encoding reference.\n            path_val (Path): Path to the validation dataset.\n            verbose (bool, optional): Whether to print detailed logs. Defaults to False.\n            random_state (Optional[int], optional): Random seed for reproducibility.\n                Defaults to 0.\n            test_size (float, optional): Proportion of the dataset to use as a test set\n                when performing target encoding. Defaults to 0.2.\n        \"\"\"\n        super().__init__(\n            learners_dict=learners_dict,\n            criterion=criterion,\n            aggregate=aggregate,\n            verbose=verbose,\n            random_state=random_state,\n        )\n        self.dataloader = ProcessedDataLoader(task=self.task, encoding=self.encoding)\n        self.resampler = Resampler(\n            classification=self.classification, encoding=self.encoding\n        )\n        self.path_train = path_train\n        self.path_val = path_val\n        self.test_size = test_size\n        self.data, self.data_processed, self.X, self.y = self._prepare_validation_data()\n\n    def _prepare_validation_data(\n        self,\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n        \"\"\"Loads and prepares validation data for model evaluation.\n\n        This function loads the dataset, applies necessary transformations,\n        and encodes categorical variables if required.\n\n        Returns:\n                Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series]:\n                    - data (pd.DataFrame): Raw validation dataset.\n                    - data_processed (pd.DataFrame): Preprocessed dataset.\n                    - X (pd.DataFrame): Feature matrix.\n                    - y (pd.Series): Target labels.\n\n        Raises:\n            ValueError: If model type is not supported for extraction.\n        \"\"\"\n        data_train = self.dataloader.load_data(path=self.path_train)\n        data_train = self.dataloader.transform_data(data=data_train, fit_encoder=True)\n\n        # 2) Load + transform (val) with fit_encoder=False\n        data_val = self.dataloader.load_data(path=self.path_val)\n        data_processed = self.dataloader.transform_data(\n            data=data_val, fit_encoder=False\n        )\n\n        X_val = data_processed.drop(columns=[self.y])\n        y = data_processed[self.y]\n\n        # If using target encoding, apply target encoding logic here:\n        if self.encoding == \"target\":\n            train_df, test_df = self.resampler.split_train_test_df(\n                df=data_train, seed=self.random_state, test_size=self.test_size\n            )\n            X_train, y_train, _, _ = self.resampler.split_x_y(\n                train_df=train_df, test_df=test_df\n            )\n\n            X_train, X_val = self.resampler.apply_target_encoding(\n                X=X_train, X_val=X_val, y=y_train\n            )\n\n            missing_cols = set(X_train.columns) - set(X_val.columns)\n            for col in missing_cols:\n                X_val[col] = 0  # Fill with zero if missing\n            X_val = X_val[X_train.columns]\n\n        if self.encoding == \"one_hot\":\n            if hasattr(self.model, \"get_booster\"):\n                expected_columns = set(self.model.get_booster().feature_names)\n            elif hasattr(self.model, \"feature_names_in_\"):\n                expected_columns = set(self.model.feature_names_in_)\n            else:\n                raise ValueError(\"Model type not supported for feature extraction\")\n\n            current_columns = set(X_val.columns)\n            missing_columns = expected_columns - current_columns\n            for col in missing_columns:\n                X_val[col] = 0\n\n            X_val = X_val[list(self.model.feature_names_in_)]\n\n        if self.group_col in X_val:\n            X_val = X_val.drop(columns=[self.group_col])\n\n        return data_val, data_processed, X_val, y\n\n    def perform_validation(self, verbose: bool = False) -&gt; pd.DataFrame:\n        \"\"\"Runs model evaluation on the validation dataset.\n\n        This function computes predictions and evaluates the model's performance\n        using predefined metrics such as F1-score or other classification metrics.\n\n        Args:\n            verbose (bool, optional): Whether to print detailed evaluation metrics.\n                Defaults to False.\n\n        Returns:\n            pd.DataFrane: A dataframe containing computed evaluation metrics.\n        \"\"\"\n        best_threshold = getattr(self.model, \"best_threshold\", None)\n\n        final_probs = get_probs(\n            model=self.model, classification=self.classification, X=self.X\n        )\n\n        if (\n            self.criterion == \"f1\"\n            and final_probs is not None\n            and np.any(final_probs)\n            and best_threshold is not None\n        ):\n            final_predictions = (final_probs &gt;= best_threshold).astype(int)\n        else:\n            final_predictions = self.model.predict(self.X)\n\n        metrics = final_metrics(\n            classification=self.classification,\n            y=self.y,\n            preds=final_predictions,\n            probs=final_probs,\n            threshold=best_threshold,\n        )\n        unpacked_metrics = {\n            k: round(v, 4) if isinstance(v, float) else v for k, v in metrics.items()\n        }\n        results = {\n            \"Learner\": self.learner,\n            \"Tuning\": \"final\",\n            \"Criterion\": self.criterion,\n            **unpacked_metrics,\n        }\n\n        df_results = pd.DataFrame([results])\n        if verbose:\n            pd.set_option(\"display.max_columns\", None, \"display.width\", 1000)\n            print(\"\\nFinal Model Metrics Summary:\\n\", df_results)\n        return df_results\n</code></pre>"},{"location":"reference/wrapper/validator/#periomod.wrapper.Validator.__init__","title":"<code>__init__(learners_dict, criterion, aggregate, path_train, path_val, verbose=False, random_state=0, test_size=0.2)</code>","text":"<p>Initializes the Validator class.</p> <p>Parameters:</p> Name Type Description Default <code>learners_dict</code> <code>Dict</code> <p>Dictionary containing trained models.</p> required <code>criterion</code> <code>str</code> <p>Performance criterion for evaluation (e.g., \"f1\", \"brier_score\").</p> required <code>aggregate</code> <code>bool</code> <p>Whether to aggregate results across multiple models.</p> required <code>path_train</code> <code>Path</code> <p>Path to the training dataset used for encoding reference.</p> required <code>path_val</code> <code>Path</code> <p>Path to the validation dataset.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print detailed logs. Defaults to False.</p> <code>False</code> <code>random_state</code> <code>Optional[int]</code> <p>Random seed for reproducibility. Defaults to 0.</p> <code>0</code> <code>test_size</code> <code>float</code> <p>Proportion of the dataset to use as a test set when performing target encoding. Defaults to 0.2.</p> <code>0.2</code> Source code in <code>periomod/wrapper/_val.py</code> <pre><code>def __init__(\n    self,\n    learners_dict: Dict,\n    criterion: str,\n    aggregate: bool,\n    path_train: Path,\n    path_val: Path,\n    verbose: bool = False,\n    random_state: int = 0,\n    test_size: float = 0.2,\n):\n    \"\"\"Initializes the Validator class.\n\n    Args:\n        learners_dict (Dict): Dictionary containing trained models.\n        criterion (str): Performance criterion for evaluation\n            (e.g., \"f1\", \"brier_score\").\n        aggregate (bool): Whether to aggregate results across multiple models.\n        path_train (Path): Path to the training dataset used for encoding reference.\n        path_val (Path): Path to the validation dataset.\n        verbose (bool, optional): Whether to print detailed logs. Defaults to False.\n        random_state (Optional[int], optional): Random seed for reproducibility.\n            Defaults to 0.\n        test_size (float, optional): Proportion of the dataset to use as a test set\n            when performing target encoding. Defaults to 0.2.\n    \"\"\"\n    super().__init__(\n        learners_dict=learners_dict,\n        criterion=criterion,\n        aggregate=aggregate,\n        verbose=verbose,\n        random_state=random_state,\n    )\n    self.dataloader = ProcessedDataLoader(task=self.task, encoding=self.encoding)\n    self.resampler = Resampler(\n        classification=self.classification, encoding=self.encoding\n    )\n    self.path_train = path_train\n    self.path_val = path_val\n    self.test_size = test_size\n    self.data, self.data_processed, self.X, self.y = self._prepare_validation_data()\n</code></pre>"},{"location":"reference/wrapper/validator/#periomod.wrapper.Validator.perform_validation","title":"<code>perform_validation(verbose=False)</code>","text":"<p>Runs model evaluation on the validation dataset.</p> <p>This function computes predictions and evaluates the model's performance using predefined metrics such as F1-score or other classification metrics.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>Whether to print detailed evaluation metrics. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrane: A dataframe containing computed evaluation metrics.</p> Source code in <code>periomod/wrapper/_val.py</code> <pre><code>def perform_validation(self, verbose: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Runs model evaluation on the validation dataset.\n\n    This function computes predictions and evaluates the model's performance\n    using predefined metrics such as F1-score or other classification metrics.\n\n    Args:\n        verbose (bool, optional): Whether to print detailed evaluation metrics.\n            Defaults to False.\n\n    Returns:\n        pd.DataFrane: A dataframe containing computed evaluation metrics.\n    \"\"\"\n    best_threshold = getattr(self.model, \"best_threshold\", None)\n\n    final_probs = get_probs(\n        model=self.model, classification=self.classification, X=self.X\n    )\n\n    if (\n        self.criterion == \"f1\"\n        and final_probs is not None\n        and np.any(final_probs)\n        and best_threshold is not None\n    ):\n        final_predictions = (final_probs &gt;= best_threshold).astype(int)\n    else:\n        final_predictions = self.model.predict(self.X)\n\n    metrics = final_metrics(\n        classification=self.classification,\n        y=self.y,\n        preds=final_predictions,\n        probs=final_probs,\n        threshold=best_threshold,\n    )\n    unpacked_metrics = {\n        k: round(v, 4) if isinstance(v, float) else v for k, v in metrics.items()\n    }\n    results = {\n        \"Learner\": self.learner,\n        \"Tuning\": \"final\",\n        \"Criterion\": self.criterion,\n        **unpacked_metrics,\n    }\n\n    df_results = pd.DataFrame([results])\n    if verbose:\n        pd.set_option(\"display.max_columns\", None, \"display.width\", 1000)\n        print(\"\\nFinal Model Metrics Summary:\\n\", df_results)\n    return df_results\n</code></pre>"}]}